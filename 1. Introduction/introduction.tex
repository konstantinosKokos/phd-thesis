\chapter{Introduction}
\label{chapter:Introduction}

\chapabstract{In the beginning there was the word, and the word had a \type.}

Our story begins with the (over-ambitious, in hindsight) musings of one of the world's most well-renowned mathematicians, David Hilbert. 
Unhappy with the numerous paradoxes and inconsistencies of mathematics at the end of the 19th century, Hilbert would postulate the existence and advocate the formulation of a finite set of axiomatic rules, which, when put together, would give rise to the most well-behaved system known to [wo]mankind, capable of acting as a universal meta-theory for all mathematics, in the process absolving all mathematicians of their sins.
The idea was of course appealing and gained traction, not the least due to Hilbert's influence over the field (and his will to exercise it).
As with all ideas that generate traction, however, it was not long before a cultural counter-movement would develop.
Intuitionism, with Luitzen Egbertus Jan Brouwer as its forefather, would challenge Hilbert's program by questioning the objective validity of (any) mathematical logic.
What it would claim, instead, is that mathematics is but a subjective process of construction that abides by some rules of inference, which, internally consistent as they may be, hold no reflection of deeper truth or meaning.
In practice, intuitionists would reject the law of the excluded middle (an essential tool for Hilbert's school of formalists) and argue that for a proof to be considered valid, it has to provide concrete instructions for the construction of the object it claims to prove.
The dispute went on for a couple of decades, its flame carried on by the respective students of the two rivals.
Logic, intrigue, conflict, fame, no \LaTeX{} errors...  these truly were the years to be an active mathematician.
Eventually, in a critical moment of clarity and inspiration, and tired by the ongoing drama, Kurt G\"odel, with his famous incompleteness theorem, would declare Hilbert's program unattainable, thus putting a violent end to the line of formalist heathens, and paving the way for the true revolution that was to come.
This is in reference, of course, to the biggest discovery of the last century\footnote{In proof theory, at least.}, made independently (using wildly different words every time) by various mathematicians and logicians spanning different timelines.
Put plainly, what is now known as the Curry-Howard correspondence establishes a syntactic equivalence between deductive systems in intuitionistic brands of logic and corresponding computational systems, called $\lam$-calculi.
Put even more plainly, it suggests that valid proofs in such logics constitute in fact compilable code for functional progams, bridging in essence the seemingly disparate fields of mathematical logic and computer science.
The repercussions of this discovery were enormous, and are more tangible today than ever before; type systems comprised of higher-order $\lam$-calculi and their logics provide the theoretical foundations for modern programming languages and proof assistants (this last fact is both important and interesting, but won't concern us much presently).

In a more niche (but equally beautiful) fragment of the academic world, and in parallel to the above developments, applied logicians and formally inclined linguists have been demonstrating a stunning perserverance in their self-imposed quest of modeling natural language syntax and semantics, making do only with the vocabulary provided by formal logics.
This noble endeavour traces its origins back to Aristotle, but its modern incarnation is due to Jim Lambek, who was the first to point out that the grammaticality of a natural language utterance can be equated to provability in a certain logic (or type inhabitation, if one is to borrow the terminology of constructive type theories), if the grammar (a collection of empirical linguistic rules) were to be treated as a substructural logic (a collection of formal mathematical rules).
Funnily enough, the kind of logics Lambek would employ for his purposes would be exactly those at the interesection of intuitionistic and linear logic, the latter only made formally explicit in a breakthrough paper by Jean-Yves Girard almost three decades later.
By that time, Richard Montague had already come up with the fantastically novel idea of seeing no distinction between formal and natural languages, single-handedly birthing and popularizing the field of formal semantics (which would chiefly invole semantic computations using $\lam$-calculus notation).
With this, he fulfilled Gottlob Frege's long-prophesized principle of compositionality, which would once and for all put the Chomskian tradition to rest\footnote{In some corners of the Earth, this part of the prophecy is yet to transpire.}, ushering linguistics into a new era.
With the benefit of posterity, it would be tempting for us to act smart and exclaim that Lambek and Montague's ideas were remarkably aligned. 
In reality, it took another couple of decades for someone to notice.
The credit is due to Johan van Benthem, who basically pointed out that Lambek's calculi make for the perfect syntactic machinery for Montague's program, seeing as they admit the Curry-Howard correspondence, and are therefore able to drive semantic composition virtually for free (in fact one could go as far as to say that they are the only kind of machinery that can accomplish such a feat without being riddled with ad-hoc transformations).
This revelation, combined with the contemporary bloom of substructural logics, was the spark that ignited a renewed interest in Lambek's work.
The culmination point for this interest was type-logical grammars (or categorial type logics): families of closely related type theories extending the original calculi of Lambek with unary operators lent from modal logic, intended to implement a stricter but more linguistically faithful modeling of the composition of natural language form and meaning.

In this chapter, we will isolate some key concepts from this frantic timeline and expound a bit on their details.
Other than reinvented notation or perhaps some fresh example, no novel contributions are to be found here; the intention is merely to establish some common grounds before we get to proceed. 
If confident in your knowledge of the subject matter, \texttt{goto} Chapter~\ref{chapter:chapter_2}, but at your own risk.

%\newpage

\section{The Simple Theory of Types}\label{section:simple_type_theory}

Simple type theory is the computational formalization of intuitionistic logic. 
It is in essence an adornment of the rules of intuitionistic logic with the computational manipulations they dictate upon mathematical terms.
Dually, it provides a decision procedure that allows one to infer the type of a given program by inspecting the operations that led up to its construction.
It is a staple of almost folkloric standing for computer scientists across the globe, tracing its origins to the seminal works of Russel and Church~\cite{russel1908,church1940}.
The adjective ``simple'' is not intended as either a diminutive nor a condescending remark pertaining to the difficulty of the subject matter, but rather to distinguish it from the broader class of intuitionistic type theories, which attempt to systematize the notions of quantification (universal and existential), stratification of propositional hierarchies, and more recently equivalence (neither of which we will concern ourselves with).

Our presentation will begin with intuitionistic logic. 
Once that is done, we will give a brief account of the the Curry-Howard correspondence, which shall allow us to give a computational account of the logic, that being the simply typed $\lam$-calculus.

\subsection{Intuitionistic Logic}
\label{subsection:intuitionistic_logic}
Intuitionistic logic is due to Arend Heyting~\cite{heyting1930}, who was the first to formalize Bouwer's intuitionism.
It is a restricted version of classical logic, where the laws of the excluded middle (\textit{tertium non datur}) and the elimination of the double negation no longer hold universally.
The first states that one must choose between a proposition $\prop{a}$ and its negation $\neg \prop{a}$ ($\prop{a} \vee \neg \prop{a}$), whereas the second that a double negation is equivalent to an identity ($\neg \neg \prop{a} \equiv \prop{a}$).
The absence of these two laws implies that several theorems of classical logic are no longer derivable in intuitionistic logic, meaning that the logic is weaker in terms of expressivity.
On the bright side, it has the pleasant effect that proofs of intuitionistic logic are constructive, i.e. they explicitly demonstrate the formation of a concrete instance of whatever proposition they claim to be proving.

Focusing on the disjunction-free fragment of the logic, we have a tiny recursive language that allows us to define the various shapes of logical \textit{propositions} (or \textit{formulas}).%
\footnote{The full logic also includes disjunctive formulas, but we will skip them from this presentation as they are of little interest to us. For brevity, we will from now on use intuitionistic logic to refer to its disjunction-free fragment.}
Given some finite set of \textit{propositional constants} (or \textit{atomic formulas}) \propcon{}, and \prop{a}, \prop{b}, \prop{c} arbitrary well-formed propositions, the language of propositions in Backus-Naur form is inductively defined as:
\begin{equation}
\prop{a}, \prop{b}, \prop{c} := p \ | \ \prop{a} \ii \prop{b} \ | \ \prop{a} \times \prop{b} 
\label{equation:IL}
\end{equation}
where $p \in \propcon{}$.
Propositions are therefore closed under the two binary \textit{logical connectives} $\ii$ and $\times$; we call the first an \textit{implication}, and the second a \textit{conjunction}. 
A \textit{complex} proposition is any proposition that is not a member of \propcon{}, and its \textit{primary} (or main) connective is the last logical connective used when writing it down according to the grammar~(\ref{equation:IL}).

Besides propositions, we have \textit{structures}. 
Structures are built from propositions with the aid of a single binary operation, the notation and properties of which can vary between different presentations of the logic.
In our case, we will indicate valid structures with Greek uppercase letters $\Gamma$, $\Delta, \Theta$, and define structures inductively as 
\begin{equation}
	\Gamma, \Delta, \Theta := 1 \ | \ \prop{a} \ | \ \Gamma, \Delta
\end{equation}
In other words, structures are an inductive set closed under the operator $\_,\_$ which satisfies associativity and is equipped with an identity element $1$ (the \textit{empty} structure), i.e. a monoid.
A perhaps more down-to-earth way of looking at a structure is as a \textit{list} or \textit{sequence} of propositions.

Given propositions and structures, we can next define \textit{judgements}, statements of the form
$\Gamma \vdash \prop{a}$.
We read such a statement as a suggestion that from \textit{assumptions} $\Gamma$ (i.e. a structure of \textit{hypotheses}) one can derive a proposition \smallprop{A}.
Formulas occurring within $\Gamma$ are said to occur in \textit{antecedent} position, whereas \prop{a} is in \textit{succedent} position.

A \textit{rule} is a two-line statement separated by a horizontal line.
Above the line, we have a (possibly empty) sequence of judgements, which we call the \textit{premises} of the rule.
Below the line, we have a single judgement, which we call the rule's \textit{conclusion}.
The rule can be thought of as a formal guarantee that if all of its premises are deliverable, then so is the conclusion.
Each rule has an identifying name, written directly to the right of the horizontal line.

Rules may be split in two conceptual categories.
\textit{Logical} rules, on the one hand, provide instructions for eliminating and introducing logical connectives.
Figure~\ref{subfigure:intuitionistic_logic_rules:logical} presents the logical rules of intuitionistic logic.
The first rule, the axiom of identity $\Ax$, contains no premises and asserts the reflexivity of the provability operator $\vdash$. 
It states that from a proposition $\smallprop{A}$ one can infer, guess what, that very proposition.
The remaining logical rules come in pairs, one per logical connective.
The elimination of the implication (or modus ponens) states that, given a proof of a proposition $\smallprop A \ii \prop{b}$ from assumptions $\Gamma$ and a proof of proposition $\smallprop{A}$ from assumptions $\Delta$, one can join the two to derive a proposition $\smallprop{B}$.
Dually, the introduction of the implication (or \textit{deduction theorem}) states that from a proof of a proposition \prop{b} given assumptions $\Gamma, \smallprop{A}$, one can use $\Gamma$ alone to derive an implicational proposition $\smallprop A \ii \smallprop B$.
In a similar manner, the elimination of the conjunction $\times E $ states that, given a proof of a proposition $\smallprop A \times \smallprop B$ from assumptions $\Gamma$, and a proof that the triplet $\Delta, \smallprop A, \smallprop B$ allows us to derive a proposition $\smallprop C$, one could well use $\Gamma$ together with $\Delta$ to derive $\prop C$ directly.
And dually again, the introduction of the conjunction $\times I$ permits us to join two unrelated proofs, one of $\smallprop A$ from $\Gamma$ and one of $\smallprop B$ from $\Delta$ into a single proof, that of their product $\smallprop A \times \smallprop B$, from $\Gamma$ joined with $\Delta$.

\textit{Structural} rules, on the other hand, allow us to manipulate structures (who would have thought); they are presented in Figure~\ref{subfigure:intuitionistic_logic_rules:structural}.
Structural rules have a two-fold role.
First, they explicate an extra propery of our structure binding operator, namely commutativity.
One could also make do with an implicit exchange rule by treating structures as \textit{multisets} rather than lists -- having it explicit, however, will keep us conscious of its presence and strengthen our emotional bond to it, in turn making us really notice its absence when it will no longer be there (it also keeps the presentation tidier).
Second, they give an account of propositions as permanent and reusable facts.
The weakening rule $\Weakening{}$ states that if we were able to derive a proposition $\smallprop{B}$ from some assumptions $\Gamma$, we will also be able to do so if the assumptions were to contain some arbitrary extra proposition $\smallprop{A}$.
Conversely, the contraction rule $\Contraction{}$ states that if we needed some assumption structure containing two instances of a proposition $\smallprop{A}$ to derive a proposition $\smallprop{B}$, we could also make do with just one instance of it, discarding the other without remorse.

A \textit{proof}, finally, is a heterogeneous variadic tree.
At its root, it has a judgement, guaranteed to be derivable (provided we did not mess up somewhere), called its \textit{endsequent}.
Its branches are themselves proofs, fused together by a rule -- the number of premises being the local tree's arity.
At its leaves, it has identity axioms -- the smallest kind of proof.

\begin{figure}
	\centering
	\begin{subfigure}{1\textwidth}
		\centering
		\begin{tabularx}{0.7\textwidth}{@{}C@{\qquad}C@{}}
		\multicolumn{2}{@{}c@{}}{$\infer[\Ax]{\prop{a} \vdash \prop{a}}{}$}\\[\smallsep]
		$\infer[\ii E]{\Gamma, \Delta \vdash \prop{b}}{\Gamma \vdash \prop{a} \ii \prop{b} & \Delta \vdash \prop{b}}$ 
		& 
		$\infer[\ii I]{\Gamma \vdash \prop{a} \ii \prop{b}}{\Gamma, \prop{a} \vdash \prop{b}}$\\[\smallsep]
		$\infer[\times E]{\Gamma,\Delta \vdash \prop C}{\Gamma \vdash \prop{a} \times \prop{b} & \Delta, \prop{a} , \prop{b} \vdash \prop C}$ 
		&
		$\infer[\times I]{\Gamma, \Delta \vdash \prop{a} \times \prop{b}}{\Gamma \vdash \prop{a} & \Delta \vdash \prop{b}}$
		\end{tabularx}
		\caption{Logical rules.}
		\label{subfigure:intuitionistic_logic_rules:logical}
	\end{subfigure}\\[\midsep]
	\begin{subfigure}{1\textwidth}
		\centering
		\begin{tabularx}{0.7\textwidth}{@{}C@{\qquad}C@{}}
		\multicolumn{2}{@{}c@{}}{$\infer[\Exchange{}]{\Delta, \Gamma \vdash \prop{a}}{\Gamma, \Delta \vdash \prop{a}}$}\\[\smallsep]
		$\infer[\Weakening{}]{\Gamma, \prop{a} \vdash \prop{b}}{\Gamma \vdash \prop{b}} $ 
		&
		$\infer[\Contraction{}]{\Gamma, \prop{a} \vdash \prop{b}}{\Gamma, \prop{a}, \prop{a} \vdash \prop{b}}$
		\end{tabularx}
		\caption{Structural rules.}
		\label{subfigure:intuitionistic_logic_rules:structural}
	\end{subfigure}
	\caption{Intuitionistic Logic $\IL_{\ii,\times}$.}
	\label{figure:intuitioistic_logic_rules}
\end{figure}

\subsubsection{Proof Equivalences}
The same judgement may be provable in more than one ways.
The difference between two proofs of the same judgement can be substantial, when they indeed describe distinct derivation procedures, or trivial.
Trivial variations come in two kinds: syntactic equivalences (i.e. sequences of rule applications that can safely be rearranged) and redundant detours (i.e. sequences of rule applications that can altogether removed).

The first kind is not particularly noteworthy.
In essence, we say that two proofs are syntactically equivalent if they differ only in the positioning of structural rule applications.
This notion can be formally captured by establishing an equivalence relation between proofs on the basis of commuting conversions.

The second kind is more interesting and slightly more involved.
A proof pattern in which a logical connective is introduced, only to be immediately eliminated, is called a \textit{detour} (or $\beta$ redex).
Detours can be locally resolved via proof rewrites -- the fix-point of performing all applicable resolutions is called \textit{proof normalization} and yields a canonical proof form. 
The strong normalisation property guarantees that a canonical form exists for any proof in the logic, and in fact the choice of available rewrites to apply at each step is irrelevant, as all paths have the same end point~\cite{groote1999strong}.
Figure~\ref{figure:intuitionistic_proof_reduction} presents rewrite instructions for the two detour patterns we may encounter (one per logical connective).
Read bottom-up%
\footnote{In the small-to-big rather than literal sense! If confused: start from the proof leaves and go down.},
the first one suggests that if one were to hypothesize a proposition $\smallprop{A}$, use it within an (arbitrarily deep) proof $s$ together with extra assumptions $\Gamma$ to derive a proposition $\smallprop{B}$, before finally redacting the hypothesis and composing with a proof $t$ that derives $\smallprop{A}$ from assumptions $\Delta$, it would have been smarter (and more concise!) to just plug in $t$ directly when previously hypothesizing $\smallprop{A}$, since then no redaction or composition would have been necessary.
In a similar vein, the second suggests that if one were to derive and merge proofs $s$ and $t$ (of propositions $\smallprop{A}$ and $\smallprop{B}$, respectively), only to eliminate their product against hypothetical instances of $\smallprop{A}$ and $\smallprop{B}$ that were used in proof $u$ to derive $\smallprop{C}$ (together with assumptions $\Theta$), the proof can be reduced by just plugging $s$ and $t$ in place of the axiom leaves of $u$.
Note the use of horizontal dots at the axiom leaves, denoting simultaneous substitutions of \textit{all} occurrences of redundant hypotheses, and the use of unnamed vertical dots, denoting (invertible) sequences of \Contraction{} and/or \Exchange{} rules.

\begin{figure}
	\centering
	\begin{tabularx}{0.9\textwidth}{@{}c@{~}c@{~}c@{}}
	$\infer[\ii E]{\Gamma, \Delta \vdash \prop{b}}{
	\infer[\ii I]{\Gamma \vdash \prop{a} \ii \prop{b}}{
		\infer*[]{\Gamma, \prop{a} \vdash \prop{b}}{
			\infer[]{\Gamma, \prop{a} \dots \vdash \prop{b}}{
					\infer*[s]{}{\dots & \infer[\Ax]{\prop{a} \vdash \prop{a}}{}}
				}
			}
		}
		&
		\infer{\Delta \vdash \prop{a}}{
			\infer*[t]{}{}
		}
	}$
	&
	\raisebox{20pt}{$\implies$}
	&
	$ 
	\infer*[]{\Gamma, \Delta \vdash \prop{b}}{
		\infer[]{\Gamma, \Delta \dots \vdash \prop{b}}{
			\infer*[s]{}{
			\dots &
			\infer[]{\Delta \vdash A}{
				\infer*[t]{}{}
			}}
		}
	}
	$\\[\smallsep]
	$
	\infer[\times E]{\Gamma, \Delta, \Theta \vdash \prop{c}}{
		\infer[\times I]{\Gamma, \Delta \vdash \prop{a} \times \prop{b}}{
			\infer[]{\Gamma \vdash \prop{a}}{\infer*[s]{}{}}
			&
			\infer[]{\Delta \vdash \prop{b}}{\infer*[t]{}{}}
		}		
		&
		\hspace{-15pt}
		\infer*[]{\Theta, \prop{a}, \prop{b} \vdash \prop{c}}{
			\infer[]{\Theta, \prop{a}, \prop{b}\dots \vdash \prop{c}}{
				\infer*[u]{}{
					\infer[\Ax]{\prop{a} \vdash \prop{a}}{
					}
					&
					\dots
					&
					\infer[\Ax]{\prop{b} \vdash \prop{b}}{}
				}
			}
		}
	}
	$
	&
	\raisebox{20pt}{$\implies$}
	&
	$
	\infer*[]{\Gamma, \Delta, \Theta \vdash \prop{c}}{
		\infer[]{\Gamma, \Delta, \Theta \dots \vdash \prop{c}}{
			\infer*[u]{}{
				\infer[]{\Gamma \vdash \prop{a}}{
					\infer*[s]{}{}
				}
				&
				\dots 
				&
				\infer[]{\Delta \vdash \prop{b}}{
					\infer*[t]{}{}
				}
			}
		}
	} 
	$
	\end{tabularx}	\caption{Intuitionistic $\beta$ redexes.}
	\label{figure:intuitionistic_proof_reduction}
\end{figure}


\subsection{The Curry-Howard Correspondence}
The Curry-Howard correspondence asserts an equivalence between the above presentation of the logic in natural deduction, and a system of computation known as the $\lam$-calculus.
It was first formulated by Haskell Curry in the 30s before being independently rediscovered by William Alvin Howard and Nicolaas Govert de Bruijn in the 60s~\cite{curry1934functionality,de1983automath,howard1980formulae}.
The entry point for such an approach is to interpret propositions as \textit{types} of a minimal functional programming language (a perhaps more aptly named alternative to the Curry-Howard correspondence is the \textit{propositions-as-types interpretation}).
In that sense, the set of propositional constants \propcon{} becomes the programming language's basic set of \textit{primitive} types (think of them as built-ins).
Implicational formulas $\smallprop{a} \ii \smallprop{b}$ are read as \textit{function} types, and conjunction formulas are read as \textit{tuple} (or cartesian product) types.
From now we will use formulas, propositions and types interchangeably.
Following along the correspondence allows us to selectively speak about individual, named instances of propositions -- we call these \textit{terms}.
The simplest kind of term is a \textit{variable}, corresponding to a hypothesis in the proof tree.
Each logical rule is identified with a programming pattern: the axiom rule is variable \textit{instantiation}, introduction rules are \textit{constructors} of complex types, and elimination rules are their \textit{destructors}.
The question of whether a logical proposition is provable translates to the question of whether the corresponding type is inhabited; i.e. whether an object of such a type can be created -- we will refer to the latter as a \textit{well-formed} term.

Rather than present a grammar of terms and later ground it in the logic, we will instead simply revisit the rules we established just above, now adorning each with a term rewrite instruction -- the result is a tiny yet still elegant and expressive type theory, presented in Figure~\ref{figure:simple_type_theory}.
Given an infinite but enumerable set $\vars$ consisting of (unique names for) indexed variables with elements \{\term{\vari, \varj, \vark, \varl, \dots}\}, and denoting arbitrary but well-formed terms with \smallterm{s}, \smallterm{t}, \smallterm{u}, we will use $\smallterm{s}: \smallprop{a}$ (or $\smallterm{s}^{\smallprop{A}}$) to indicate that term \smallterm{s} is of type \smallprop{a}.
Assumptions $\Gamma$, $\Delta$ will now denote a \textit{typing environment}:
\begin{equation}
\varj : \prop a_1 ,~  \vark : \prop a_2 \dots ~ \Var{n} : \prop a_n
\end{equation}
i.e. rather than a sequence of formulas, we have a sequence of distinct variables, each of a specific type, and a judgement $\Gamma \vdash \term{s} : \prop{b}$ will now denote the derivation of a term \term{s} of type \prop{b} out of such an environment.

\begin{figure}
	\centering
	\begin{tabularx}{0.975\textwidth}{@{}C@{~}C@{}}
		\multicolumn{2}{@{}c@{}}{$\infer[\Ax]{\term{\vari } : \prop a \vdash \term{\vari } : \prop{a}}{}$}\\[2em]
		$\infer[\ii E]{\Gamma, \Delta \vdash \term{s~t} : \prop b}{\Gamma \vdash \term{s} : \prop{a} \ii \prop{b} & \Delta \vdash \term{t} : \prop{b}}$ 
		& 
		$\infer[\ii I]{\Gamma \vdash \term{\lam \vari .s} : \prop a \ii \prop{b}}{\Gamma, \term{\vari }: \prop{a} \vdash \term{s}:\prop{b}}$\\[2em]
		$\infer[\times E]
			{\Gamma,\Delta \vdash
			\caseof{s}{\vari}{t}: \prop{c}}
			{\Gamma \vdash \term{s}: \prop{a} \times \prop{b} & 
			\Delta, \term{\vari }: \prop{a} , \term{\varj}: \prop{b} \vdash \term{t}: \prop{c}}$ 
		&
		$\infer[\times I]
			{\Gamma, \Delta \vdash \cterm{(\term{s}, \term{t})}: \prop{a} \times \prop{b}}
			{\Gamma \vdash \term{s}: \prop{a} & \Delta \vdash \term{t}:\prop{b}}$\\[\smallsep]
		\multicolumn{2}{@{}c@{}}{$\infer[\Exchange{}]{\Delta, \Gamma \vdash \term{s}: \prop{a}}{\Gamma, \Delta \vdash \term{s}: \prop{a}}$}\\[2em]
		$\infer[\Weakening{}]
			{\Gamma, \term{\vari }: \prop{a} \vdash \term{s}: \prop{b}}
			{\Gamma \vdash \term{s}:\prop{b}} $ 
		&
		$\infer[\Contraction{}]
			{\Gamma, \term{\vark}: \prop{a} \vdash \term{s}_{[\term{\vari } \mapsto \term{\vark}, \term{\varj} \mapsto \term{\vark}]}:\prop{b}}
			{\Gamma,  \term{\vari }: \prop{a}, \term{\varj}: \prop{a} \vdash \term{s}: \prop{b}}$
		\end{tabularx}
	\caption{Simple type theory.}
	\label{figure:simple_type_theory}
\end{figure}

Inspecting Figure~\ref{figure:simple_type_theory}, things for the most part look good.
The implication elimination rule $\ii E$ provides us with a composite term $\term{s~t}$ that denotes the function application of \textit{functor} \smallterm{s} on \textit{argument} \smallterm{t}.
Function application is left-associative: $\term{s~t~u}$ is the bracket-economic presentation of $(\term{s~t})~\term{u}$ -- we have no choice but to use brackets if want to instead denote $\term{s}~(\term{t~u})$.
The dual rule, $\ii I$, allows us to create (so-called anonymous) functions by deriving a result \smallterm{s} dependent on some hypothesized argument \term{\vari } which is then \textit{abstracted} over as \smallterm{\lam \vari .s}.
Any occurrence of \smallterm{\vari } within \term{s} is then \textit{bound} by the abstraction; variables that do not have a binding abstraction are called \textit{free}.
The conjunction introduction $\times I$ allows us to create tuple objects \smallterm{(s, t)} through their parts \smallterm{s} and \smallterm{t}.
Its dual, $\times E$, gives us the option to identify the two coordinates of a tuple \smallterm{s} with variables \smallterm{\vari } and \smallterm{\varj}, when the latter are hypothesized assumptions for deriving some program \smallterm{t}.
If our assumptions are not in order, blocking the applicability of some rule, we can put them back where they belong with $\Exchange{}$.
With $\Contraction{}$ we can pretend to be using two different instances \smallterm{\vari } and \smallterm{\varj} of the same type before identifying the two as a single object \term{\vari } in term \term{s} with term \term{t}'');
note here the meta-notation for \textit{variable substitution}, $\smallterm{s}_{[\smallterm{\vari } \mapsto \smallterm{t}]}$, which reads as ``replace any occurrence of variable \term{\vari }.
And finally, we can introduce throwaway variables into our typing environment with $\Weakening{}$ (arguably useful for creating things like constant functions).

There's just a few catches to beware of.
The first has to do with tracing variables in a proof; the concatenation of structures $\Gamma$, $\Delta$ is only valid if $\Gamma$ and $\Delta$ contain no variables of the same name; if that were to be the case, we would be dealing with variable shadowing, a situation where the same name could ambiguously refer to two distinct objects (a horrible thing).
The second has to do with do with the $\Exchange{}$ rule. 
The careful reader might notice that the rule leaves no imprint on the term level, meaning we cannot distinguish between a program where variables were \textit{a priori} provided in the correct order, and one where they were shuffled into position later on.
This is justifiable if one is to treat the rule as a syntactic bureaucracy that has no real semantic effect, i.e. if we consider the two proofs as equivalent, following along the commuting conversions mentioned earlier (supporting the idea that in this type theory, asssumptions are multisets rather than sequences).
A slightly more perverse problem arises out of the product elimination rule $\times E$.
The rule posits that two assumptions $\smallterm{\vari }: \smallprop{A}$ and $\smallterm{\varj}: \smallprop{B}$ can be substituted by a single (derived) term of their product type $\smallterm{s}: \smallprop{A}\times\smallprop{B}$. 
Choosing different depths within the proof tree upon which to perform this substitution will yield distinct terms (because indeed they represent distinct sequences of computation); whether there's any merit in distinguishing between the two is, however, debatable.
Finally, whereas other rules can be read as syntactic operations on terms, (this presentation of) the $\Contraction{}$ rule contains meta-notation that is not part of the term syntax itself.
That is to say, $\smallterm{s}_{[\smallterm{\vari } \mapsto \smallterm{t}]}$ is \textit{not} a valid term -- even if the result of the operation it denotes is.
Generally speaking, substitution of objects for others of the same type is (modulo variable shadowing) an admissible property of the type system.
Mixing syntax and meta-syntax in the same system is a dirty but useful trick people sporadically employ; this surely invites some trouble, but conscious use of it can be worth it, since it significantly simplifies presentation.

\subsubsection{Term Equivalences}
There exist three kinds of equivalence relations between terms, each given an identifying Greek letter.\footnote{The denotational significance of these letters I have yet to understand -- legend has it that it only starts making sense after having written your 10th compiler from scratch.}

\paragraph{$\alpha$ conversion} is a semantically null rewrite obtained by renaming variables according to the substitution meta-notation $\smallterm{s}_{[\smallterm{\vari } \mapsto \smallterm{\varj}]}$ described above. 
Despite seeming innocuous at a first glance, $\alpha$ conversion is an evil and dangerous operation that needs to be applied with extreme caution so as to avoid variable capture, i.e. substituting a variable's name with one that is already in use.  
Two terms are $\alpha$ equivalent if we can rewrite one into the other using just $\alpha$ conversions, e.g.
\begin{equation}
	\lam \vari .\vari ^{\smallprop{A}} \overset{\alpha}{\equiv} \lam \varj.\varj^{\smallprop{A}}
\end{equation}
Standardizing variable naming, e.g. according to the distance between variables and their respective binders, alleviates the effort required to check for $\alpha$ equivalence by casting it to simple syntactic equality (string matching).

\paragraph{$\beta$ reduction}
The term rewrites we have so far inspected were either provided by specific rules, or were notational overhead due to the denominational ambiguity of variables.
Aside from the above, our type system provides two minimal computation steps that tell us how to reduce expressions that involve the deconstruction of a just-constructed type:
\begin{align}
(\term{\lam \vari .s})~\term{t} & \overset{\beta}{\leadsto} \term{s}_{[\term{\vari } \mapsto \term{t}]}\\
\caseof{(s, t)}{(\vari, \varj)}{u} & \bred \term{u_{[\term{s} \mapsto \term{\vari }, \term{t} \mapsto \term{\varj}]}}
\end{align}

A term on which no $\beta$ reductions can be applied is said to be in $\beta$-normal form.
The Church-Rosser theorem asserts first that one such form exists for all well-formed terms, and second, that this form is inevitable and inescapable -- any reduction strategy followed to the end will bring us to it~\cite{barendregt1984lambda}.
Two terms are $\beta$ equivalent to one another if they both reduce to the same $\beta$-normal form.

If you are at this point getting a feeling of deja vu, rest assured this is not on you; we have indeed gone through this before, last time around with proofs rather than terms.
If one were to replicate the above term reductions with their corresponding proofs, they would end up exactly with the proof reduction patterns of Figure~\ref{figure:intuitionistic_proof_reduction}.
I will spare you the theatrics of faking surprise at this fact, but if this not something you were exposed to previously, take a moment here to marvel at the realization that proof normalization is in reality ``just'' computation.
This ground-shattering discovery lies at the essence of the Curry-Howard correspondence.

\paragraph{$\eta$ conversion}
In contrast to $\beta$ conversion, which tells us how to simplify an introduce-then-eliminate pattern, $\eta$ conversion tells us how to modify an eliminate-then-introduce pattern.
An $\eta$ long (or normal) form of a term is one in which the arguments to type operators are made explicit (i.e. all introductions of a connective are preceded by its elimination), whereas an $\eta$ contracted (or pointfree) form is one where arguments are kept hidden~\cite{prawitz1965proof}.
We refer to the simplification of an expanded form as $\eta$ reduction, which is the computational dual of $\beta$ reduction; 
the reverse process is an $\eta$ expansion.
Both directions are facets of $\eta$ conversion -- the equivalence relation enacted by this conversion is called $\eta$ equivalence.
\begin{align}
	\term{\lam \vari .s~\vari } & \overset{\eta}{\equiv} \term{s}\\
	(\caseof{s}{(\vari, \varj)}{\vari}, \caseof{s}{(\vark,\varl)}{\varl}) & \overset{\eta}{\equiv} \term{s}
\end{align}


\subsubsection{In Place of a Summary}
Table~\ref{table:CHC} summarizes the subsection.

\begin{table}
	\centering
	\begin{tabularx}{0.8\textwidth}{@{}CC@{}}
	\textbf{Logic}			& \textbf{Computer Science}\\
	\toprule
	Propositional Constant	& Base Type\\
	Logical Connectives 	& Type Constructors\\
	Implication				& Function Type\\
	Conjunction				& Product Type\\
	Axiom					& Variable\\
	Introduction Rule		& Constructor Pattern\\
	Elimination Rule		& Destructor Pattern\\
	Proof Normalization		& Computation\\
	Provability				& Type Inhabitation	 
	\end{tabularx}
	\caption{The Curry-Howard correspondence in tabular form.}
	\label{table:CHC}
\end{table}

\subsection{Intermezzo}
\label{subsection:utility_definitions}
We now know how to prove things (or compute with types). 
Before moving along with this chapter's agenda, we will take a brief pause to provide some auxiliary definitions and notations that should prove relevant later on.
This is also a chance to do a bit of warming up with some baby examples before some real world proofs start coming our way.


\paragraph{Formula Polarity}
Each unique occurrence of (part of) a formula within a judgement can be assigned a polarity value, positive or negative.
All antecedent formulas are positive, and the lone succdent formula right is negative.
Complex formulas propagate polarities to their constituents depending on their own polarity and primary connective -- this way, all subformulas down to propositional constants are polarized.
Conjunctive formulas propagate their polarity unchanged to both their coordinates, whereas implicative formulas flip their polarity for the constituent left of the arrow; see Table~\ref{table:polarity_induction}.
Intuitively, we can think of negative formulas as being in argument position (conditions for the proof to proceed), and positive formulas as being in result position (conditionally provable statements).
The two judgements of the next paragraph have their subformulas annotated with a superscript denoting their polarity, for illustrative purposes.

\begin{table}
	\centering
	\begin{tabularx}{0.6\textwidth}{@{}XX@{\qquad}cc@{}}
	\multicolumn{2}{@{}l}{Complex formula /}	&	\multicolumn{2}{@{}l}{Constituent polarity}\\
	\multicolumn{2}{@{}c}{of polarity}		& 	\prop{a} & \prop{b}\\
	\toprule
	\multirow{2}{*}{$\prop{a} \times \prop{b}$} 	& $+$ & $+$ & $+$\\
													& $-$ & $-$ & $-$\\[0.5em]
	\multirow{2}{*}{$\prop{a} \ii \prop{b}$}		& $+$ & $-$ & $+$\\
													& $-$ & $+$ & $-$
	\end{tabularx}
	\caption{Polarity induction.}
	\label{table:polarity_induction}
\end{table}


\paragraph{Type Raising}
Type raising $\pprop{A} \vdash (\nprop{A} \ii \pprop{B}) \ii \nprop{B}$ is a derivable theorem of intuitionistic logic presented in Figure~\ref{figure:type_raising}. 
It states that for \smallprop{A}, \smallprop{B} arbitrary propositions, from \smallprop{A} one can derive its raised form $\smallprop{A} \ii \smallprop{B}$.
The converse, i.e. type lowering, is not true: $(\pprop{A} \ii \nprop{B}) \ii \pprop{B} \nvdash \nprop{A}$.
\begin{figure}
	\[
		\infer[\ii I]{\varj: \prop{a} \vdash \term{\lam \vari.\vari~\varj}: (\prop{a} \ii \prop{b}) \ii \prop{b}}{
			\infer[\ii E]{\term{\vari}: \prop{a} \ii \prop{b}, \term{\varj}: \prop{a} \vdash \term{\vari~\varj}: \prop{b}}{
				\infer[\Ax]{\term{\vari}: \prop{a} \ii \prop{b} \vdash \term{\vari}: \prop{a} \ii \prop{b}}{}
				&
				\infer[\Ax]{\term{\varj}: \prop{a} \vdash \term{\varj}: \prop{a}}{}
			}
		}
	\]
	\caption{Type raising.}
	\label{figure:type_raising}
\end{figure}

\paragraph{Function Order} The implication-only fragment of the logic includes $\ii$ as its sole logical connective.
The resulting type theory is one that deals only with functions; for its types, we can define their order $\mathcal{O}$ as follows:
\begin{equation}
	\begin{aligned}
	\mathcal{O}(p \in \propcon{}) & := 0 \\
	\mathcal{O}(\prop{a} \ii \prop{b}) & := \mathrm{max}(\mathcal{O}(\prop{a}) + 1, \mathcal{O}(\prop{b}))
	\end{aligned}
\end{equation}
Types whose order is above 1 are called \textit{higher-order} types; they denote functions that accept functions as their arguments.
For instance, for $p$ and $s$ atomic propositions of order 0, their respective identity functions $p\ii p$ and $s\ii s$ are of order 1, and the raised form of $p$ into $s$, i.e. $(p\ii s)\ii s$, is of order 2.

\paragraph{Notational Shorthands}
The verbosity of term-decorated proofs can get cumbersome in the long run, and does not play well with the unforgiving horizontal margins enforced by the template imposed on writer and reader alike.
It is probably inevitable that at some point proofs will need a smaller font size to fit in a page (or, worse yet, some neck-breaking rotations of the orientation plane), but in a futile attempt to postpone such emergency measures, we will occasionally make use of a shorthand notation for natural deduction proofs that avoids repetition, at the cost of maybe requiring some extra time to visually parse.
In this notation, axioms will be rewritten as follows:
\[
\infer[\Ax]{\term{\vari }: \prop{a} \vdash \term{\vari }: \prop{a}}{} \quad \raisebox{7pt}{$=:$} \quad \infer[\Ax]{\term{\vari }:\prop{a}}{}
\]
And assumptions will appear without type assignments (if uncertain of what some variable's type is, just trace it back to its axiom). 
We will always provide type declarations for derived terms (right of the turnstile).
The examples of the next paragraph (and many of the ones to follow) will use this alternative notation.

\paragraph{Currying}
A product type occurring in the argument position of an implication is interderivable with a longer implication where its coordinates are sequentialized: $(\smallprop{A}\times\smallprop{B}) \ii \smallprop{C} \dashv \vdash \smallprop{A} \ii \smallprop{B} \ii \smallprop{C}$.
The forward direction is called currying, and the backward uncurrying; you can find a proof for each in Figure~\ref{figure:curry_uncurry}.
Having proven that once, we can reuse that proof for deriving implicational equivalents from conjunctions (including nested ones, provided they occur as arguments to an implication).
Combined with type raising, this trick is interesting, as it permits us to indirectly argue about product types as higher-order implications, even in presentations of the theory that do not include an explicit product (and thus avoid the issues related to its elimination), e.g. we have:
\begin{equation}
	\prop{a} \times \prop{b} \vdash ((\prop{a} \times \prop{b}) \ii \prop{c}) \ii \prop{c} \dashv \vdash (\prop{a} \ii \prop{b} \ii \prop{c}) \ii \prop{c}
\end{equation}
Keep a mental note.

\begin{figure}
	\centering
	\begin{subfigure}{1\textwidth}
		\[
		\infer[\ii I (x2)]{\vari \vdash \lam \varj\vark.\vari~(\varj, \vark): \prop{a} \ii \prop{b} \ii \prop{c}}{
			\infer[\ii E]{\vari, \varj, \vark \vdash \vari~(\varj, \vark): \prop{c}}{
				\infer[\Ax]{\vari: (\prop{a} \times \prop{b}) \ii \prop{c}}{}
				&
				\infer[\times I]{\varj, \vark \vdash (\varj, \vark): \prop{a} \times \prop{b}}{
					\infer[\Ax]{\varj: \prop{a}}{}
					&			
					\infer[\Ax]{\vark: \prop{b}}{}
				}
			}
		}
		\]
		\caption{Currying}
		\label{subfigure:currying}
	\end{subfigure}\\[\midsep]
	\begin{subfigure}{1\textwidth}
		\[
			\infer[\ii I]{\varj \vdash \term{\lam \vari.}\caseof{\vari}{(\varj,\vark)}{\varj~\vark~\varl}:(\prop{a} \times \prop{b})\ii \prop{c}}{
				\infer[\Exchange{}]{\varj, \vari  \vdash \caseof{\vari}{(\varj,\vark)}{\varj~\vark~\varl}:\prop{c}}{
					\infer[\times E]{\vari, \varj \vdash \caseof{\vari}{(\varj,\vark)}{\varj~\vark~\varl}:\prop{c}}{
						\infer[\Ax]{\vari: \prop{a} \times \prop{b}}{}
						&
						\infer[\ii E]{\varj, \vark, \varl \vdash \term{\varj~\vark~x_3}: \prop{c}}{
							\infer[\ii E]{\varj, \vark \vdash \term{\varj~\vark}: \prop{b} \ii \prop{c}}{
								\infer[\Ax]{\varj: \prop{a} \ii \prop{b} \ii \prop{c}}{}
								&
								\infer[\Ax]{\vark: \prop{a}}{}
							}
							&
							\infer[\Ax]{\varl: \prop{b}}{}
						}
					}
				}
			}
		\]
		\caption{Uncurrying}
		\label{subfigure:uncurrying}
	\end{subfigure}
	\caption{Interderivability of product and arrow.}
	\label{figure:curry_uncurry}
\end{figure}

\paragraph{Proof Search}
Attempting to derive a judgement of the form $\Gamma \vdash \smallprop{A}$ amounts to searching for a suitable proof of that statement, a process called \textit{proof search}.
We distinguish two directions of proof search: the \textit{backward chaining} (or top-down) approach starts from the goal judgement and iteratively expands it into judgements with smaller assumptions -- one judgement per premise generated by the rule of inference applied -- with the intention being the eventual deconstruction of all branches into axioms of identity.
The other direction is called \textit{forward chaining} (or bottom-up), and starts from a collection of hypothesized propositions (axioms) that are glued together to form progressively more complex structures, until the goal judgement is reached.
Without digressing further, it is important to realize that both directions suffer the same issue, albeit from different angles, namely hypothetical reasoning.
Forward chaining requires a perfect guess of any and all propositions reqired in deriving $\smallprop{A}$ from $\Gamma$, even those that will be redacted and thus never occur in $\Gamma$.
Dually, backward chaining might require introduction of substructures and subformulas that are nowhere to be found in either the antecedents or the succedent of the current judgement due to the modus ponens-like behavior of implication elimination.
Long story short, proof search is hard.

\section{Going Linear}\label{section:linear_type_theory}
We are now ready to start charting grounds in substructural territories: we will gradually impoverish our logic by removing structural rules one by one, and see where that gets us. 
The weakest links are the $\Contraction{}$ and $\Weakening{}$ rules.
These two rules are a cultural and ideological remnant of a long-gone age infested by delusions of prosperity and abundance.
In their presence, propositions are proof objects that can be freely replicated and discarded.
Removing them (or controlling their applicability via other means) directs us towards a more eco-conscious regime by turning propositions into finite resources, the production and/or consumption of which is not to be taken for granted.
Removing $\Contraction{}$ yields Affine Logic, a logic in which resources can be used no more than once.
Removing $\Weakening{}$ yields Relevant Logic, a logic in which resources can be used no less than once.
Removing both yields Linear Logic, a logic in which resources can be used \textit{exactly} once.
The intuitionistic formulations of the above give rise to corresponding type theories~\cite{pierce2004advanced}.
For the purposes of this manuscript, we will focus our presentation on linear type theory.

\subsection{Linear Types}
Linear logic is due to Jean-Yves Girard~\cite{girard1987linear}, and its computational interpretation due to Samson Ambramsky~\cite{abramsky1993computational}.
The full logic includes two disjunctive connectives as well as a modality that allows one to incorporate non-linear propositions into the presentation, but we will happily forget about those.
Note that with these missing connectives included, the logic is not impoverished but rather enhanced -- full linear logic in fact subsumes intuitionistic logic; we have no use of this much expressivity here though.
Insights from the previous section carry over to this one; we will no longer seperate the presentation between the logic and the type theory, but instead do both in one go.

For the fragment of interest to us, the type grammar becomes:
\begin{equation}
\prop{a}, \prop{b}, \prop{c} := p \ | \ \prop{a} \li \prop{b} \ | \ \prop{a} \otimes \prop{b} \ | \ \prop{a} \with \prop{b} 
\label{equation:ILL}
\end{equation}
There is not really much we have to do to manipulate these new types, other than a slight cognitive rewiring.
We will note first that the meaning of the implication arrow changes from material implication to transformation process; i.e. where we previously had $\smallprop{A} \ii \smallprop{B}$ to denote that \smallprop{B} logically follows from \smallprop{A}, we will now have $\smallprop{A} \li \smallprop{B}$ to denote an irreversible process that transforms a single \smallprop{A} into a single \smallprop{B}, consuming the former in the process (we can think of this as a perfect chemical reaction).
The new, weird-looking arrow of linear implication is read as lolli(pop) due to its suggestive appearance.\footnote{If trying to typeset it yourself, DO NOT duckduckgo for ``lolli in latex''. It can be found as \texttt{\textbackslash multimap}. You are welcome.}
Conjunction $\times$ is now separated into two distinct operators, the multiplicative $\otimes$ and the additive $\with$. 
The first denotes a linear tuple, and $\smallprop{A} \otimes \smallprop{B}$ is read as \textit{both} \prop{a} \textit{and} \prop{b}.
A linear tuple offers no possibility of projection: we will need to use both coordinates going forward.
The second denotes a choice, and $\smallprop{A} \with \smallprop{B}$ is read as \smallprop{A} \textit{with} \smallprop{B}, or \textit{choose one of}\smallprop{A} \textit{or} \smallprop{B}.
This choice is \textit{external}, as the freedom of applying it lies with the operator rather than the proof, and is manifested by the presence of two eliminators for our new connective: a left projection $\with E_1$ and a right projection $\with E_2$; choosing one means we lose the possibility of obtaining the other.
Unique to the $\with I$ rule is the fact that two proof branches used to derive each coordinate of the $\smallprop{A} \with \smallprop{B}$ conclusion share the same assumptions $\Gamma$.
The subset of linear logic concerning the connectives discussed is presented in Figure~\ref{figure:linear_logic_rules}, together with its term rewrites (assumptions, judgements, rules and proofs look just like before).
For the sake of homogeneity and explicitness, \Exchange{} still makes an appearance as the sole structural rule.

Notationally, the absence of non-linear intuitionistic terms allows us to freely reuse our prior term notation without fear of ambiguity.
We have three new term patterns: $\langle \smallterm{s}, \smallterm{t} \rangle$ to denote the choice between proof terms $\smallterm{s}$ and $\smallterm{t}$ (contrast to the linear tuple $(\smallterm{s}, \smallterm{t})$), and $\cterm{fst}(\_)$ and $\cterm{snd}(\_)$ to denote the first and second projections.
Similarly for the implication introduction rule $\li I$, no redundant variables means that \smallterm{\vari } must now appear free once in the abstraction body \smallterm{s} -- in other words, we have no way of syntactically instantiating constant functions.

\begin{figure}
	\centering
	\begin{subfigure}{1\textwidth}
		\centering
		\begin{tabularx}{0.95\textwidth}{@{}C@{~}C@{\qquad}C@{}}
		\multicolumn{3}{@{}c@{}}{$\infer[\Ax]{\term{\vari }: \prop{a} \vdash \term{\vari }: \prop{a}}{}$}\\[2em]
		\multicolumn{2}{@{}c}{
		$\infer[\li E]{\Gamma, \Delta \vdash \term{s~t}: \prop{b}}{\Gamma \vdash \term{s}: \prop{a} \li \prop{b} & \Delta \vdash \term{t}: \prop{a}}$}
		& 
		$\infer[\li I]{\Gamma \vdash \term{\lam \vari .s}: \prop{a} \li \prop{b}}{\Gamma, \term{\vari }: \prop{a} \vdash \term{s}: \prop{b}}$\\[\smallsep]
		\multicolumn{2}{@{}c}{
		$\infer[\otimes E]{\Gamma,\Delta \vdash \cterm{case \smallterm{s} of (\smallterm{\vari }, \smallterm{\varj}) in \smallterm{t}}: \prop C}{\Gamma \vdash \term{s}: \prop{a} \otimes \prop{b} & \Delta, \term{\vari }: \prop{a} , \term{\varj}: \prop{b} \vdash \term{t}: \prop C}$}
		&
		$\infer[\otimes I]{\Gamma, \Delta \vdash (\term{s}, \term{t}): \prop{a} \otimes \prop{b}}{\Gamma \vdash \term{s}: \prop{a} & \Delta \vdash \term{t}: \prop{b}}$\\[\smallsep]
		$
		\infer[\with E_1]{\Gamma \vdash \cterm{fst(\smallterm{s})}: \prop{a}}{\Gamma \vdash \smallterm{s}: \prop{a} \with \prop{b}}
		$
		&
		$
		\infer[\with E_2]{\Gamma \vdash \cterm{snd(\smallterm{s})}: \prop{b}}{\Gamma \vdash \term{s}: \prop{a} \with \prop{b}}
		$
		&
		$
		\infer[\with I]{\Gamma \vdash \langle\term{s}, \term{t}\rangle: \prop{a} \with \prop{b}}{\Gamma \vdash \term{s}: \prop{a} & \Gamma \vdash \term{t}: \prop{b}}
		$
		\end{tabularx}
		\caption{Logical rules.}
		\label{subfigure:linear_logic_rules:logical}
	\end{subfigure}\\[\midsep]
	\begin{subfigure}{1\textwidth}
		\centering
		\begin{tabularx}{0.7\textwidth}{@{}C@{}}
		$
		\infer[\Exchange{}]{\Delta, \Gamma \vdash \term{s}: \prop{a}}{\Gamma, \Delta \vdash \term{s}:  \prop{a}}
		$
		\end{tabularx}
		\caption{Structural rule.}
		\label{subfigure:linear_logic_rules:structural}
	\end{subfigure}
	\caption{Linear Logic $\ILL_{\li,\otimes,\with}$ and its type theory.}
	\label{figure:linear_logic_rules}
\end{figure}

\subsubsection{Proof \& Term Reductions}
The notions of proof and term equivalence discussed in the previous section hold also for linear logic.
Proof normalization looks almost identical to before in the case of $\ii$ and $\times$ (substituting of course for $\li$ and $\otimes$).
The key difference lies in the absence of horizontal dots and unnamed vertical dots (since the \Contraction{} rule is no more, meaning that there can only be a single occurrence of each axiom replaced with a proof).
The extra connective $\with$ introduces its own two redexes (one per eliminator); the reduction of the first projection is shown in Figure~\ref{figure:linear_proof_reductions}. 
Its reading is straightforward: if one were to use $\Gamma$ to independently derive \smallprop{A} and \smallprop{B} along two parallel proofs, proceed by constructing a choice between the two, and then also make that choice (favoring either), there was never any need for the other in the first place.
The equivalent term reduction steps this time around are:
\begin{align}
(\term{\lam \vari .s})~\term{t} & \bred \term{s}_{[\term{\vari } \mapsto \term{t}]}\\
\cterm{case (\smallterm{s}, \smallterm{t}) of (\smallterm{\vari }, \smallterm{\varj}) in \smallterm{u}} & \bred \term{u_{[\term{s} \mapsto \term{\vari }, \term{t} \mapsto \term{\varj}]}}\\
\cterm{fst}(\langle\term{s},\term{t}\rangle) & \bred \term{s}\\
\cterm{snd}(\langle\term{s},\term{t}\rangle) & \bred \term{t}
\end{align}


\begin{figure}
	\centering
	\begin{tabularx}{0.95\textwidth}{@{}ccc@{}}
	$\infer[\li E]{\Gamma, \Delta \vdash \prop{b}}{
	\infer[\li I]{\Gamma \vdash \prop{a} \li \prop{b}}{
		\infer*[s]{\Gamma, \prop{a} \vdash \prop{b}}{
				\infer[\Ax]{\prop{a} \vdash \prop{a}}{} 
			}
		}
		&
		\infer{\Delta \vdash \prop{a}}{
			\infer*[t]{}{}
		}
	}$
	&
	\raisebox{20pt}{$\implies$}
	&
	$ 
	\infer[]{\Gamma, \Delta \vdash \prop{b}}{
		\infer*[s]{}{
		\infer[]{\Delta \vdash A}{
			\infer*[t]{}{}
		}}
	}
	$\\[\smallsep]
	$
	\infer[\otimes E]{\Gamma, \Delta, \Theta \vdash \prop{c}}{
		\infer[\otimes I]{\Gamma, \Delta \vdash \prop{a} \otimes \prop{b}}{
			\infer[]{\Gamma \vdash \prop{a}}{\infer*[s]{}{}}
			&
			\infer[]{\Delta \vdash \prop{b}}{\infer*[t]{}{}}
		}		
		&
		\infer*[u]{\Theta, \prop{a}, \prop{b} \vdash \prop{c}}{
			\infer[\Ax]{\prop{a} \vdash \prop{a}}{}
			& 
			\infer[\Ax]{\prop{b} \vdash \prop{b}}{}
		}
	}
	$
	&
	\raisebox{20pt}{$\implies$}
	&
	$
	\infer*[s]{\Gamma, \Delta, \Theta \vdash \prop{c}}{
		\infer[]{\Gamma \vdash \prop{a}}{
			\infer*[t]{}{}
		}
		&
		\infer[]{\Delta \vdash \prop{b}}{
			\infer*[u]{}{}
		}
	}
	$\\[\smallsep]
	$
	\infer[\with E_1]{\Gamma \vdash \prop{a}}{
		\infer[\with I]{\Gamma \vdash \prop{a} \with \prop{b}}{
			\infer*[s]{\Gamma \vdash \prop{a}}{}
			&
			\infer*[t]{\Gamma \vdash \prop{b}}{}
		}
	}
	$
	&
	\raisebox{10pt}{$\implies$}
	&
	$
	\infer*[s]{\Gamma \vdash \prop{a}}{}
	$
	\end{tabularx}
	\caption{Linear $\beta$ redexes.}
	\label{figure:linear_proof_reductions}
\end{figure}


\subsection{Proof Nets}
We have basked in the beauty of the natural deduction presentation adopted so far, and seen how it gives rise to a straightforward computational interpretation.
We have also seen how it is at times overly bureaucratic in its explication of structural rules that are null from a computational perspective.
To our luck, an additional representation makes itself available as soon as we step into linear grounds, namely \textit{proof nets}, also due to Girard~\cite{girard1987linear}. 
Proof nets are best suited for the multiplicative fragment of the logic (they are amenable to extensions with additive connectives, but things get uglier there).
In our case, we have foregone the disjunctive connectives, and we have already suggested that the multiplicative conjunction $\otimes$ is (to an extent) interchangeable with the implication arrow $\li$ (we actually did this for intuitionistic connectives, but there is no need to repeat ourselves -- the story looks identical with their linear variants).
This gives us the much needed excuse to justify limiting our presenting of proof nets to the implication-only fragment of linear logic, $\ILL_{\li}$, where things are easy and intuitive.

In natural deduction, our proofs are built sequentially.
We start with some hypothesized variables and combine them via rules to derive more complex terms, which then serve as premises for a next iteration of rule applications.
As long as we are careful not to get stuck in some detour loop hell, we rinse and repeat, and eventually, after a finite number of steps, we end up with our conclusion, at which point we can call it a day.
Proof nets offer an appealing alternative: they are parallel, in the sense that they allow multiple conclusions to be derived simultaneously.
They also have no notion of temporal precedence: everything happens in a single instant, meaning that positive subformulas are good to use without having to wait for their conditions to be met.

To see this in practice, a simple but concrete example will prove helpful.
We will consider the natural deduction proof of linear function composition of Figure~\ref{figure:funcomp} and translate it into its proof net equivalent of Figure~\ref{figure:funcomppn}.

\begin{figure}
	\[
		\infer[\Exchange]{\term{\vari}, \term{\varj} \vdash \term{\lam \vark.\varj~(\vari~\vark)}: \prop{a} \li \prop{c}}{
			\infer[\li I]{\term{\varj}, \term{\vari} \vdash \term{\lam \vark.\varj~(\vari~\vark)}: \prop{a} \li \prop{c}}{
				\infer[\li E]{\term{\varj}, \term{\vari}, \term{\vark} \vdash \term{\vark~(\varj~\vari)}: \prop{a} \li \prop{c}}{
					\infer[\Ax]{\term{\varj}:\prop{b}\li\prop{c}}{}
					&
					\infer[\li E]{\term{\vari}, \term{\vark} \vdash \term{\vari~\vark}: \prop{b}}{
						\infer[\Ax]{\term{\vari}: \prop{a}\li\prop{b}}{}
						&
						\infer[\Ax]{\term{\vark}: \prop{a}}{}
					}
				}
			}
		}
	\]
	\caption{Linear function composition.}
	\label{figure:funcomp}
\end{figure}

The first thing we need to do is write formulas as their \textit{decomposition trees}; it sounds fancy, but in reality this is just recompiling formulas according to their underlying grammar, but now in tree form:
propositional constants become leaves, and logical connectives become branch nodes.
To make this a tiny bit more interesting, we will decorate atomic subformulas with a superscript denoting their polarity, according to the schema of Subsection~\ref{subsection:utility_definitions}.
For formula trees with positive roots (i.e. trees occuring left of the turnstile), we will put a label beneath them to identify them with the variable that provides them.
We will also distinguish tree edges originating from a negative implication and pointing to a positive tree by marking them with dashed lines -- these denote positively rooted trees that are either nested within a higher-order positive implication, or in the argument position of an implication right of the turnstile.%
\footnote{An alternative notation employs two distinct symbols for the positive and negative versions of an implication. In the literature, these can be encountered as tensor ($\otimes)$ and par ($\parlink$) links.}
Such positive formulas play the role of hypotheses that have been abstracted over, therefore we need to also give these a name; we can put it right next to the dashed line.
An arrangement of the decomposition trees of all formulas as they occur within a judgement is called a \textit{proof frame}; the one for our running example can be seen in Figure~\ref{subfigure:proof_frame_funcomp}.

\begin{figure}
	\centering
	\begin{subfigure}{1\textwidth}
		\centering
		\begin{tikzpicture}
		\node (A1)	at (-0.66, 1) {$\vnorm{\nprop{A}}$};
		\node (B1)	at (0.66, 1) {$\vnorm{\pprop{B}}$};
		\node (li) 	at (0, 0) {$\vnorm{\li}$};
		\draw (li) -- (A1);
		\draw (li) -- (B1);
		\node (x0)	at (0, -0.5)	{$\term{\vari}$};
		%%%
		\node (B2)	at (2.33, 1) {$\vnorm{\nprop{B}}$};
		\node (C1)	at (3.66, 1) {$\vnorm{\pprop{C}}$};
		\node (li2) 	at (3, 0) {$\vnorm{\li}$};
		\draw (li2) -- (B2);
		\draw (li2) -- (C1);
		\node (x1)	at (3, -0.5)	{$\term{\varj}$};
		%%%
		\node (A2)	at (5.33, 1) {$\vnorm{\pprop{A}}$};
		\node (C2)	at (6.66, 1) {$\vnorm{\nprop{C}}$};
		\node (li3) 	at (6, 0) {$\vnorm{\li}$};
		\draw[dashed] (li3) -- (A2) node[midway, left]{$\term{\vark}$};
		\draw (li3) -- (C2);
	\end{tikzpicture}
	\caption{Proof frame.}
	\label{subfigure:proof_frame_funcomp}
	\end{subfigure}\\[\midsep]
	\begin{subfigure}{1\textwidth}
		\centering
		\begin{tikzpicture}
		\node (A1)	at (-0.66, 1) {$\vnorm{\nprop{A}}$};
		\node (B1)	at (0.66, 1) {$\vnorm{\pprop{B}}$};
		\node (li) 	at (0, 0) {$\vnorm{\li}$};
		\draw (li) -- (A1);
		\draw (li) -- (B1);
		\node (x0)	at (0, -0.5)	{$\term{\vari}$};
		%%%
		\node (B2)	at (2.33, 1) {$\vnorm{\nprop{B}}$};
		\node (C1)	at (3.66, 1) {$\vnorm{\pprop{C}}$};
		\node (li2) 	at (3, 0) {$\vnorm{\li}$};
		\draw (li2) -- (B2);
		\draw (li2) -- (C1);
		\node (x1)	at (3, -0.5)	{$\term{\varj}$};
		%%%
		\node (A2)	at (5.33, 1) {$\vnorm{\pprop{A}}$};
		\node (C2)	at (6.66, 1) {$\vnorm{\nprop{C}}$};
		\node (li3) 	at (6, 0) {$\vnorm{\li}$};
		\draw[dashed] (li3) -- (A2) node[midway, left]{$\term{\vark}$};
		\draw (li3) -- (C2);
		\draw[->] (A1) -- ($(A1.north) + (0, 0.5)$) -- ($(A2.north) + (0, 0.5)$) -- (A2);
		\draw[->] (B2) -- ($(B2.north) + (0, 0.25)$) -- ($(B1.north) + (0, 0.25)$) -- (B1);
		\draw[->] (C2) -- ($(C2.north) + (0, 0.75)$) -- ($(C1.north) + (0, 0.75)$) -- (C1);
	\end{tikzpicture}
	\caption{Proof structure.}
	\label{subfigure:proof_structure_funcomp}
	\end{subfigure}
	\caption{Proof net construction for the proof of Figure~\ref{figure:funcomp}.}
	\label{figure:funcomppn}
\end{figure}

A proof frame must satisfy an invariance property before the eligibility of the judgement it prescribes can even be considered.
Namely, it must contain an equal number of positive and negative occurrences of each unique atomic formula that appears within it.
This perfectly fits the linear logic paradigm: everything produced has to be consumed, and vice versa.
In our case, we have three unique formulas, \smallprop{A}, \smallprop{B} and \smallprop{C}, each one of which has a single positive and a single negative occurrence: check.
The next thing to do in building a proof net is to establish a bijection between atomic formulas of opposite polarity, and draw it as an extra set of edges, pointed negative atoms to their positive matches: we call those \textit{axiom links}.
Axiom links essentially specify the elimination of function types: they identify function arguments with their concrete inputs in a geometric fashion.
We do not need to put much thought for our running example: since all atoms have a single occurrence of each polarity, there is just one possible bijection to consider, presented in Figure~\ref{subfigure:proof_structure_funcomp}.
A proof frame with axiom links on top is called a \textit{proof structure}, and this representation provides all of the information contained within a proof.

Alas, the relation from proofs structures is not one-to-one: there exist many more proof structures than proofs.
A proof structure is a \textit{proof net} if and only if it satisfies a correctness criterion.
There have been various formulations of this criterion, each with a different time complexity, ranging from exponential to linear~\cite{girard1987linear,danos1989structure,murawski2000dominator,guerrini2011linear}.
We will adopt an (informally rephrased) version of the acyclicity and connectedness criterion of \citet{danos1989structure}.
We are going to treat a proof structure as a heterogeneous graph, consisting of two node types, logical connectives and atomic formulas, and three edge types: tree-structure edges, further subcategorized according to their polarity, and axiom links.
We will then attempt a traversal of that graph using an algorithm defined on the basis of the above parameterization, that further utilizes the  ancillary tree labels we have assigned earlier.
At each step of the traversal, the algorithm will write down a (partial) term or term instruction.
We will expect the traversal to be terminating (i.e. to not get stuck in a loop) and complete without repeats (i.e. to have passed through all nodes and edges exactly once), and the term it has transcribed at its last step to be well-formed, at which point we will happily claim the proof structure to indeed be a proof net.

\paragraph{Traversing $\ILL_{\li}$ Nets}
Now, let's get our crayons out and sketch the outline of the traversal algorithm.
We will have two traversal modes: negative (or upward) mode and positive (or downward) mode.
In negative mode, we move upwards along negative nodes.
When encountering a negative implication, we will create a $\lam$ abstraction over the variable specified by its dashed edge.
When encountering a negative leaf, we will traverse across the axiom link to its positive counterpart and switch to positive mode.
In positive mode, we move downwards along positive nodes.
When we encounter a positive implication, we will add  its negative (upward) branch to our mental stack and proceed downwards.
Upon running out of positive nodes to visit, we will write down the variable label assigned to the positive tree's root, and then perform a negative traversal of each of the negative branches that live in our stack (i.e. we have encountered going down), in reverse order.
We will start from the root of the formula tree occurring right of the turnstile (we know which one that is by the fact it has no variable label underneath it) in negative mode.

In our case, this is a negative implication, the dashed line of which reads $\smallterm{\vark}$, so we start by writing down $\smallterm{\lam \vark. (\dots)}$. 
We move on to \nprop{C}, which is a leaf, so we cross over to \pprop{C} and switch to positive mode. Going down, we encounter an implication as the positive root, and write down the positive tree's name, getting $\smallterm{\lam \vark.\varj (\dots)}$.
We proceed in negatative mode to \nprop{B}, cross over to \pprop{B} and repeat the above, getting $\smallterm{\lam \vark.\varj~(\vari~\dots)}$ before going into negative mode again.
The final axiom link points us to \pprop{A}, which is its own root, named $\smallterm{\vark}$.
At this point, our traversal has transcribed the term $\smallterm{\lam \vark.\varj~(\vari~\vark)}$ and we have ran out of paths to explore.
By now, all our nodes and edges have been visited, and our final term is both well-formed and identical to the one prescribed by the natural deduction proof of Figure~\ref{figure:funcomp}: a joyful outcome!
If we consider ourselves bound to the permutation of assumptions dictated by the variable indices, the only thing we would need to do going backwards is guess the presence (and position) of the \Exchange{} rule.

\paragraph{Axiom Links and Where to Find Them}
It would be understandable if at this point we allowed ourselves a feeling of complacency; navigating a proof net is no small feat, after all.
But upon careful inspection, you might realize that you have been tricked.
There never was any room for error in transitioning from the proof frame of Figure~\ref{subfigure:proof_frame_funcomp} to the proof structure of Figure~\ref{subfigure:proof_structure_funcomp}. 
A rare and lucky coindidence, or perhaps the result of carefully planned concealment?
Regardless of what it might have been, we cannot reasonably anticipate there to always be a single possible set of axiom links, so we need a decision procedure that tells us how to actually extract them from a less forgiving proof.

\begin{figure}
	\centering
	\begin{tikzpicture}[
		   focus/.style={circle, draw, minimum size=18pt},
		   link/.style={thick, dashed}
	]
	\node[inner sep=0pt, outer sep=0pt] (bg) at (0,0) 
	{
	\textcolor{gray!80}{
	$
		\infer[\Exchange]{\nprop{A}_k\li\pprop{B}_l, \nprop{B}_i\li\pprop{C}_j \vdash \pprop{A}_m \li \nprop{C}_j}{
			\infer[\li I]{\nprop{B}_i\li\pprop{C}_j, \pprop{A}_k\li\pprop{B}_l \vdash  \pprop{A}_m \li \nprop{C}_j}{
				\infer[\li E (2)]{\nprop{B}_i\li\pprop{C}_j, \nprop{A}_k\li\pprop{B}_l, \pprop{A}_m \vdash \nprop{C}_j}{
					\infer[\Ax]{\nprop{B}_i\li\pprop{C}_j\vdash \pprop{B}_i \li \nprop{C}_j}{
					}
					&
					\infer[\li E (1)]{\nprop{A}_k\li\pprop{B}_l, \pprop{A}_m \vdash \nprop{B}_l}{
	  					\infer[\Ax]{\nprop{A}_k\li\pprop{B}_l \vdash \pprop{A}_k\li\nprop{B}_l}{	
						}
						&
						\infer[\Ax]{\pprop{A}_m \vdash \nprop{A}_m}{
						}
					}
				}
			}
		}
	$
	}
	};
	\node[focus] (ak) at (0.65, 1.2) {};
	\node[focus] (am) at (3.85	,  1.2) {};
	\draw[link, <-] (ak) to[bend left=45] (am); %$(am.north)$);
	\node[focus] (bl1) at	(1.55, 1.2) {};
	\node[focus] (bl2) at (2.6, 0.6) {};
	\draw[link, ->] (bl1) -- (bl2);
	\node[focus] (bi) at (-3.45, 0.6) {};
	\node[focus] (cj1) at (-2.55, 0.6) {};
	\node[focus] (cj2) at (0.7, -0.05) {};	
	\draw[link, <-] (bi) to[bend left=60] (bl2);
	\draw[link, ->] (cj1) -- (cj2);
	\end{tikzpicture}
	\caption{Extracting axiom links from the proof of Figure~\ref{figure:funcomp}.}
	\label{figure:funcomp_annotated}
\end{figure}

Let's revisit the natural deduction proof of Figure~\ref{figure:funcomp}.
This time around, we will explicate polarity information, and put an identifying index to each atomic formula occurrence at its axiom leaves; 
the turnstile mirrors indices faithfully, but inverts atomic polarities.
Going bottom up through Figure~\ref{figure:funcomp_annotated}, every encounter of an implication elimination allows us (i) to identify the set of indices coming from the functor's negative part with the set of indices provided by the counterpart positive argument and (ii) to propagate the negative indices of the functor's remainder to the succedent of the next conclusion.
That is, the first elimination $\li E(1)$ creates the identification $\{k \leftrightarrow m\}$, and propagates the positive leftover $\nprop{B}_l$ to the next proof step, whereas the next elimination $\li E(2)$ identifies $\{i \leftrightarrow l\}$ and propagates $\nprop{C}_j$ downwards.
Upon reaching the proof's conclusion, we get to merge all identifications established along the proof%
\footnote{
Let's avoid any misteps here. The joining described is \textit{not} set-theoretic union. Rather, we first take the set-theoretic union, and then iteratively reduce the set by conflating all identifications that agree in at least one element, up to a fixpoint.
For instance, joining $\{ i \leftrightarrow k\}$ and $\{ l\leftrightarrow i, j \leftrightarrow m\}$ yields $\{i \leftrightarrow k \leftrightarrow l, j \leftrightarrow m \}$. Such a situation could arise for example when attempting to find the axiom links of non $\beta$ normal proofs, like
\[
\term{\left(\lam \vari.\vari^{\prop{a}_i \li \prop{b}_j}~\varj^{\prop{a}_k}\right)~\vark^{\prop{a}_l \li \prop{b}_m}}
\]
The added burden has the enormous benefit of yielding ``$\beta$ normalized'' links and resolving potential future headaches \textit{a priori}.
}%
, which can then be applied as a mapping (e.g. to the lexicographically first element) yielding a link-decorate judgement, in our case:
\[
	\nprop{a}_k \li \pprop{b}_i, \nprop{b}_i \li \pprop{c}_j \vdash \pprop{a}_k \li \nprop{c}_j
\]
Matching indices correspond exactly to the axiom links of Figure~\ref{subfigure:proof_structure_funcomp} -- the two representations are in fact equivalent.
Now, we really do know how to freely move back and forth between the proof net and natural deduction presentation of proofs in $\ILL_{\li}$.

The question then is: when should we use which?
The original intention of proof nets was to provide a compact, bureacracy-free representation of proofs that abstracts away from structural rules.
In that sense, their strength is also their weakness; same as the $\lam$-terms they prescribe, they encode the semantically essential part of a proof, but hide structural subtleties that can prove hard to guess or recover.
At the same time, performing search over proof nets is a horrible idea; the number of possible links we need to consider scales factorially with respect to the number of atoms in the proof frame, and checking whether a set of links is valid is in the best case linear.
Due to these limitations, proof nets were envisaged as a compiled form of an existing proof, rather than a canvas to find that proof on.
We will not see proof nets again for a while, but we will keep their memory warm in our hearts.
Because when we do, we will challenge this perception and see how their parallel nature can actually be very convenient for proof search.
Until then, we can temporarily store them in our mental backlog. 


\section{Lambek Calculi}\label{section:lambek_calculi}
\subsection{Dropping Commutativity}
There is only one structural rule left\footnote{Or is there?}: it is time for \Exchange{} to go.
Dropping \Exchange{} makes the structures of our logic non-commutative.
The transition, however, requires some care.
If we were to naively go about our business using the inherited $\ILL$ connectives, we would soon stumble upon a pitfall.
Recalling the shape of the $\li E$ rule, we come to the realization that functions carried over to this new logic are suddenly picky; they can only be applied to arguments to their right.
This should raise some flags: a directionally flavoured version of the implication is not bad in itself, but the presence of just such one such version is --  where shall we look for the left-biased one?
The answer is simple: the conflation between the two directions was natural, up until a moment ago; having them both would not amount to much, since by \Exchange{} they would be interderivable.
With \Exchange{} removed, the veil is lifted and we can now see this clearly: there were always two implications, except disguised by the same symbol!
Let us do our newfound friend justice, and make this distinction explicit.

The logic that provides us with the tools to accomplish this is due to Jim Lambek~\cite{lambek1958mathematics}, and has come to by known as the Lambek calculus $\LC$.
At this point, the careful reader will notice a chronological inconsistency in our presentational tour: the Lambek calculus predates Linear Logic! Despite the fact, it is in essence a \textit{refinement} of its purely linear part -- a substructural logic within a substructural logic -- and our previous exposition makes us better equiped to appreciate it.
With commutativity gone, the Lambek calculus brings \textit{order} to Linear Logic -- in the literal sense -- assumptions must now be used exactly in the order they were instantiated.
It also brings forth the notion of \textit{adjacency}: structures joined by a rule are now immobile, and therefore obliged to remain adjacent from then on, unless broken apart by abstractions.

Formulas in the Lambek calculus are generated by the grammar:
\begin{equation}
\prop{a}, \prop{b}, \prop{c} := p \ | \ \prop{a} \divleft \prop{b} \ | \ \prop{a} \divright \prop{b} \ | \ \prop{a} \otimes \prop{b}
% \ | \ \prop{a} \otimes \prop{b} \ | \ \prop{a} \with \prop{b}
\label{equation:LC}
\end{equation}
The rules of this fragment are presented in Figure~\ref{figure:lambek_calculus_rules}.
Alternative presentations can include additive conjunction and/or either of the disjunctions, but the key feature of interest lies in the two implications, $\divright$ and $\divleft$.
The intuitive way of reading those is as directed fractionals, the formula hidden under the cover of the slash being the divisor, and the formula lying on it the dividend.
The elimination rule $\divright E$ (resp. $\divleft E$) can then be read as fractional simplifications, whereby right (resp. left) multiplication by the divisor cancels out the division as a whole.
An analogus reading can be attributed to the introduction rules, them now being the instantiation of a division by withdrawing items from the left or right of the assumption sequence (it might be helpful to think of $\divright I$ as dequeuing and $\divleft I$ as popping from the assumptions in the premise).
The division paradigm is of pedagogical utility only, and we will not take it any further for fear of (incorrectly) hinting at other properties of fractionals being applicable in the logic.
A noteworthy change of notation appears in the elimination of the product: with $\Delta\ctx{\Gamma}$ we denote a structure $\Delta$ containing substructure $\Gamma$: $\Delta\ctx{\_}$ now serves as a \textit{context}, i.e. a structure of assumptions with a hole.
The rule now claims it is acceptable to \textit{replace} substructure $\smallprop{A}, \smallprop{B}$ in $\Delta$ by $\Gamma$, if $\Gamma \vdash \smallprop{A}\otimes\smallprop{B}$ holds.
The notions of structure and substructure depend of course on the logic used -- in the current setting, $\Delta$ is a sequence, to which $\Gamma$ is a subsequence. 
The reformulation of the rule is necessary to arbitrate elimination of nested products, since their extraction to the right or left edge of an assumption sequence is no longer possible.
This also serves to better illustrate a remark made earlier: the rule can be applied at arbitrary nesting depths, each position corresponding to a supposedly different proof (consider for instance that if $\Delta\ctx{\Gamma}$, and $\Gamma\ctx{\smallprop{A}, \smallprop{B}}$, then it is also the case that $\Delta\ctx{\prop{a}, \prop{b}}$).

\begin{figure}
	\centering
	\begin{tabularx}{0.975\textwidth}{@{}CC@{}}
		\multicolumn{2}{@{}c@{}}{$\infer[\Ax]{\term{\vari }: \prop{a} \vdash \term{\vari }: \prop{a}}{}$}\\[2em]
		$\infer[\divright E]{\Gamma, \Delta \vdash \term{s}\appright\term{t}: \prop{b}}{\Gamma \vdash  \term{s}: \prop{b} \divright \prop{a} & \Delta \vdash \term{t}: \prop{a}}$
		& 
		$\infer[\divright I]{\Gamma \vdash \term{\lam \vari .s}: \prop{b} \divright \prop{a}}{\Gamma, \term{\vari }: \prop{a} \vdash \term{s}: \prop{b}}$\\[2em]
		$\infer[\divleft E]{\Gamma, \Delta \vdash \term{s}\appleft\term{t}:\prop{b}}{\Gamma \vdash \term{s}:\prop{a} & \Delta \vdash \term{t}:\prop{a}\divleft\prop{b}}$
		& 
		$\infer[\divleft I]{\Gamma \vdash \term{\adbmal \vari .s}: \prop{a} \divleft \prop{b}}{\term{\vari }: \prop{a}, \Gamma \vdash \term{s}: \prop{b}}$\\[2em]
		$\infer[\otimes E]{\Delta\ctx{\Gamma} \vdash \cterm{case \term{s} of \term{(\vari , \varj)} in \term{t}}: \prop C}{
			\Gamma \vdash \term{s}: \prop{a}\otimes\prop{b}
			&
			\Delta\ctx{\term{\vari }: \prop{a}, \term{\varj}: \prop{b}}  \vdash \term{t}: \prop C}$
		&
		$\infer[\otimes I]{\Gamma,\Delta \vdash \term{(s,t)}: \prop{a} \otimes \prop{b}}{\Gamma \vdash \term{s}: \prop{a} & \Delta \vdash \term{t}: \prop{b}}$\\[2em]
	\end{tabularx}
	\caption{Lambek calculus $\LC$.}
	\label{figure:lambek_calculus_rules}
\end{figure}

The Lambek calculus hails from an intuitionistic tradition, and is thus amenable to a propositions as types interpretation~\cite{wansing1990formulas}.
Addorning its rules with faithful term rewrites translates into a type system that is both linear and ordered~\cite{pierce2004advanced}.
Things get funky there: we now have two distinct modes of function application and $\lam$ abstraction, each pair with its own reduction.
We use $\appright$ and $\appleft$ to denote right and left application, respectively -- the mnemonic is that the triangle points to the function -- and $\lam$ and $\adbmal$ to denote the two kinds of anonymous functions.

\subsubsection{Proof \& Term Reductions}
The proof reductions of Figure~\ref{figure:lambek_proof_reductions} should be at this point straightforward to decode.
The only addition is the symmetric version of the familiar implicational redex.
For the redex of the product, the substituted $\smallprop{A}$ and $\smallprop{B}$ hypotheses are now wrapped on both sides by a context $\Theta$, following the formulation of $\otimes E$.
The corresponding term reductions are:
\begin{align}
(\term{\lam \vari .s})\appright\term{t} & \bred \term{s}_{[\term{\vari } \mapsto \term{t}]}\\
\term{t}\appleft(\term{\adbmal \vari .s}) & \bred \term{s}_{[\term{\vari } \mapsto \term{t}]}\\
\cterm{case (\smallterm{s}, \smallterm{t}) of (\smallterm{\vari }, \smallterm{\varj}) in \smallterm{u}} & \bred \term{u_{[\term{s} \mapsto \term{\vari }, \term{t} \mapsto \term{\varj}]}}
\end{align}

\begin{figure}
	\centering
	\begin{tabularx}{0.95\textwidth}{@{}ccc@{}}
	$\infer[\divright E]{\Gamma, \Delta \vdash \prop{b}}{
	\infer[\divright I]{\Gamma \vdash \prop{b} \divright \prop{a}}{
		\infer*[s]{\Gamma, \prop{a} \vdash \prop{b}}{
				\infer[\Ax]{\prop{a} \vdash \prop{a}}{} 
			}
		}
		&
		\infer{\Delta \vdash \prop{a}}{
			\infer*[t]{}{}
		}
	}$
	&
	\raisebox{20pt}{$\implies$}
	&
	$ 
	\infer[]{\Gamma, \Delta \vdash \prop{b}}{
		\infer*[s]{}{
		\infer[]{\Delta \vdash \prop{a}}{
			\infer*[t]{}{}
		}}
	}
	$\\[\smallsep]
	$\infer[\divleft E]{\Gamma, \Delta \vdash \prop{b}}{
		\infer{\Gamma \vdash \prop{a}}{
			\infer*[t]{}{}
		}
		&
		\infer[\divleft I]{\Delta \vdash \prop{a} \divleft \prop{b}}{
			\infer*[s]{\prop{a}, \Delta \vdash \prop{b}}{
				\infer[\Ax]{\prop{a} \vdash \prop{a}}{} 
			}
		}
	}$
	&
	\raisebox{20pt}{$\implies$}
	&
	$ 
	\infer[]{\Gamma, \Delta \vdash \prop{b}}{
		\infer*[s]{}{
		\infer[]{\Gamma \vdash \prop{a}}{
			\infer*[t]{}{}
		}}
	}
	$\\[\smallsep]
	$
	\infer[\otimes E]{\Theta\ctx{\Gamma, \Delta} \vdash \prop{c}}{
		\infer[\otimes I]{\Gamma, \Delta \vdash \prop{a} \otimes \prop{b}}{
			\infer[]{\Gamma \vdash \prop{a}}{\infer*[s]{}{}}
			&
			\infer[]{\Delta \vdash \prop{b}}{\infer*[t]{}{}}
		}		
		&
		\infer*[u]{\Theta\ctx{\prop{a}, \prop{b}} \vdash \prop{c}}{
			\infer[\Ax]{\prop{a} \vdash \prop{a}}{}
			& 
			\infer[\Ax]{\prop{b} \vdash \prop{b}}{}
		}
	}
	$
	&
	\raisebox{20pt}{$\implies$}
	&
	$
	\infer*[u]{\Theta\ctx{\Gamma, \Delta} \vdash \prop{c}}{
		\infer[]{\Gamma \vdash \prop{a}}{
			\infer*[s]{}{}
		}
		&
		\infer[]{\Delta \vdash \prop{b}}{
			\infer*[t]{}{}
		}
	}
	$
	\end{tabularx}
	\caption{Lambek $\beta$ redexes.}
	\label{figure:lambek_proof_reductions}
\end{figure}

\subsection{Dropping Associativity}
Judging by the apparent absence of any more structural rules to remove, someone eager to be done with the whole story could at this point proclaim our substructural tour finished.
We are not quite done yet, however, for one last structural equivalence still remains unchecked (one we have made extensive use of, for that matter).
The culprit can be found by going back to our original definition of structures in the long and distant past of Subsection~\ref{subsection:intuitionistic_logic} -- by treating them as sequences, we have mindlessly equipped them with \textit{associativity} for free, the use of which we never made explicit.
The one to notice was Lambek once more~\cite{lambek1961calculus}.
In the new logic (pragmatically named the non-associative Lambek calculus $\NL$) the definition of a structure changes to:
\begin{equation}
	\Gamma, \Delta, \Theta := \ \prop{a} \ | \ (\Gamma \sbind \Delta)
\end{equation}
i.e. the structural unit of the empty sequence is no more, and the scope of the binary structural binder is made explicit with brackets (we use the distinct symbol \sbind{} to tell this new structural binder apart from its associative sibling).
On top of adjacency and order, the non-associative Lambek calculus further considers \textit{constituency}; structures are now binary trees, with atomic propositions as their leaves and \sbind{} as branching nodes, and judgements are differentiated on the basis of the binary branching form their assumptions take.
Formulas remain as they were, but the presentation of the rules changes to that of Figure~\ref{figure:non_assoc_lambek_rules} in order to accommodate the new, stricter structures.
Merging structures $\Gamma$ and $\Delta$ via $\divleft E$, $\divright E$ or $\otimes I$ is translated to building up a tree with the two as branches.
Decomposing a structure via an abstraction $\divleft I$ or $\divright I$ now requires that the formula abstracted over occurs not just at the edge of the tree's linear projection, but also at its top-most branching level.
The notation $\Gamma\ctx{\Delta}$ now denotes that $\Delta$ is a \textit{subtree} of $\Gamma$ -- for the product elimination $\otimes E$ to be applicable, $\prop{a}$ and $\prop{b}$ need not just be adjacent, but also commonly rooted.

\begin{figure}
	\centering
	\begin{tabularx}{0.99\textwidth}{@{}CC@{}}
		\multicolumn{2}{@{}c@{}}{$\infer[\Ax]{\term{\vari }: \prop{a} \vdash \term{\vari }: \prop{a}}{}$}\\[2em]
		$\infer[\divright E]{(\Gamma \sbind \Delta) \vdash \term{s}\nappright\term{t}: \prop{b}}{\Gamma \vdash \term{s}: \prop{b} \divright \prop{a} & \Delta \vdash \term{t}: \prop{a}}$
		& 
		$\infer[\divright I]{\Gamma \vdash \term{\lam \vari .s}: \prop{b} \divright \prop{a}}{(\Gamma \sbind \term{\vari }: \prop{a}) \vdash \term{s}:\prop{b}}$\\[2em]
		$\infer[\divleft E]{(\Gamma \sbind \Delta) \vdash \term{s}\nappleft\term{t}:\prop{b}}{\Gamma \vdash \term{s}: \prop{a} & \Delta \vdash \term{t}: \prop{a}\divleft\prop{b}}$
		& 
		$\infer[\divleft I]{\Gamma \vdash \term{\adbmal \vari .s}: \prop{a} \divleft \prop{b}}{(\term{\vari }: \prop{a} \sbind \Gamma) \vdash  \term{s}:\prop{b}}$\\[2em]
		$\infer[\otimes E]{\Delta\ctx{\Gamma} \vdash \cterm{case \term{s} of \term{(\vari , \varj)} in \term{t}}: \prop C}{
			\Gamma \vdash \term{s}: \prop{a}\otimes\prop{b}
			&
			\Delta\ctx{(\term{\vari }: \prop{a}, \term{\varj}: \prop{b})}  \vdash \term{t}: \prop C}$
		&
		$\infer[\otimes I]{\Gamma,\Delta \vdash \term{(s,t)}: \prop{a} \otimes \prop{b}}{\Gamma \vdash \term{s}: \prop{a} & \Delta \vdash \term{t}: \prop{b}}$
	\end{tabularx}
	\caption{Non-associative Lambek calculus $\NL$.}
	\label{figure:non_assoc_lambek_rules}
\end{figure}

The syntax of the isomorphic $\lam$-calculus is identical to before, except this time we use $\nappright$ and $\nappleft$ to notationally differentiate with the non-associative application (not unlike how we replaced the intuitionistic implication $\ii$ with its linear counterpart $\li$ earlier).
The new structural constraint on the introduction of a directed implication can be intuititively translated to a constraint on the applicability of an abstraction.
Namely, the variable to abstract over needs to occur at the top-most level of a function application in the term's inductive body.%
\footnote{Abusing terminology, here by inductive body we mean the term itself (if it's an implicative one), the term's inner body (if it's a sequence of abstractions), the left or right coordinate (if it's a product), or the nested body on which substitution is performed (if it's a deconstructed product).}

\subsubsection{Proof \& Term Reductions}
Proof \& term reductions are notationally identical to those of the previous subsection, modulo bracketing, and substituting white for black triangles.
I trust the missing picture is easy enough to create mentally.


\subsection{The Full Landscape}
We have seen $\NL$ as a refinement of $\LC$, and $\LC$, in turn, as a refinement of $\ILL$.
The three can be perceived as points in a lattice of substructural logics, upon which we can move by adding or removing structural rules at a global level; this view lends $\ILL$ its alternative name $\LP$, for the Lambek calculus with permutation (also encountered as the Lambek-van Benthem Calculus~\cite{van1988semantics}).
At the top of the diamond we have $\ILL$, where (linearity aside), anything goes, and at the bottom we have $\NL$, where neither associativity nor commutativity hold.
At the center, there's $\LC$, where only associativity holds.
Next to it, an unexpected curiosity pops up: $\NLP$ (for the non-associative Lambek with permutation), an offbeat logic where associativity holds but commutativity doesn't -- its structures are \textit{mobiles}: orderless, binary branching trees that make no distinction between left and right daughters.
Unlike its relatives, $\NLP$ has received limited attention from theorists and practitioners alike.
This will still remain the case even after (if?) this manuscript sees the light of day, but its peculiar structures will reemerge and have their moment to shine  later on.

\begin{figure}
	\centering
	\begin{tikzpicture}[g/.style={outer sep=3pt, inner sep=0pt,minimum width=4pt},e/.style={thick,->}]
		\node[g] (ILL) at (0,0)			{$\LP \equiv \ILL$};
		\node[g] (LC)	at (-1, -1.5)		{$\LC$};
		\node[g] (NL)	at (0, -3)		{$\NL$};
		\node[g] (NLP) at (1, -1.5)		{$\NLP$};
		
		\draw[e] (ILL) -- (LC) node[midway,left]{-comm};
		\draw[e] (LC) -- (NL) node[midway,left]{-asso};
		\draw[e] (ILL) -- (NLP) node[midway,right]{-asso};
		\draw[e] (NLP) -- (NL) node[midway,right]{-comm};
		\
	\end{tikzpicture}
	\label{figure:substructural_logics}
	\caption{$\logic{(N)L(P)}$: $\ILL$ and substructural friends.}
\end{figure}


%\paragraph{Proof nets}
%\todo{}

\section{Restoring Control}\label{section:modalities}
With every step we have taken further into substructuraland, we have been paying a price in expressivity; it is now time for us to acknowledge the accumulated bill.
Dropping \Contraction{} and \Weakening{} made us resource conscious, but theorems of $\IL$ that required resource duplication or erasure became underivable.
Dropping \Exchange{} forced us to pay attention to the order of assumptions, but costed us access to theorems that required permutation to derive.
Substituting the structural comma $\_,\_$ with the non-associative $(\_\sbind\_)$ casted our sequences to trees, this time at the expense of theorems that required rebracketing.
Woe is us -- is there even anything left we can derive?

Perhaps this is painting an overly dramatic picture, considering that none of this is necessarily bad.
From an epistemic perspective, the less structural equivalences we take for granted, the better our mental grasp of structural difference becomes.
In the best case, if it just so happens that the kind of structures we want to investigate overlaps \textit{fully} with the kind of structures our logic can explicitly reason about, the distinction between theorem and non-theorem becomes a refinement rather than a loss of expressivity.
From a more pragmatic perspective, more structural constraints means easier proof search, and less theorems means faster exhaustion of possibilities.
To make the scale of the combinatorics tangible, reflect for a second on this.
A single judgement of $n$ hypotheses in $\NL$ is but one of the Catalan number of bracketings $C(n)$ it would be syntactically undistinguishable from in $\LC$, each one of which in turn is but one of the factorially many permutations $n!$ it would be equivalent to in $\LP$.%
\footnote{Boom, goes the combinatorial explosion.}
In the case for checking the satisfiability of a judgement (i.e. searching for \textit{any} valid proof), all the above would have made for potential proof candidates;
in the case for attempting to enumerate the proofs of a judgement (i.e. searching for \textit{all} valid proof), they would all have needed to be exhausted.
The point to take home is that proof search becomes decidedly easier in the absence of syntactic equivalences, so perhaps a double-edged sword would have made for a better analogy than a bill.

The defeatist attitude here would be to just accept the trade-off between expressivity and complexity, weep for the theorems forever lost, take our victory and walk away.
The problem lies however in the common occasion where the structure of objects under scrutiny overlaps only \textit{partially} with a specific substructural flavour, modulo some exceptional but real cases that require added expressivity.
In such a scenario, taking a step up in the hierarchy would cause an undesirable combinatorial explosion, whereas staying put would sacrifice our ability to argue about these exceptional cases. 
By contrast, the maximalist attitude makes no concessions and seeks both for the cake to be whole and the dog to be fed.%
\footnote{Direct translation of a silly but fitting Greek aphorism that won by a small margin over the Italian equivalent (wine barrel full and wife drunk). In any case, cake is bad for dogs.}
What if there was a way to keep our logic computationally tractable but with temporary and on-demand access to normally excluded reasoning tools?

\subsection{The Logic of Modalities}\label{subsec:modal_logic}
The answer comes in the form of unary \textit{modalities}, type-forming operators lent from modal logics, that allow navigation between logics of different structural properties.
Unary modalities hold a key role in the presentation of full linear logic; there, a single operator $!$ (called \textit{bang}) would allow an embedding of intuitionistic (non-linear) propositions into the linear regime, essentially acting as a licensor of \Contraction{} and \Weakening{}.
In our case, we will make do with two modalities from temporal logic, the diamond $\diamond$ and the box $\bx$.

The two form a residuated pair, the properties of which can be formulated either (i) in the form of a type-level biconditional derivability relation:
\begin{equation}
		\diamond \prop{a} \vdash \prop{b}\text{ iff }\prop{a} \vdash \bx\prop{b}
\end{equation}
or (ii) the monotonic behavior of its parts:
\begin{align}
		\prop{a} \vdash \prop{b} & \implies \diamond \prop{a}\vdash \diamond\prop{b}\\
		\prop{a} \vdash \prop{b} & \implies \bx{\prop{a}}\vdash \bx\prop{b}
\end{align}
and the adjointness of their compositions, where $\diamond\bx(\_)$ is an \textit{interior} and $\bx\diamond(\_)$ a \textit{closure} operator:
\begin{align}
		\Gamma \vdash \prop{a} 				& \implies \Gamma \vdash \bx\diamond\prop{a} \label{equation:closure}\\
		\Gamma \vdash \diamond\bx\prop{a} & \implies \Gamma \vdash \prop{a} \label{equation:interior}
\end{align}

\begin{figure}
	\centering
	\begin{tabularx}{0.9875\textwidth}{@{}CC@{}}
		$\infer[\bx E]{\bracket{\Gamma} \vdash \boxelim{\term{s}}: \prop{a}}{
			\Gamma \vdash \term{s}: \bx\prop{a}
		}$
		&
		$\infer[\bx I]{\Gamma \vdash \boxintro{\term{s}}:\bx\prop{a}}{
			\bracket{\Gamma} \vdash \term{s}:\prop{a}
		}$
		\\[\smallsep]
		$\infer[\diamond E]{\Gamma\ctx{\Delta} \vdash \cterm{case \term{\diaelim t} of \term{\vari } in \term{s}}: \prop{b}}{
			\Gamma\ctx{\bracket{\term{\vari }:\prop{a}}}\vdash\term{s}:\prop{b}
			&
			\Delta \vdash \term{t}:\diamond\prop{a}
		}
		$
		&
		$\infer[\diamond I]{\bracket{\Gamma} \vdash \diaintro{\term{s}}: \diamond \prop{a}}{
			\Gamma \vdash \term{s}:\prop{a}
		}$
	\end{tabularx}
	\caption{Logical rules of modal inference.}
	\label{figure:modal_logical}
\end{figure}

The logical manipulation of these modalities is handled by corresponding elimination and introduction rules, presented in Figure~\ref{figure:modal_logical}.
The presentation is intentionally detached from a specific subtructural strand -- modalities are plug-and-play to any member of the $\logic{(N)L(P)}$ family.
Their incorporation adds a new kind of structure to the ones provided by the underlying logic, altering judgements accordingly:
\begin{equation}
	\Gamma, \Delta, \Theta := \dots \ | \ \bracket{\Gamma}
\end{equation}

Angular brackets denote unary tree branches that behave slightly different to the rest; they act as an impenetrable barrier that permits or hinders the introduction or elimination of modal connectives in a judgement.
The box elimination rule $\bx E$ grants us the option of removing a logical box from the succedent of the premise (as long as it is its main connective), but encloses the premises in angular brackets in the process.
Its introduction counterpart $\bx I$ does the exact opposite: it frees a judgement's assumptions from their brackets, but puts the succedent proposition under the scope of a box.
The diamond behaves just the other way around.
Its introduction rule $\diamond I$ is straightforward: it offers the possibility of putting the succedent under the scope of a diamond, in exchange wrapping the antecedents with brackets.
The elimination rule $\diamond E$ is more of a problem child, behaving akin to a unary product.
Without locality restrictions, it inspects a proof of $\smallprop{B}$, the assumptions of which contain a substructure $\bracket{\smallprop{A}}$ within context $\Gamma{\ctx{\_}}$, and allows the post-hoc substitution of the hypothesis together with its brackets by a structure $\Delta$, if from it one can derive $\diamond \smallprop{A}$.

Rules are adorned with term rewrite instructions in the propositions as types style, similar to how temporal logic can be operationalized in the $\lam$-calculus~\cite{wansing2002sequent}.
The mnemonic is now two-dimensional: upward triangles denote introduction and downward ones elimination, whereas black triangles are for the box, white ones for the diamond.
Term constructions for the single-premise rules are uncomplicated: each type operation just leaves the corresponding term footprint.
This is not the case for the $\diamond E$ rule, which requires some attention:
the structural substitution of $\bracket{\smallprop{A}}$ for $\Delta$ necessitates a case construct that calls for a term substitution of the variable $\smallterm{\vari }$ for $\diaelim\smallterm{t}$.
Note that the free variables of the resulting expression ($\caseof{\diaelim t}{\vari }{s}$) are the union of the free variables of \smallterm{t} and those of \smallterm{s} except for \smallterm{\vari }, which becomes \textit{bound} by the case construct.

\subsubsection{Proof \& Term Reductions}
\begin{figure}
	\begin{subfigure}{1\textwidth}
	\centering
		\begin{tabularx}{0.85\textwidth}{@{}ccc@{}}
		$\infer[\bx E]{\bracket{\Gamma} \vdash \prop{a}}{
			\infer[\bx I]{\Gamma \vdash \bx \prop{a}}{
				\infer*[s]{\bracket{\Gamma} \vdash  \prop{a}}{}
			}
		}
		$
		&
		\raisebox{10pt}{$\implies$}
		&
		$\infer*[s]{\bracket{\Gamma} \vdash \prop{a}}{}
		$\\[\smallsep]
		$\infer[\diamond E]{\Gamma\ctx{\bracket{\Delta}} \vdash \prop{b}}{
			\infer*[s]{\Gamma\ctx{\bracket{\prop{a}}} \vdash \prop{b}}{
				\infer[\Ax]{\prop{a} \vdash \prop{a}}{}}
			&
			\infer[\diamond I]{\bracket{\Delta} \vdash \diamond\prop{a}}{
				\infer*[t]{\Delta \vdash \prop{a}}{}
			}
		}
		$
		&
		\raisebox{20pt}{$\implies$}
		&
		$\infer*[s]{\Gamma\ctx{\bracket{\Delta}} \vdash \prop{b}}{
			\infer*[t]{\Delta \vdash \prop{a}}{}
		}
		$
		\end{tabularx}
		\caption{Modal $\beta$ redexes.}
		\label{subfigure:modal_beta_reductions}
	\end{subfigure}\\[\midsep]
	\begin{subfigure}{1\textwidth}
	\centering
		\begin{tabularx}{0.8\textwidth}{@{}ccc@{}}
			$\infer[\bx I]{\Gamma \vdash \bx\prop{a}}{
			\infer[\bx E]{\bracket{\Gamma} \vdash \prop{a}}{
					\infer*[s]{\Gamma \vdash  \bx\prop{a}}{}
				}
			}
			$
			&
			\raisebox{5pt}{$\equiv$}
			&
			$\infer*[s]{\Gamma \vdash \bx\prop{a}}{}
			$\\[\smallsep]
			$\infer[\diamond E]{\Delta \vdash \diamond\prop{a}}{
				\infer[\diamond I]{\bracket{\prop{a}}\vdash \diamond \prop{a}}{
					\infer[\Ax]{\prop{a} \vdash \prop{a}}{}
				}
				&
				\infer*[s]{\Delta \vdash \diamond\prop{a}}{}
			}$
			&
			\raisebox{15pt}{$\equiv$}
			&
			$\infer*[s]{\Delta \vdash \diamond \prop{a}}{}
			$
		\end{tabularx}
	\caption{Modal $\eta$ redexes.}
	\label{subfigure:modal_eta_reductions}
	\end{subfigure}
	\caption{Modal redexes}
	\label{figure:modal_reductions}
\end{figure}

The proof patterns of Figure~\ref{subfigure:modal_beta_reductions} exhibit introduction elimination chains of modal operators, and thus constitute $\beta$ redexes subject to normalization.
The first one is trivial: it just says that a sequential application of $\bx I$ followed by $\bx E$ can be safely excised.
The second one proposes that if a $\diamond I$ is the last rule to have been applied on the substitution branch $t$ of the $\diamond E$ rule, it would make sense to simply plug proof $t$ in place of the proposition $\smallprop{A}$ hypothesized in the other  branch $s$.
On the term level, these rules correspond to computations:
\begin{align}
\boxelim \boxintro \term{s} & \bred \term{s}\\
\cterm{case \term{\diaelim \diaintro t} of \term{x} in \term{s}} & \bred \term{s}_{[\term{x} \mapsto \term{t}]}
\end{align}

The dual direction of $\eta$ equivalences also holds -- since these are discovered in the literature less frequently than the more pedestrian implication and product equivalences, we explictly present them in Figure~\ref{subfigure:modal_eta_reductions}.
The term equivalences they materialize are:
\begin{align}
\boxintro \boxelim \term{s} & \overset{\eta}{\equiv} \term{s}\\
\cterm{case \term{\diaelim{s}} of \term{x} in \term{\diaintro{x}}} & \overset{\eta}{\equiv} \term{s}
%\cterm{case \term{\diaelim \diaintro t} of \term{x} in \term{s}} & \bred \term{s}_{[\termt{x} \mapsto \term{t}]}
\end{align}

\subsubsection{A Digression on Modal Terms}
For the modally savvy, the term rewrites attributed to the modal rules might seem unorthodox.
A more common presentation employs the simpler meta-syntax notation of term substitution.
For instance, $\diamond E$ can often be spotted in the wild as:
\[
	\infer[\diamond E]{\Gamma\ctx{\Delta} \vdash \term{s}_{[\term{\vari }\mapsto\term{\diaelim t}]}: \prop{b}}{
		\Gamma\ctx{\bracket{\term{\vari }:\prop{a}}}\vdash\term{s}:\prop{b}
		&
		\Delta \vdash \term{t}:\diamond\prop{a}
	}
\]
In this disguise, the rule is again seen as realizing a retroactive substitution of $\smallterm{\vari }$ with $\smallterm{\diaelim t}$, except this time around the substitution is \textit{actually} performed, resulting in less cumbersome terms being carried around.

Opting for this alternative notation has, however, a number of negative consequences.
The more superficial one is that the main term connective does not take scope at the outermost layer of rule's yield, but rather nested arbitrarily deeply within it, unlike its better behaved version.
From a proof-theoretic perspective, normalization is now baked directly into the theory, as the term yield of the rule exactly coincides with its $\beta$ reduced form.
At the same time, all rule permutations boil down to having the exact same reduction, i.e. multiple previously distinct terms are conflated into a single representation.
This establishes an impicit syntactic equivalence on proofs that claims that the exact position of the $\diamond E$ rule is \textit{syntactically irrelevant} (so long of course as the same variable $\smallterm{\vari }$ is substituted by the same term $\smallterm{\diaelim t}$).
Finally, the shorthand version hides variables; hypotheses that would be bound by the case construct are instead erased and forgotten, obfuscating the term-to-proof correspondence.
All these are perhaps minor points not worth taking too seriously, but for one concerned with concrete implementation the extra merit of notational simplicity comes at the cost of equality checking become way more tedious.
With this in mind (and in a rare moment of excessive formal zeal), we will exercise some self restraint and avoid indulging in the convenience of this version.

\subsubsection{Properties}

\begin{figure}
	\centering
	\begin{subfigure}{1\textwidth}
		\[
			\infer[\diamond E]{\term{\varj}: \diamond \prop{a} \vdash  \cterm{case \term{\diaelim \varj} of \term{\vari } in \term{s}}: \diamond \prop{b}}{
					\infer[\diamond I]{\bracket{\term{\vari }: \prop{a}} \vdash \diaintro \term{s}: \diamond \prop{b}}{
						\infer*[s]{\term{\vari }: \prop{a} \vdash \term{s}: \prop{b}}{}
					}
					&
					\infer[\Ax]{\term{\varj}: \diamond \prop{a} \vdash \term{\varj}: \diamond \prop{a}}{}
			}
		\]
		\caption{Monotonicity of the diamond.}
		\label{subfigure:modal_properties:diamond_mono}
	\end{subfigure}\\[\smallsep]
	\begin{subfigure}{1\textwidth}
		\[
			\infer[\bx I]{\term{\varj}: \bx \prop{a} \vdash \boxintro{(\term{\lam \vari .s})~\term{\boxelim \varj}}:\bx \prop{b}}{
				\infer[\li E]{\bracket{\term{\varj}: \bx \prop{a}} \vdash (\term{\lam \vari .s})~\term{\boxelim \varj}: \prop{b}}{
					\infer[\li I]{\vdash \term{\lam \vari .s}: \prop{a} \li\prop{b}}{
						\infer*[s]{\term{\vari }: \prop{a} \vdash \term{s}: \prop{b}}{}
					}
					&
					\infer[\bx E]{\bracket{\term{\varj}: \bx \prop{a}} \vdash \boxelim\term{\varj}:\prop{a}}{	
						\infer[\Ax]{\term{\varj}: \bx\prop{a} \vdash \term{\varj}: \bx\prop{a}}{}
					}
				}
			}
		\]
		\caption{Monotonicity of the box.}
		\label{subfigure:modal_properties:box_mono}
	\end{subfigure}\\[\smallsep]
	\begin{subfigure}{0.4\textwidth}
		\[
		\infer[\bx I]{\Gamma \vdash \boxintro\diaintro\term{s}:\bx \diamond \prop{a}}{
			\infer[\diamond I]{\bracket{\Gamma} \vdash \diaintro\term{s}: \diamond \prop{a}}{
				\infer*[s]{\Gamma \vdash \term{s}: \prop{a}}{}
			}
		}
		\]
		\caption{The closure $\diamond\bx(\_)$.}
		\label{subfigure:modal_properties:closure}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\[
		\infer[\diamond E]{\Gamma \vdash \cterm{case \term{\diaelim{s}} of \term{\vari } in \term{\boxelim \vari }}: \prop{a}}{
						\infer[\bx E]{\bracket{\term{\vari }: \bx \prop{a}}\vdash \boxelim\term{\vari }:\prop{a}}{
							\infer[\Ax]{\term{\vari }: \bx\prop{a} \vdash \term{\vari }: \bx\prop{a}}{}
						}
						&
						\infer*[s]{\Gamma\vdash \term{s}: \diamond \bx \prop{a}}{}
				}
		\]
		\caption{And the interior $\bx\diamond(\_)$.}
		\label{subfigure:modal_properties:interior}
	\end{subfigure}\\[\smallsep]
	\begin{subfigure}{1\textwidth}
		\[
			\infer[\diamond E]{\term{\varj}: \diamond \prop{a} \vdash \caseof{\diaelim \varj}{\vari }{s}:\prop{b}}{
				\infer[\bx E]{\bracket{\term{\vari }: \prop{a}} \vdash \boxelim\term{s}: \prop{b}}{
					\infer*[s]{\term{\vari }: \prop{a} \vdash \term{s}: \bx\prop{b}}{}
				}
				&
				\infer[\Ax]{\term{\varj}: \diamond \prop{a} \vdash \term{\varj}: \diamond \prop{a}}{}
			}
		\]
	\caption{Residuation law: from $\smallprop{A} \vdash \bx\smallprop{B}$ to $\diamond\smallprop{A} \vdash \smallprop{B}$.}
	\label{subfigure:modal_properties:residuation:1}	
	\end{subfigure}\\[\smallsep]
	\begin{subfigure}{1\textwidth}
		\[
			\infer[\bx I]{\term{\varj}: \diamond \prop{a} \vdash \boxintro(\caseof{\diaelim \diaintro \varj}{\vari }{\lam \vari .s}): \prop{b}}{
				\infer[\diamond E]{\bracket{\term{\varj}: \prop{a}} \vdash \caseof{\diaelim \diaintro \varj}{\vari }{\lam \vari .s}: \prop{b}}{
					\infer[\li I]{\vdash \term{\lam \vari .s}: \diamond\prop{a} \li \prop{b}}{
						\infer*[s]{\term{\vari }: \diamond\prop{a} \vdash \term{s}: \prop{b}}{}
					}
					&
					\infer[\diamond I]{\bracket{\term{\varj}: \prop{a}} \vdash \term{\diaintro \varj}: \diamond \prop{a}}{
						\infer[\Ax]{\term{\varj}: \prop{a} \vdash \term{\varj}: \prop{a}}{}
					}
				}
			}
		\]
		\caption{Ditto, the other way around.}
		\label{subfigure:modal_properties:residuation:2}	
	\end{subfigure}
	\caption{Derivations for the various aspects of residuation.}
	\label{figure:modal_properties:residuation}
\end{figure}

Situating our unary operators within the modal logic zoo is no trivial endeavour (especially if you, like me, have had limited exposure to it before).
They are best characterized by the properties they satisfy, so inspecting them should shed some light on their proof-theoretic behavior (as a bonus, it will also help us get better acquainted with the kind of term rewrites their rules prescribe).
Figure~\ref{figure:modal_properties:residuation} presents the proof transformations equivalent to the properties foretold:
 (\subref{subfigure:modal_properties:diamond_mono}) and (\subref{subfigure:modal_properties:box_mono}) for monotonicity, (\subref{subfigure:modal_properties:closure}) and (\subref{subfigure:modal_properties:interior}) for composition, and (\subref{subfigure:modal_properties:residuation:1}) and (\subref{subfigure:modal_properties:residuation:2}) for the two directions of the residuation law.
 
Worth a special mention are also the so-called triple laws:
\begin{align}
	\diamond\prop{a} \dashv\vdash \diamond\bx\diamond\prop{a}\\
	\bx\prop{a} \dashv\vdash \bx\diamond\bx\prop{a}
\end{align}
which can be intuitively read as claiming that prepending an already modal type with (one or more) diamond-box pairs in alteration has no real effect, as these can unconditionally cancel out or be expanded into.
Figure~\ref{figure:modal_triple_laws} presents proofs of the above in both directions.


\begin{figure}
	\centering
	\begin{subfigure}{1\textwidth}
		\[
			\infer[\bx I]{\term{\varj}: \bx\diamond\bx\prop{a} \vdash \boxintro(\caseof{\diaelim \boxelim \varj}{\vari }{\boxelim{\vari }}): \bx\prop{a}}{
				\infer[\diamond E]{\bracket{\term{\varj}: \bx\diamond \prop{a}} \vdash \caseof{\diaelim \boxelim \varj}{\vari }{\boxelim{\vari }}: \prop{a}}{
					\infer[\bx E]{\bracket{\term{\vari }: \bx \prop{a}} \vdash \term{\boxelim \vari }:\prop{a}}{
						\infer[\Ax]{\term{\vari }: \bx\prop{a} \vdash \term{\vari }: \bx\prop{a}}{}
					}
					&
					\infer[\bx E]{\bracket{\term{\varj}:\bx\diamond\bx\prop{a}} \vdash \term{\boxelim \varj}:\diamond\bx\prop{a}}{
						\infer[\Ax]{\term{\varj}: \bx\diamond\bx\prop{a}\vdash\term{\varj}:\bx\diamond\bx\prop{a}}{}
					}
				}
			}
		\]
		\caption{Contraction of $\bx\diamond\bx(\_)$ to $\bx(\_)$.}
		\label{subfigure:triple_law:box_collapse}
	\end{subfigure}\\[\midsep]
	\begin{subfigure}{1\textwidth}
		\[
			\infer[\bx I]{\term{\vari }:\bx\prop{a} \vdash \boxintro\diaintro\term{\vari }:\bx\diamond\bx\prop{a}}{
				\infer[\diamond I]{\bracket{\term{\vari }: \bx\prop{a}} \vdash \diaintro\term{\vari }:\diamond\bx\prop{a}}{
					\infer[\Ax]{\term{\vari }:\bx\prop{a}\vdash \term{\vari }:\bx\prop{a}}{}
				}
			}
		\]
		\caption{Expansion of $\bx(\_)$ to $\bx\diamond\bx(\_)$.}
		\label{subfigure:triple_law:box_expand}
	\end{subfigure}\\[\midsep]
	\begin{subfigure}{1\textwidth}
		\[
			\infer[\diamond E]{\term{\varj}:\diamond\bx\diamond\prop{a} \vdash \caseof{\diaelim \varj}{\vari }{\boxelim \vari }: \diamond\prop{a}}{
				\infer[\bx E]{\bracket{\term{\vari }: \bx\diamond\prop{a}} \vdash \boxelim\term{\vari }: \diamond\prop{a}}{
					\infer[\Ax]{\term{\vari }: \bx\diamond\prop{a} \vdash \term{\vari }: \bx\diamond\prop{a}}{}
				}
				&
				\infer[\Ax]{\term{\varj}: \diamond\bx\diamond\prop{a} \vdash \term{\varj}: \diamond\bx\diamond\prop{a}}{}
			}
		\]
		\caption{Contraction of $\diamond\bx\diamond(\_)$ to $\diamond(\_)$.}
		\label{subfigure:triple_law:diamond_collapse}
	\end{subfigure}\\[\midsep]
	\begin{subfigure}{1\textwidth}
		\[
			\infer[\diamond E]{\term{\varj}: \diamond \prop{a} \vdash \caseof{\diaelim \varj}{\vari }{\diaintro\boxintro\diaintro \vari }:\diamond\bx\diamond\prop{a}}{
				\infer[\diamond I]{\bracket{\term{\vari }:\prop{a}} \vdash \diaintro\boxintro\diaintro\term{\vari }:\diamond\bx\diamond\prop{a}}{
					\infer{\term{\vari }: \prop{a} \vdash \boxintro\diaintro\term{\vari }: \bx\diamond\prop{a}}{\text{(\ref{subfigure:modal_properties:closure})}}
				}
				&
				\infer[\Ax]{\term{\varj}: \diamond \prop{a} \vdash \term{\varj}:\diamond\prop{a}}{}
			}
		\]
		\caption{Expansion of $\diamond(\_)$ to $\diamond\bx\diamond(\_)$.}
		\label{subfigure:triple_law:diamond_expand}
	\end{subfigure}
	\caption{The triple laws for the two modalities in both directions.}
	\label{figure:modal_triple_laws}
\end{figure}

\subsection{Structural Reasoning}

\begin{figure}
	\centering
	\begin{subfigure}{1\textwidth}
		\centering
		\begin{tabularx}{0.75\textwidth}{@{}c@{\qquad}c@{}}	
		$
		\infer[\rulestyle{ass}_{\diamond}]{\Gamma\ctx{(\Delta, \Theta), \bracket{\Phi}} \vdash \prop{a}}{
			\Gamma\ctx{\Delta, (\Theta, \bracket{\Phi})} \vdash \prop{a}
		}
		$
		&
		$
		\infer[\rulestyle{mix}_{\diamond}]{\Gamma\ctx{(\Delta, \Phi), \bracket{\Theta}} \vdash \prop{a}}{
			\Gamma\ctx{(\Delta, \bracket{\Theta}), \Phi} \vdash \prop{a}
		}
		$
		\end{tabularx}
		\caption{In rule format.}
		\label{subfigure:modal_structural_rules:rules}
	\end{subfigure}\\[\midsep]
	\begin{subfigure}{1\textwidth}
		\centering
		\begin{tabularx}{0.99\textwidth}{@{}cccXccc@{}}
		\begin{tikzpicture}
		\draw node[rectangle, minimum width=50pt,draw=black, minimum height=110pt,dotted,thick,label={$\Gamma$}] (x) at (-0.1,-1.5) {};
		\Tree 
			[ 
				[
					{$\Delta$}
					{$\Theta$}
				]
				\edge[unary]; {$\Phi$}
			]
		\end{tikzpicture}
		&
		\raisebox{60pt}{$\xleftarrow{\rulestyle{ass}_{\diamond}}$}
		&
		\begin{tikzpicture}
		\draw node[rectangle, minimum width=50pt,draw=black, minimum height=110pt,dotted,thick,label={$\Gamma$}] (x) at (0.1,-1.5) {};
		\Tree 
			[
				[.{$\Delta$} ] 
				[
					{$\Theta$}
					\edge[unary]; {$\Phi$}
				]
			]
		\end{tikzpicture}
		&
		&
		\begin{tikzpicture}
		\draw node[rectangle, minimum width=50pt,draw=black, minimum height=110pt,dotted,thick,label={$\Gamma$}] (x) at (-0.1,-1.5) {};
		\Tree
			[
				[
					{$\Delta$}
					{$\Phi$}
				]
				\edge[unary]; {$\Theta$}
			]
		\end{tikzpicture}
		&
		\raisebox{60pt}{$\xleftarrow{\rulestyle{mix}_{\diamond}}$}
		&
		\begin{tikzpicture}
		\draw node[rectangle, minimum width=50pt,draw=black, minimum height=110pt,dotted,thick,label={$\Gamma$}] (x) at (-0.1,-1.5) {};
		\Tree
			[
				[
					{$\Delta$}
					\edge[unary]; {$\Theta$}
				]
				{$\Phi$}
			]
		\end{tikzpicture}
		\end{tabularx}
		\caption{Corresponding tree transformations. Double edges denote bracketed substructures.}
		\label{subfigure:modal_structural_rules:trees}
	\end{subfigure}
	\caption{Controlled associativity/mixed commutativity.}	
	\label{figure:modal_structural_rules}
\end{figure} 


This detour may have proven lengthy, but has hopefully helped us acquire a first taste for modalities.
We now know how to introduce and eliminate them and what the effect of doing so is on the antecedent structure, and got a first glimpse of their properties, the term rewrites they prescribe and the type inequalities (in the form of unidirectional derivations) they give rise to.
The question then becomes how to actually use them for the task at hand, namely disciplined traversal between substructural logics.
Structural reasoning is accomplished via structural postulates, rules of inference that enact commutativity and associativity (or combinations thereof), except in a controlled fashion.
These are permissible only under strict conditions on the substructures constituent to the antecedent structure -- this is exactly where the new kind of structures will prove useful.
There is no fixed vocabulary of structural rules, as they are intended for application-specific finetuning of a universal logical core, so we are free to design and populate it according to our own needs.
Prime examples and standard items for consideration include the controlled associativity and mixed associativity-commutativity rules of Figure~\ref{subfigure:modal_structural_rules:rules} (and the corresponding tree transformations of Figure~\ref{subfigure:modal_structural_rules:trees}, if you have a disdain for brackets).
The first rule $\rulestyle{ass}_{\diamond}$ allows a unary branch $\bracket{\Phi}$ to escape its bind to its neighbour $\Theta$, forcing it to associate to the structure $\Delta$ to its left instead.
The second one $\rulestyle{mix}_{\diamond}$ allows a unary $\bracket{\Theta}$ to swap position with its right-adjacent neighbor $\Phi$, disassociating from its left neighbour $\Delta$ in the process.
In domains where even finer control is needed, one can consider indexed families of (interacting) modalities, each with their own structural brackets and rulesets.

\section{The Linguistic Perspective}\label{section:linguistics}
Despite their presentation having intentionally been left vague and abstract, the ideas explored so far have been a keystone element of computer science, from its inception until recent modernity.
Beyond that, they form the common theoretical underpinnings for the formal treatment of natural languages and their various aspects, where they manifest as so-called \textit{Categorial Grammars}.
Categorial grammars is a heavily overloaded term that refers to a wide and diverse family of related formalisms, each with its own ambitions, goals, strengths and weaknesses.
The most encompassing way of defining a categorial grammar is thus best accomplished through a high-level intersection of their common points.
A categorial grammar is usually tied to a logic, commonly a choice from the ones reviewed so far (or at least loosely inspired by one).
The choice of logic is part personal preference, but is usually motivated by the degree of alignment between the options under consideration and the characteristics of the target language -- a factor that also comes into play is also the trade-off between expressivity and complexity.
On the basis of the chosen logic, a categorial grammar has a \textit{lexicon}; a mapping from primitive linguistic entries (i.e. words) to formulas of that logic.
Their dependence on a lexicon grants categorial grammars their \textit{strongly lexicalized} title -- as the slogan goes, words carry their combinatorics on their sleeves.
With these two components in hand, compiling composite structures for complex linguistic entries (i.e. parsing) becomes a process of formal deduction dictated by the interplay between the types of the participating atomic elements, and the rules of inference the logic is equipped with.
Categorial grammars are a staple of the linguistic tradition and a point of attraction for practitioners, logicians and linguists alike. 
In this section we will examine some of their main strands, with a special emphasis on two spiritual progenitors of the unique flavour that is to be developed and presented later in this thesis.

\subsection{Type-Logical Grammars}
\label{subsection:typelogical}
The earliest take at a categorial grammar are the AB grammars attributed to Kazimierz Adjukiewicz~\cite{ajdukiewicz1935syntaktische} and Yehoshua Bar-Hillel~\cite{bar1953quasi}, but it was Jim Lambek that raised the existing notation and operations into the glory of a fully-fledged type theory.
In their original purpose as envisaged by Lambek, his calculi would find use as \textit{grammar logics}, i.e. universal systems of \textit{grammatical} computation -- a perspective adopted and advanced into what has presently come to be known as type-logical grammars~\cite{morrill1994type,moortgat1997categorial,sep-typelogical-grammar}.
In a natural language setting, the linear base of the Lambek calculi is naturally equated to the resource sensitivity of grammar: words play a single grammatical role in the phrases they help form -- there's no ignoring or reusing items at will.
There, the original Lambek calculus $\LC$ would be the logic of \textit{strings}; it can faithfully portray the generation of natural language utterances, where arbitrary reordering is a destructive process that ruins coherence.
Its stricter version $\NL$ would instead be the logic of \textit{constituency trees}; on top of word order, it further specifies constituency structure, allowing a distinction between different syntactic analyses of the same surface form.
Type-logical grammars extend the Curry-Howard correspondence with a new axis, that of natural language; the transference of points of interest across that axis is presented in Table~\ref{table:CHC-lang}; our motto shall from now on be \textit{parsing as deduction}.

\begin{table}
	\centering
	\begin{tabularx}{0.99\textwidth}{@{}ccC@{}}
	\textbf{Logic}			& \textbf{Computer Science} 	& \textbf{Linguistics}\\
	\toprule
	Propositional Constant	& Base Type						& Syntactic Category\\
	Inference Rule			& Term Rewrite					& Phrase Formation\\
	Axiom					& Variable						& Word (or Empty Category)\\
	Provability				& Type Inhabitation	 			& Grammaticality\\
	Deduction				& Program Synthesis				& Parsing
	\end{tabularx}
	\caption{The Curry-Howard correspondence applied in linguistics.}
	\label{table:CHC-lang}
\end{table}

To see this in action, let's consider first an instantiation of a Lambek calculus $\NL$ with the set of primitive types $\propcon$ populated with signs characterizing the grammatical role of a piece of text that can independently stand on its own (i.e. phrasal categories or, more crudely, parts of speech).
In a toy fragment and for illustrative purposes, this could look like:
\[
	\propcon := \{\smalln, \smallnp, \smallsmain, \smallpp\}
\]
for a grammar able to reason about nouns \smalln, noun phrases and bare nouns \smallnp{}, sentential clauses \smallsmain{}, prepositional phrases \smallpp{} and functions thereof in English.
One might wonder: what happened to the remaining kinds of phrasal categories like verbs, adjectives and adverbs?
These would indicate grammatical functions, and in fact should be represented as such.
An intransitive phrase, for instance, is a grammatical function that would consume a left-adjacent noun phrase to produce a sentence, therefore it would materialize as $\smallnp\divleft\smallsmain$.
It follows that a transitive phrase or copula would then be of type $(\smallnp\divleft\smallsmain)\divright\smallnp$, a function that requires a right-adjacent noun phrase to produce an intransitive phrase, whereas a bitransitive, requiring two, would be $((\smallnp\divleft\smallsmain)\divright\smallnp)\divright\smallnp$, etc.
In the same vein, determiner phrases consume right-adjacent nouns and lift them to noun phrases $\smallnp\divright\smalln$, whereas prenominal adjectives are noun phrase (or noun) endomorphisms modifying them but keeping their type intact, $\smallnp\divright\smallnp$ (and the other way around for postnominal use).
Adverbs would also be endomorphisms, except this time higher-order -- \linebreak$(\smallnp\divright\smallnp)\divright(\smallnp\divright\smallnp)$ for adjectival and $(\smallnp\divleft\smalls)\divleft(\smallnp\divleft\smalls)$ for verbal modification, respectively.

\begin{table}
	\centering
	\begin{tabularx}{0.99\textwidth}{@{}r@{\quad::\quad}ll}
		eye											& $\smalln$\\
		\multicolumn{1}{@{}r@{\quad\hphantom{::}\quad}}{oceans, suns, deeps	, dolphins}				
													& \multicolumn{2}{@{}l}{}\\
		sea-nymphs, whirlpools						& \multicolumn{2}{@{}l}{$\smallnp$}\\
		the											& \multicolumn{2}{@{}l}{$\smallnp\divright\smalln$}\\
		opiate, strange, unrememberable, their		& \multicolumn{2}{@{}l}{$\smallnp\divright\smallnp$}\\
		poured 										& $\smallgtype{itv}$ & $:= \smallnp\divleft\smalls$\\
		behold										& $\smallgtype{tv}$ &  $:=(\smallnp\divleft\smalls)\divright\smallnp$\\
		there 										& $\smallgtype{adv}_{\divleft}$ & $:= (\smallnp\divleft\smalls)\divleft(\smallnp\divleft\smalls)$\\
		never										& $\smallgtype{adv}_{\divright}$ & $ := (\smallnp\divleft\smalls)\divright(\smallnp\divleft\smalls)$\\
		litten 										& \multicolumn{2}{@{}l}{$(\smallnp\divleft\smallnp)\divright\smallpp$}\\
		may											& $\smallgtype{aux}$ & $:= (\smallnp\divleft\smalls)\divright(\smallnp\divleft\smalls)$
	\end{tabularx}
	\caption{Toy lovecraftian lexicon of pure Lambek types.}
	\label{table:toy_lambek_lexicon}
\end{table}

\begin{figure}
		\begin{subfigure}{0.5\textwidth}
		\smaller
			\[
				\infer[\divright E]{\w{strange}\sbind\w{dolphins} \vdash \np}{
					\infer[\Lex]{\w{strange}\vdash \np\divright\np}{}
					&
					\infer[\Lex]{\w{dolphins}\vdash \np}{}
				}
			\]
			\caption{Derivation for \textex{strange dolphins}.}
			\label{subfigure:strange_dolphins}
		\end{subfigure}%
		\begin{subfigure}{0.5\textwidth}
		\smaller
			\[
				\infer[\divright E]{\w{the}\sbind\w{eye} \vdash \np}{
					\infer[\Lex]{\w{the}\vdash \np\divright\n}{}
					&
					\infer[\Lex]{\w{eye}\vdash \n}{}
				}
			\]
			\caption{Derivation for \textex{the eye}.}
			\label{subfigure:the_eye}
		\end{subfigure}\\[\midsep]
		\begin{subfigure}{1\textwidth}
			\smaller
			\[
				\infer[\divright E]{\w{litten}\sbind(\w{by}\sbind\w{suns}) \vdash \np\divleft\np}{
					\infer[\Lex]{\w{litten} : (\np\divleft\np)\divright\pp }{}
					&
					\infer[\divright E]{\w{by}\sbind\w{suns} \vdash \pp}{
						\infer[\Lex]{\w{by}: \pp\divright\np}{}
						&
						\infer[\Lex]{\w{suns}: \np}{}
					}
				}
			\]
			\caption{Derivation for \textex{litten by suns}.}
			\label{subfigure:litten_by_suns}
		\end{subfigure}\\[\midsep]
		\begin{subfigure}{1\textwidth}
			\smaller
			\[
				\infer[\divleft E]{\w{sea-nymphs}\sbind(\w{of}\sbind(\w{unrememberable}\sbind\w{deeps}))\vdash \np}{
					\infer[\Lex]{\w{sea-nymphs}: \np}{}
					&
					\infer[\divright E]{\w{of}\sbind(\w{unrememberable}\sbind\w{deeps}) \vdash \np\divleft\np}{
						\infer[\Lex]{\w{of}: (\np\divleft\np)\divright\np}{}
						&
						\infer[\divright E]{\w{unrememberable}\sbind\w{deeps} \vdash \np}{
							\infer[\Lex]{\w{unremeberable}: \np\divright\np}{}
							&
							\infer[\Lex]{\w{deeps}: \np}{}
						}
					}
				}
			\]
			\caption{Derivation for \textex{sea-nymphs of unrememberable deeps}.}
			\label{subfigure:sea-nymphs of unrememberable deeps}
	\end{subfigure}\\[\midsep]
	\begin{subfigure}{1\textwidth}
		\smaller
		\[
			\infer[\divleft E]{(\w{opiate}\sbind\w{oceans})\sbind(\w{poured}\sbind\w{there}) \vdash \s}{
				\infer[\divright E]{\w{opiate}\sbind\w{oceans} \vdash \np}{
					\infer[\Lex]{\w{opiate} : \np\divright\np}{}
					&
					\infer[\Lex]{\w{oceans}: \np}{}
				}
				&
				\infer[\divleft E]{\w{poured}\sbind\w{there} \vdash \np\divleft\s}{
					\infer[\Lex]{\w{poured}: \np\divleft\s}{}
					&
					\infer[\Lex]{\w{there}: \gtype{adv}_{\divleft}}{}
				}
			}
		\]
		\caption{Derivation for \textex{opiate oceans poured there}.}
		\label{subfigure:opiate_oceans_poured_there}
	\end{subfigure}
	\caption{Deriving simple multiplicative phrases in $\NL$.}
	\label{figure:nl_applicative_examples}
\end{figure}

Linguistic reasoning is not done \textit{ex nihilo} -- formulas like the above are supplied by and grounded in the lexicon. 
This does not exclude the option of utilizing hypotheticals instantiated by the axiom rule $\Ax$ -- hypothetical reasoning lives, in fact, at the core of the type-logical inferential process, as we will soon see.
It means, rather, that our building blocks will for the most part be \textit{lexical constants}, proof objects that behave just like variables, except they are neither wantonly typed nor amenable to abstraction.
To convey the difference between the two, we will instantiate the latter with a seemingly new rule of inference, $\Lex$, which simply performs lexical lookup, i.e. pulls a word's type from the lexicon.

The internet guide \textit{how to write a dissertation} I am consulting insists it is important to set clear goals and stick to them.
It seems like sound advice, so we are going to do just that, and attempt to demonstrate the analysis of a non-contrived example in the type-logical framework.
The following looks like a fitting match:
\begin{quote}
\textex{Opiate oceans poured there, litten by suns that the eye may never behold, and having in their whirlpools strange dolphins and sea-nymphs of unrememberable deeps.}
\begin{flushright} H.P. Lovecraft, \textit{Azathoth}  (1938). In \textit{Leaves (2).}\end{flushright}
\end{quote}
Let's pave the way towards this ambitious goal with the miniature mock-up lexicon of Table~\ref{table:toy_lambek_lexicon}, and see just how far it can get us.

Figure~\ref{figure:nl_applicative_examples} presents derivations for parts of the goal phrase, and our very first linguistic examples (!) -- their purely applicative nature should make them straightforward to decipher.
The two proofs of~\ref{subfigure:opiate_oceans_poured_there} and~\ref{subfigure:litten_by_suns} can readily be combined to yield a derivation for the phrase \textex{opiate oceans litten by suns poured there}. 
Close, but not quite there...
The participial \textex{litten}, which acts here as a postnominal modifier, has the special property of being able to position itself either immediately after the noun phrase \textex{opiate oceans} it modifies, or deferred until after the matrix head \textex{poured} has made an appearance (with any adverbials attached to it).
Attempting to produce a derivation for the original version seems like a dead-end enterprise, though.
We are not to blame for this incompetence: the problem lies with the grammar -- we could never hope to capture this behavior with our current machinery.
Despite their elegance and formal appeal, grammars relying purely on Lambek calculi suffer from an aversion to anomalies like discontinuities and long-distance dependencies, which natural languages tend to exhibit at an unfortunately striking degree.

One could of course attept to cop out of the problem by just introducing ad-hoc raised forms for movable parts, one per distinct position they can be found at. 
The repercussions of such a move would soon, however, prove catastrophic.
On the one hand, the once reliably concise lexicon would become overpopulated by endless variations on the same theme: each expansion point of a lexical type would percolate into all other lexical items it interacts with (either as consumers or producers thereof), the effect cascading at progressively larger lexical neighborhoods, until (if ever) an eventual equilibrium is reached.
On the other hand, raised types obfuscate the functional relations and constituency structures we have worked so hard to reveal and incorporate, virtually beating the very purpose of the logic.
Relaxing the structural constraints of the logic to globally allow movement and/or rebracketing is no good either.
Spurious ambiguity would be the least of our concerns as we would be faced with \textit{overgeneration}, i.e. the unwelcome ability to derive proofs that have no correspondence to correct linguistic structures whatsoever, leading us back to square zero.
If you have not skipped any parts yet, your reward should now manifest as an unwavering faith for a solution, and a premonition of what is to come: modalities to the rescue!

\subsubsection{The Role of Modalities}
Ever since their original integration with the vanilla multiplicative toolkit, (the early pioneer being none other than my \#1 supervisor!) modalities have played an indispensable role in the history and development of type-logical grammars~\cite{hendriks1995ellipsis, moortgat1996multimodal,kurtonina1997structural,moortgat1997categorial,vermaat1999controlling}.
They find use as either licensors or licensees of structural rewrites, now in the form of movement and rebracketing of words and phrases.
Figure~\ref{figure:lovecract_rel_clause} progresses our agenda by accounting for the presence of a (hypothetical) movable postnominal modifier via the rules of Figure~\ref{figure:modal_structural_rules}.
To make the hypothesis movable, we need to instantiate it as a box -- for the pure function contained therein to be applicable, the box needs to be removed, enclosing the hypothesis in angular brackets, which in turn license its structural extraction to the rightmost edge of the assumptions via the $\rulestyle{mix}_{\diamond}$ rule.
At that point, we need to eliminate the bracketed variable with a term of the corresponding type, plus a diamond.
For this to work, we need to make the tiniest of modifications to our lexicon so as to get access to the saught-after diamond:
\begin{equation}\label{equation:litten}
	\w{litten} \qquad :: \qquad \diamond\bx (\np[s]\divleft\np[s])\divright\pp[s]
\end{equation}
Intuitively, the new type requests a prepositional phrase complement to the right, after the consumption of which it produces a movable postnominal modifier that can penetrate constituent phrase boundaries to the left.
Equipped with it, we can derive both the local versions hinted at earlier, and their discontinuous variations; see Figure~\ref{figure:lovecraft_postnominal} for a proof of concept.%
\footnote{Get it? It's an actual \textit{proof}.} 

\begin{figure}
	\centering
	\begin{subfigure}{1\textwidth}
		\smaller
		\[
			\infer[\rulestyle{mix}_{\diamond}]{((\w{opiate}\sbind\w{oceans})\sbind(\w{poured}\sbind\w{there}))\sbind\bracket{\term{\vari }} \vdash \s}{
				\infer[\divleft E]{((\w{opiate}\sbind\w{oceans})\sbind\bracket{\term{\vari }})\sbind(\w{poured}\sbind\w{there}) \vdash \s}{
					\infer[\divleft E]{(\w{opiate}\sbind\w{oceans})\sbind\bracket{\term{\vari }} \vdash \np}{
						\infer*{\w{opiate}\sbind\w{oceans} \vdash \np}{}
						&
						\infer[\bx E]{\bracket{\term{\vari }} \vdash \np\divleft\np}{
							\infer[\Ax]{\term{\vari }: \bx (\np\divleft\np)}{}
						}
					}
					&
					\infer*{\w{poured}\sbind\w{there} \vdash \np\divleft\s}{}
				}
			}
		\]
		\caption{Extracting a hypothetical postnominal modifier...}
		\label{subfigure:control:movement}
	\end{subfigure}\\[\midsep]
	\begin{subfigure}{1\textwidth}
	\centering
		\smaller
		\[
			\infer[\diamond E]{((\w{opiate}\sbind\w{oceans})\sbind(\w{poured}\sbind\w{there}))\sbind(\w{litten}\sbind(\w{by}\sbind\w{suns})) \vdash \s}{
				\infer{((\dots)\sbind(\dots))\sbind\bracket{\term{\vari }} \vdash \s}{\text{(\ref{subfigure:control:movement})}}
				&
				\infer[\divright E]{\w{litten}\sbind(\w{by}\sbind\w{suns}) \vdash \diamond\bx(\np\divleft\np)}{
					\infer[\Lex]{\w{litten} : \diamond\bx(\np\divleft\np)\divright\pp }{}
					&
					\infer[\divright E]{\w{by}\sbind\w{suns} \vdash \pp}{
						\infer[\Lex]{\w{by}: \pp\divright\np}{}
						&
						\infer[\Lex]{\w{suns}: \np}{}
					}
				}
			}
		\]
		\caption{...before substituting the hypothesis for its material instance.}
		\label{subfigure:control:substitution}
	\end{subfigure}
	\caption{Deriving long-distance postnominal modification with the aid of type assignment~(\ref{equation:litten}).}
	\label{figure:lovecraft_postnominal}
\end{figure}


This methodology is in fact adopted from \citet{moortgat1999constants}, where it finds similar use in dealing with the grammatical ambivalence of relativizers like \textex{that} or \textex{which}.
Bound relative clauses headed by complementizers like the above contain a subordinate sentence with a \textit{gap}, which can vary in its position.
Let's make things unnecessarily convoluted for the sake of clich{\'e}d self-referentialism by considering the relative clause \textex{which can very in its position} of the previous sentence.
There, the subordinate clause \textex{\gap{} can vary in its position} contains a gap in the subject position, which the head \textex{a gap} occupies implicitly.
This is not the case in the last relative clause \textex{which the head \textex{gap} occupies implicitly}, whose subordinate clause \textex{the head \textex{gap} occupies \gap{} implicitly} contains a non-peripheral (nested) gap in direct object position.
What a mess! 
The subject-relative case can easily be dealt with in a pure Lambek grammar, as the gap hypothesis occurs adjacent to the verb phrase, but
the same cannot be said for the object-relative case, whose structurally free gap seems to pose a challenge.
The solution comes in the form of two distinct type assignments for the relativizer, one per grammatical role fulfilled:
\begin{align}
	\w{that} &:: \subcat{l}{rel}{s} := (\np\divleft\np)\divright(\np\divleft\s)\\
	\w{that} &:: \subcat{l}{rel}{o} := (\np\divleft\np)\divright(\s\divright\diamond\bx \np ) \label{equation:objrel}
\end{align}
The second version launches a mobile \np[s] hypothesis via the same diamond-box pattern showcased earlier.
The proof of Figure~\ref{figure:lovecract_rel_clause} employs this typing in combination with the $\rulestyle{ass}_{\diamond}$ rule to derive the object-relative clause \textex{that the eye may never behold}, which applied to \textex{suns} and combined with the proof of Figure~\ref{figure:lovecraft_postnominal} yields the correct form of the postnominal modifier \textex{opiate oceans poured there, litten by suns that the eye may never behold}, bringing us one step closer to success.

\begin{figure}
	\centering
	\begin{subfigure}{1\textwidth}
		\smaller
		\[
			\infer[\divright E]{\gamma := \w{that}\sbind((\w{the}\sbind\w{eye})\sbind(\w{may}\sbind(\w{never}\sbind\w{behold}))) \vdash \np\divleft\np}{
				\infer[\Lex]{\w{that} : \subcat{l}{rel}{o}}{}
				&
				\hspace{-40pt}\infer[\divright I]{(\w{the}\sbind\w{eye})\sbind(\w{may}\sbind(\w{never}\sbind\w{behold})) \vdash \s\divright\diamond\bx\np}{
						\infer[\diamond E]{((\w{the}\sbind\w{eye})\sbind(\w{may}\sbind(\w{never}\sbind\w{behold})))\sbind\term{\varj}\vdash \s}{
							\infer[\rulestyle{ass}_{\diamond}]{((\w{the}\sbind\w{eye})\sbind(\w{may}\sbind(\w{never}\sbind\w{behold})))\sbind\bracket{\term{\vari }}\vdash \s}{
								\infer[\rulestyle{ass}_{\diamond}]{(\w{the}\sbind\w{eye})\sbind((\w{may}\sbind(\w{never}\sbind\w{behold}))\sbind\bracket{\term{\vari }})\vdash \s}{
									\infer[\rulestyle{ass}_{\diamond}]{(\w{the}\sbind\w{eye})\sbind(\w{may}\sbind((\w{never}\sbind\w{behold})\sbind\bracket{\term{\vari }}))\vdash \s}{
										\infer[\divleft E]{(\w{the}\sbind\w{eye})\sbind(\w{may}\sbind(\w{never}\sbind(\w{behold}\sbind\bracket{\term{\vari }})))\vdash \s}{
											\infer{\w{the}\sbind\w{eye} \vdash \np}{\text{(\ref{subfigure:the_eye})}}
											&
											\infer[\divright E]{\w{may}\sbind(\w{never}\sbind(\w{behold}\sbind\bracket{\term{\vari }})) \vdash \np\divleft\s}{
												\infer[\Lex]{\w{may} : \gtype{aux}}{}
												&
												\infer[\divright E]{\w{never}\sbind(\w{behold}\sbind\bracket{\term{\vari }}) \vdash \np\divleft\s}{
													\infer[\Lex]{\w{never}:\smallgtype{adv}_{\divright}}{}
													&
													\infer[\divright E]{\w{behold}\sbind{\bracket{\term{\vari }}} \vdash \np\divleft\s}{
														\infer[\Lex]{\w{behold}: \gtype[l]{tv}}{}
														&
														\infer[\bx E]{\bracket{\term{\vari }} \vdash \np}{
															\infer[\Ax]{\term{\vari } : \bx \np}{}
														}
													}
												}
											}
										}
									}
								}
							}						
							&
							\hspace{-52pt}
							\infer[\Ax]{\term{\varj}: \diamond\bx \np}{}
						}
					}
				}
		\]
		\caption{Deriving an object-relative clause...}
		\label{subfigure:lovecraft_rel_clause:rc}
	\end{subfigure}\\[\smallsep]
	\begin{subfigure}{1\textwidth}
		\smaller
		\[
			\infer[\divright E]{\w{litten}\sbind(\w{by}\sbind(\w{suns}\sbind(\w{that}\sbind((\w{the}\sbind\w{eye})\sbind(\w{may}\sbind(\w{never}\sbind\w{behold})))))) \vdash \diamond\bx(\np\divleft\np)}{
				\infer[\Lex]{\w{litten} : \diamond\bx(\np\divleft\np)\divright\pp}{}
				&
				\infer[\divright E]{\w{by}\sbind(\w{suns}\sbind\gamma) \vdash \pp}{
					\infer[\Lex]{\w{by} : \pp \divright \np}{}
					&
					\infer[\divleft E]{\w{suns}\sbind\gamma \vdash \np}{
						\infer[\Lex]{\w{suns}: \np}{}
						&
						\infer{\gamma \vdash \np\divleft\np}{\text{(\ref{subfigure:lovecraft_rel_clause:rc})}}
					}
				}
			}
		\]
		\caption{...and using it to derive the full long-distance postnominal modifier.}
		\label{subfigure:lovecraft_rel_clause:suns}
	\end{subfigure}
	\caption{An object-relative clause in action, prompted by type assignment~(\ref{equation:objrel}).}
	\label{figure:lovecract_rel_clause}
\end{figure}

\subsubsection{Intricacies of the Lexicon}
\label{subsubsection:intricacies}
The analysis just performed illustrated the necessity of (at least) two distinct types for the same string \textex{that}, hinting at the fact that the lexicon is \textit{not a function} from words to types, but rather a \textit{relation} between them.
One, more opinionated than I, might argue that each type is mapped to a distinct lexical item (one per relativization type), and that the identification between their strings is a mere coincidence, an idiosyncracy of the language, or anyway irrelevant; even if a string is multi-typed, each type is a witness to a unique latent word hiding behind it.
Of different effect but similar flavour would be the line of defense that appeals to null syntax, a covert process that can conditionally nominalize infinitives, determine plural nouns, relativize gerunds or do any sort of thing, really; a word is never multi-typed, but ad-hoc type conversions can take place out of the blue.
Even under premises as radical as the above, occassions of type undeterminism are all but rare.
Consider for instance the verb \textex{to have}, whose argument structure for the possessive meaning alone) is specified (according to its FrameNet entry~\cite{baker1998berkeley}) as having mandatory owner and possession semantic arguments (corresponding to syntactic subject and direct object), but also any combination of depictive, duration, explanation, manner and temporal optional complements, in various orders -- each variation necessarily expressed with a distinct type.
In our case, we need the type:
\begin{equation}
	\w{having} :: (\diamond\bx(\np\divleft\np)\divright\np)\divright\pp
\end{equation}
for a gerund that requisits first a prepositional complement phrase and then an object noun phrase (i.e. \textit{having somewhere something}) to act as a movable postnominal modifier (an argument permutation that FrameNet does not even contain an example of!).

The reality of optional arguments and non-trivial argument order variations alone should suffice to convince us of the issue at hand: \textit{lexical type ambiguity} is a real phenomenon, and one that is here to stay.
Having acknowledged that, the question shifts to how we deal with it.
From a theoretical perspective, we can incorporate the question of type choice into our proof-machinery via the additive conjunction $\with$ of $\ILL$, which is essentially recovering the functional nature of our lexicon, with type assignments reformulated as nested choices:
\begin{equation}
 \smallprop{A}_1 \with (\smallprop{A}_2 \with (\smallprop{A}_3 \dots (\smallprop{A}_{n-1}\with\smallprop{A}_{n})))
\end{equation}
and the subscript enumerating each of the possible instantiations in the context of a single sentence.
Under this regime, the lexical assignment rule $\Lex$ would need to be followed by a sequence of projections to isolate the desired type, contributing little other than excessive verbosity.%
\footnote{
A more ambitious usecase could allow the \textit{simultaneous} derivation of multiple unique analyses, and the incorporation of derivational ambiguity arising out of lexical choice as a first class citizen of the proof theory -- a proof object that resides \textit{within} it rather than a notion in the meta-theory \textit{above} it.
The repercussions of this would be magnificent for semantic applications, but no concrete results that I am aware of were ever produced in that direction.}
Given the limited use we would have for all this ``proof waste'', we will stick with the current formulation of the $\Lex$ rule -- if it helps us sleep better at night, we can imagine it as a shorthand notation for the correct sequence of projections requested by the current analysis, the construction of which we have delegated to a silent and omnipotent oracle.
Be at rest knowing that this oracle will be temporary and for presentation purposes only; we will address its demystification later on.

The ambiguity problem is exacerbated and pushed to the limit by function words enacting context-dependent chameleon roles.
Coordinators are the main culprit; they can bind together pairs of the same (almost) arbitrary type to produce an instance of the conjoined pair, a complex phrase of the same type.
We will write:
\begin{equation}\label{equation:polymorphic_nl_crd}
(\chi\divleft\chi)\divright\chi
\end{equation}
to denote the coordinator type pattern parameterized over the \textit{type variable} $\chi$, which can be instantiated as any type of our type grammar.%
\footnote{
This is in fact an exemplar of \textit{parametric polymorphism}, which is properly formalized in second-order intuitionistic logic and its type-equivalent System F~\cite{girard1972interpretation, reynolds1974towards}. 
\begin{center}
\begin{tabularx}{0.6\textwidth}{@{}cc@{}}
$\infer[\Pi I]{\Gamma \vdash \lam \alpha.\term{M}: \Pi \alpha.\sigma}{\Gamma, \alpha : \prop{TYPE} \vdash \term{M}: \sigma}$
&
$\infer[\Pi E]{\Gamma, \Delta \vdash \term{M~\prop{b}}: \sigma_{[\alpha\mapsto\smallprop{B}}]}{\Gamma \vdash\term{M}: \Pi \alpha.\sigma &  \Delta \vdash \term{B}: \prop{TYPE}}$
\end{tabularx}
\end{center}
The rules above showcase the introduction and elimination of types quantified over types $\Pi \alpha.\sigma$, and the term analogue of abstracting over types $\lam \alpha.M$.
In this notation, a coordinator would be a quantification of type $\Pi\chi.(\chi\divleft\chi)\divright\chi$, that when reduced against arbitrary type $\smallprop{A}$ would yield $(\smallprop{A}\divleft\smallprop{A})\divright\smallprop{A}$.
Other than this unique occurrence of polymorphism, second order term and type constructions are an overkill to our purposes here, relegating this comment to footnote status.
}

Armed with this last trick, we are now in possession of all the knowledge necessary to finally tackle the full derivation.
First, we must instantiate the polymorphic coordinator once by substituting $\chi$ for $\np[s]$ to derive the noun phrase conjunction \textex{strange dolphins and sea-nymphs of unremememberable deeps}, as portrayed in Figure~\ref{subfigure:strange_dolphins_and_sea_nymphs}.
This, together with our freshly typed \textex{having}, allows the derivation of the mobile postnominal modifier \textex{having in their whirlpools strange dolphins and sea-nymphs of unrememberable deeps}, as in Figure~\ref{subfigure:having_in_their_whirlpools}.
At this point, we must employ another instance of the polymorphic coordinator, this time substituting $\chi$ for $\diamond\bx (\np\divleft\np[s])$ -- this opens the door to the derivation of the structurally free complex postnominal modifier \textex{litten by suns that the eye may never behold and having in their whirlpools strange dolphins and sea-nymphs of unrememberable deeps}, which can apply to the nested \textex{opiate oceans} in the same fashion as the proof of Figure~\ref{figure:lovecraft_postnominal}.
At long last, we are rewarded with a type-checking and syntactically faithful analysis of the full sentence (and a check mark on \textit{how to write a dissertation}).
Collaging these last bits together is left as an exercise to the motivated reader, for fear of repetition sterilizing the quotation of its beauty.

\begin{figure}
	\begin{subfigure}{1\textwidth}
		\smaller[2]
			\[
				\infer[\divleft E]{\delta := (\w{strange}\sbind\w{dolphins})\sbind(\w{and}\sbind(\w{sea-nymphs}\sbind(\w{of}\sbind(\w{unrememberable}\sbind\w{deeps}))))\vdash \np}{
					\infer{\w{strange}\sbind\w{dolphins} \vdash \np}{\text{(\ref{subfigure:strange_dolphins})}}
					&
					\infer[\divright E]{\w{and}\sbind(\w{sea-nymphs}\sbind(\w{of}\sbind(\w{unrememberable}\sbind\w{deeps}))) \vdash \np\divleft\np}{
						\hspace{-2pt}
						\infer[\Lex]{\w{and}: (\np\divleft\np)\divright\np}{}
						&
						\infer{\w{sea-nymphs}\sbind(\w{of}\sbind(\w{unrememberable}\sbind\w{deeps}))\vdash \np}{\text{(\ref{subfigure:sea-nymphs of unrememberable deeps})}}
					}
				}
			\]
			\caption{Deriving noun-phrase coordination...}
			\label{subfigure:strange_dolphins_and_sea_nymphs}
	\end{subfigure}
	\begin{subfigure}{1\textwidth}
		\smaller[2]
		\[
			\infer[\divright E]{(\w{having}\sbind(\w{in}\sbind(\w{their}\sbind\w{whirlpools})))\sbind\delta) \vdash \diamond\bx(\np\divleft\np)}{
				\infer[\divright E]{\w{having}\sbind(\w{in}\sbind(\w{their}\sbind\w{whirlpools})) \vdash \diamond\bx(\np\divleft\np)\divright\np}{
					\infer[\Lex]{\w{having}: (\diamond\bx(\np\divleft\np)\divright\np)\divright\pp}{}
					&
					\infer[\divright E]{\w{in}\sbind(\w{their}\sbind\w{whirlpools}) \vdash \pp}{
						\infer[\Lex]{\w{in}: \pp\divright\np}{}
						&
						\infer[\divright E]{\w{their}\sbind\w{whirlpools} \vdash \np}{
							\infer[\Lex]{\w{their}: \np\divright\np}{}
							&
							\infer[\Lex]{\w{whirlpools}: \np}{}
						}
					}
				}
				&
				\hspace{-10pt}
				\infer{\delta \vdash \np}{\text{(\ref{subfigure:strange_dolphins_and_sea_nymphs})}}
			}
		\]
		\caption{...and using it to construct yet another postnominal modifier.}
		\label{subfigure:having_in_their_whirlpools}
	\end{subfigure}
	\caption{Filling in the missing bits using the polymorphic type~(\ref{equation:polymorphic_nl_crd}).}
	\label{figure:lovecraft_coord}
\end{figure}

\subsubsection{Subtleties of Proof Search}
The last sentence was merely a test to weed out the uncommited.
Of those that passed it and attempted to really proceed with the derivation, the observant ones should have found themselves at multiple crossroads regarding the order of applying the numerous modifiers in the sentence -- a matter carefully concealed in the derivations presented so far.
The choice of $\NL$ over $\LC$ implies that scope assigned to competing modifiers should reflect in a corresponding judgement that differs to the rest in the bracketing structure of its antecedents (and of course the proof justifying it).
The following endsequents are all valid alternatives provable with the lexical types of Figure~\ref{subfigure:strange_dolphins_and_sea_nymphs}:

{\smaller
\begin{enumerate}
\item $\w{strange}\sbind(\w{dolphins}\sbind(\w{and}\sbind(\w{sea-nymphs}\sbind(\w{of}\sbind(\w{unrememberable}\sbind\w{deeps})))))$
\item $\w{strange}\sbind((\w{dolphins}\sbind(\w{and}\sbind\w{sea-nymphs}))\sbind(\w{of}\sbind(\w{unrememberable}\sbind\w{deeps})))$
\item $(\w{strange}\sbind(\w{dolphins}\sbind(\w{and}\sbind\w{sea-nymphs})))\sbind(\w{of}\sbind(\w{unrememberable}\sbind\w{deeps}))$
\item $((\w{strange}\sbind\w{dolphins})\sbind(\w{and}\sbind\w{sea-nymphs}))\sbind(\w{of}\sbind(\w{unrememberable}\sbind\w{deeps}))$
\item $(\w{strange}\sbind\w{dolphins})\sbind(\w{and}\sbind(\w{sea-nymphs}\sbind(\w{of}\sbind(\w{unrememberable}\sbind\w{deeps})))))$
\end{enumerate}
}
This is an admittedly stretched case of \textit{derivational ambiguity}, a situation where from the same lexical assignments one can obtain multiple syntactic analyses, which may correspond to equinumerous subtly or drastically diverging semantic interpretations (more on that in a bit).
Derivational ambiguity is not necessarily bad, provided the divergence in the proofs constructed is linguistically meaningful.%
\footnote{Just think of all the different things you could do with pijamas, elephants, telescopes, etc.}
What is, however, worth noting is the structural discrepancy between what we see (a flat sequence) and what we want to parse into (a binary branching tree).
Even though constituency structure is de facto acknowledged by linguistic theory, it is a latent mental construct revealed through (or assigned by) the parsing process, rather than an observable feature of text that we can assume as a given.
The connotation of this is that even though backwards proof search in $\NL$ may find use in \textit{verifying} the plausibility of a type-assigned, pre-bracketed phrase, forward search is necessary in \textit{eliciting} a type and a bracketing structure from a phrase. 

\subsubsection{Syntax-Semantics Interface}\label{subsubsection:ssi_tlg}
The game played so far, challenging as it may be, might prove dull to someone indifferent to syntax or its type-theoretic formulation; we will attempt to fix that by expanding our target crowd to semanticists and Montagovian grammarians, who are said to recite daily before beditme:
\begin{quote}
I fail to see any interest in syntax except as a preliminary to semantics.~\cite{montague1970universal}
\end{quote}

\paragraph{Montague's Insights}
A full exposition to Montague grammar is beyond the scope of this thesis, but a brief introduction to some of its foundations will go a long way in helping us perceive its relevance to the type-logical approach.
Richard Montague was disillusioned with the tackling of natural language semantics at the time, which he found formally inadequate and lacking the elegance of contemporary approaches to mathematical syntax.
He saught to fill this gap by arguing that formal and natural languages are morally indistinguishable -- different instantiations of the same theory -- and advocating their treatment in just the same way.
Influenced by his own background on modal logic and the highly influential work of Saul Kripke on possible world semantics~\cite{kripke1963semantical}, the machinery he thought was best fit for the task at hand was a model theoretic semantics axiomatized on the basis of set theory and higher-order logic; his work is marked with heavy use of $\lam$ notation, the adoption of which by today's working linguist is largely attributed to him.

Revolutionary as it may have been at the time, this semantic machinery and its antiquated details are largely irrelevant to this work.
What is of prime interest, though, is Montague's treatment of the passage between syntax and semantics.
In his view, if syntax is an algebra describing the process of synthesizing a grammatically passable sentence, semantics is another algebra providing a logical recipe for evaluating that sentence's truth-validity.
The two systems are viewed as distinct, but not independent: they are connected by a unidirectional transformation that preserves and transports (certain aspects of) the structure of the former into the latter, in other words a \textit{homomorphism}.
The slogan ``syntax is an algebra, semantics is an algebra and meaning is a homomorphism between them'' summarizes this notion~\cite{janssen2014foundations}.
The gracefulness of this statement is easy to miss. 
It proclaims that the semantic expression assigned to complex linguistic entries mimics (or is at least informed by) the structural form of their syntactic analyses.
This perspective actuates the ideal of compositionality, a concept passed down by Gottlob Frege and summarized as stating that the meaning of a complex expression is computable on the basis of its primitive expressions and the rules that dictate their combination~\cite{partee1984compositionality}.

\paragraph{The Type-Logical View}
Let's appropriate this view and translate it to the type-logical setup, as done by \citet{van1988semantics}.
Here, syntax is a type theory: a logic whose rules are equated to term rewrite instructions, and proofs to programs.
Semantics can also be a type theory; one with its own types and terms, potentially more expressive and certainly unriddled by (some of) the structural constraints of grammar.
The meaning interpretation would then be a homomorphism that translates syntactic proofs and programs to corresponding semantic ones -- a translation from one constructive logic to another.
Its design would need to follow the rule-to-rule approach, according to which every syntactic construction will have its homomorphic image in the target system~\cite{bach1976extension}.
This viewpoint is quite open-ended and admits a whole lot of creative liberty with respect to the the nature of the target system and the details of the translations.
The only constraint imposed is the only one that matters: the high-level principle of compositionality needs to hold, i.e. the function/argument structure specified by syntax need to be carried through to the semantics.

Interestingly, the approach permits a division of labour between syntax, semantics and everything in between: the end-to-end translation can be decomposed into a sequence of homomorphisms, each intermediate step explicating an additional layer of added expressivity (or fortfeited structure) and singling out a subset of the desiderata towards the end-target.
A natural first stop would be that of $\ILL_{\li}$ as a \textit{derivational semantics} logic: it captures the function/argument structures prescribed by the syntactic proof and respects its no-reuse principle, but without the semantically void headaches of order and bracketing structures, or that of the rules manipulating them. 

To make things concrete, let's consider this in the context of the source logic $\Sigma$ being identified with the instantiation of $\NL_{\diamond,\bx}$ of the previous section, and the intermediate logic $\Tau$ being its $\ILL_{\li}$ mirror image.
Using the superscript $X := \Sigma \ | \ \Tau$ to distinguish between the two logics,
we will denote with $\propcon^X$ the set of atomic types of X, and $\types^X$ its type universe, i.e. the inductive closure of types under type operators.
Similarly, we will denote with $\cons^{X}$ its set of constants, $\vars^{X}$ its set of variable names, and $\terms^{X}$ its well-formed terms, i.e. the inductive closure of terms under term operators.

\begin{figure}
	\centering
	\begin{tikzpicture}
		\node	(ss)  at (0, 0.5) 	{Surface Syntax};
		\node[outer sep=5pt]	(nlp) at (0, 0) 	{$\Sigma := \logic{NL}_{\diamond,\bx}$};
		\node	(ds)  at (4.75, 0.5) 	{Derivational Semantics};
		\node[outer sep=10pt]  	(ill) at (4.75, 0) 	{$\Tau := \ILL_{\li}$};
		\draw[->] (nlp) -- node[above] {$\hm{.}$}  (ill) ;
		\node[outer sep=5pt] (target) at (9, 0)	{$?$};
		\node	(ts)  at (9, 0.5) 	{Target Semantics};
		\draw[->, dotted, thick] (ill) --  (target) ;
	\end{tikzpicture}
	\caption{The syntax-semantics interface in the type-logical setting.}
	\label{figure:synsemtlg}
\end{figure}

The homomorphism $\hm{.}$ operates on proofs, i.e. typed terms, and thus does double duty: it transforms both terms and types of $\Sigma$ to corresponding terms and types of $\Tau$.
It is handy, then, to define it on the basis of two components $\langle \eta, \theta \rangle$, where $\eta : \types^{\Sigma} \to \types^{\Tau}$ and $\theta: \terms^{\Sigma} \to \terms^{\Tau}$, such that $\hm{\smallterm{s}: \smallprop{A}} = \theta({\smallterm{s}}) : \eta({\smallprop{A}})$, where the typing relation at the right-hand side of the equation must hold (i.e. the two maps mutually respect derivability).
On the type level, $\eta$ must specify a pointwise mapping $\eta_0$ from the propositional constants $\propcon^{\Sigma}$ of the source logic to types $\types^{\Tau}$ of the intermediate logic.
In our case, we will consider this a bijection from $\propcon^{\Sigma}$ to $\propcon^{\Tau}$, such that $\eta_0(p) \mapsto p$ (i.e. instantiating $\propcon^{\Tau}$ as a literal copy of $\propcon^{\Sigma}$).
Then, to extend $\eta_0$ to $\eta$ we need to specify its action on complex types, where it essentially forgets the unary modalities and removes the directionality of the implications, as shown in Table~\ref{table:eta_nl}.
In the exact same vein, $\theta$ pointwise sends constants and variables to their copycat images, and is then inductively defined on complex terms, where it casts directional applications and abstractions to undirectional ones, drops modal decorations and performs the simplified substitution prescribed by the $\diamond E$ rule, as shown in Table~\ref{table:theta_nl}.
As an example, applying $\hm{.}$ to the proof of Figure~\ref{figure:lovecract_rel_clause} should yield the derivational term:
\begin{equation}\label{equation:litten_derterm}
\term{litten~(by~(that~(\lam \vari .(may~(never~(behold~\vari )))~(the~eye)))~(suns))}^{\prop{np}\li\prop{np}}
\end{equation}

\begin{table}[ht]
\begin{center}
	\begin{tabularx}{0.51\textwidth}{@{}c@{\quad$\mapsto$\quad}c@{}}
		\multicolumn{1}{@{}c}{$\types^{\Sigma}$} & \multicolumn{1}{c@{}}{$\types^{\Tau}$}\\
		\toprule
		$p \in \propcon^{\Sigma}$							& $\eta_0(p) := p \in \propcon^{\Tau}$\\
		$\smallprop{A}\divleft\smallprop{B}$,		
		$\smallprop{B}\divright\smallprop{A}$				& $\eta(\smallprop{A})\li\eta(\smallprop{B})$\\
		$\diamond\smallprop{A}$, 	$\bx\smallprop{A}$		& $\eta(\smallprop{A})$
	\end{tabularx}
	\caption{Translating $\NL_{\diamond,\bx}$ types to $\ILL_{\li}$.}
	\label{table:eta_nl}
\end{center}
\end{table}

\begin{table}[ht]
\begin{center}
	\begin{tabularx}{0.52\textwidth}{@{}c@{\quad$\mapsto$\quad}c@{}}
		\multicolumn{1}{@{}c}{$\terms^{\Sigma}$} & \multicolumn{1}{c@{}}{$\terms^{\Tau}$}\\
		\toprule
		$\w{c} \in \cons^{\Sigma}$							& $\theta_0(\w{c}) := \w{c} \in \cons^{\Tau}$\\
		$\smallterm{\vari } \in \vars^{\Sigma}$				& $\theta_0(\smallterm{\vari }) := \smallterm{\vari } \in\vars^{\Tau}$\\
		$\smallterm{s}\nappright\smallterm{t}$, 		
		$\smallterm{t}\nappleft\smallterm{s}$				& $\theta(\smallterm{s})~\theta(\smallterm{t})$\\
		$\lam\smallterm{\vari }.s$,
		$\adbmal\smallterm{\vari }.s$							& $\lam\theta(\smallterm{\vari }).\theta(\smallterm{s})$\\
		$\diaintro \smallterm{s}$, 
		$\boxintro \smallterm{s}$, 
		$\boxelim \smallterm{s}$							& $\theta(\smallterm{s})$\\
		$\caseof{\diaelim \smallterm{t}}{\smallterm{\vari }}{\smallterm{s}}$
															& $\theta(\smallterm{s})_{[\theta(\smallterm{\vari }) \mapsto \theta(\smallterm{t})]}$
	\end{tabularx}
	\caption{Translating $\NL_{\diamond,\bx}$ terms to $\ILL_{\li}$.}
	\label{table:theta_nl}
\end{center}
\end{table}

With the Curry-Howard isomorphism as our guiding star, there's no peril in navigating between syntactic and semantic theories.
Syntactic proofs are equated to syntactic terms, on which our homomorphism can be applied to yield derivational semantics terms, in turn equatable to derivational semantics proofs.
This might seem like a lot of work to simply ``forget'' syntax, but it showcases how one can step up the computational hierarchy of substructural logics in order to attain access to more expressive semantics.
Note also that such a path is merely a suggestion and not an imperative; a more ambitious line of though could maintain that word order variations (and the structural rules licensing them) can carry semantic cues which, albeit subtle, need to be upheld in the compositional meaning translation (see for instance  the contemporary work of \citet{duarte2022quantum} for some exotic interpretations of the control modalities).

\paragraph{The Role of the Lexicon}
The sentiments of the previous paragraph could be met with some skepticism.
A critical eye might argue that semantic interactions not already manifested in the syntax may never be born of this process, and thus wonder whether this added expressivity can serve any real purpose or offer any tangible benefits.
To dispel such doubts, we need to keep in mind that derivational terms refrain from specifying \textit{lexical meaning}, i.e. they treat lexical items as black boxes, from a semantic perspective.
Opening these black boxes would reveal flat entries (i.e. term constants) in the case of words providing meaning \textit{ingredients}, as opposed to structurally rich entries (i.e. complex terms with internal structure) in the case of words providing meaning \textit{recipes}.%
\footnote{This distinction is usually paralleled with the linguistic distinction between \textit{content} and \textit{function} words, but commiting to this being the case is an unecessary restriction. Depending on the end-target semantics logic and the granularity of the semantic lexicon, content words might still be assigned complex term structure -- a common trick, for instance, in delivering dependent type semantics; see the book of \citet{chatzikyriakidis2020formal} for an overview of recent developments.}
Structurally rich lexical entries can utilize \textit{any} term constructor made available by the semantic logic; crucially, this includes constructors that escape the narrow borders of the homomorphic codomain (i.e. do not have a syntactic origin).
Of course, such terms are still bound by the promise to obay the type dictated by the homomorphic translation of their original syntactic type, and must also be derivable theorems of the semantic logic they live in.
Increasing expressivity therefore may indeed not in itself add to the function/argument structures inherited by syntax, but provides the tools necessary for complex lexical semantic actions to take effect as needed.

A case in point is the coordinator \textex{and} conjoining the two modifiers of the previous section: \textex{litten by \dots \textbf{and} having in \dots}.
Each individual conjunct fulfills a descriptive filter that intersects the properties of its argument with the properties attributed by its internal meaning.
That is, of all objects of type $*$ (where $*$ an arbitrary type, denoting the interpretation target of $\smallprop{np}$), the first modifier withdraws all but those lit by unseeable suns, whereas the second one withdraws all but those with weird entities in their whirlpools.
For the full conjunction to have the intended meaning, i.e. evoke the image of exclusively this subset of oceans characterized by both the above properties, the coordinator would need to enact the role of a portable implementation of function composition%
	\footnote{Before anyone gets angry: I am neither pitching some provocative theory of conjunction semantics here,  nor secretly advocating for the dot-combinator -- just trying to make a point.} 
as in Figure~\ref{figure:funcomp}, so as to allow the iteration of the intersective modifiers:
\begin{equation}\label{equation:funcomp}
\lam \smallterm{\vari\varj\vark.\vari~(\varj~\vark)} :: (*\li *)\li(*\li *)\li * \li *
\end{equation}
Even though no non-standard term constructors are to be found in this recipe, it is nontheless \textit{not} a theorem of the source logic, as function composition is not derivable in $\NL$.
In a set-theoretic semantics domain unbound by linearity constraints, another (perhaps more reasonable) translation might make use of an added operator $\wedge : * \to * \to *$ for set-theoretic intersection ($*$ now an arbitrary set), to deliver the recipe:
\begin{equation}
\lam \smallterm{\vari\varj\vark.(\vari~\vark)\wedge(\varj~\vark)} :: (*\ii *)\ii(*\ii *)\ii * \ii *
\end{equation}

\subsection{Abstract Categorial Grammars}
\label{subsection:ACG}
So far, we have been predisposed to treating syntax as the hidden process that forms grammatically correct sentences.
It is insightful to contrast this treatment with the view of \citet{curry1961some}, who thought of syntax as a two-layered hierarchy of grammaticality criteria.
The deep layer, called \textit{tectogrammar}, would be concerned solely with the well-typedness of grammatical function domains and the validity of their interpretations.
The shallow layer, called \textit{phenogrammar}, would be where tectogrammatical proofs are transformed and cast to surface forms that abide by the linear order and constituency restrictions imposed by the language.
Type-logical grammars pose no challenge to the legitimacy of this distinction: it should be clear that phenogrammar, in Curry's terms, is our syntactic logic, and tectogrammar is what we earlier referred to as derivational semantics.
In being tectogrammar-first, however, they diverge in its operationalization.
The computational pipeline they propose is sequential in nature, and follows the Aristotelian path from observable evidence to latent variables: the surface string is perceived as the yield of a (shallow) syntactic proof, from which a deep semantic proof is extracted.
The operationalization closer to Curry would be inverted, placing phenogrammar at the top of the generative process, and following the Platonic information flow from deep and abstract to shallow and concrete.
This perspective is embodied by abstract categorial grammars~\cite{de2001towards} and their contemporary and closely related lambda grammars~\cite{muskens2001lambda}.
Both are tectogrammar-first formalisms that make use of $\ILL_{\li}$ and the Curry-Howard isomoprhism to obtain phenogrammatic realizations via homomorphic translations of the tectogrammatic parse.

\begin{figure}
	\centering
	\begin{tikzpicture}
		\node (ds)		at (0, 2)		{Phenogrammar};
		\node (dsl)		at (0, 1.5)		{$\Sigma := \ILL_{\li}$};
		\node (ss)		at (-4.5, 0)		{Tectogrammar};
		\node (ssl)		at (-4.5, -0.5)	{$\Tau := \ILL_{\li}$};
		\node (ls)		at (4.5, 0)		{Semantics};
		\node (lsl)		at (4.5, -0.5)	{?};
		\draw[->] (dsl) -- node[above] {$\hm{.}$}  (ss) ;
		\draw[->,dotted,thick] (dsl) --	(ls) ;
	\end{tikzpicture}
	\caption{The syntax-semantics interface in the abstract categorial setting.}
	\label{figure:synsemacg}
\end{figure}

\subsubsection{Basic Definitions}
The focus of our presentation will be on abstract categorial grammars, as they are closer in spirit to what is to come later.
In its original definition, an abstract categoral grammar consists of two instantiations $\Sigma$, $\Tau$ of $\ILL_{\li}$, and a map between them.
The source instantiation $\Sigma$ provides a set of base types $\propcon^{\Sigma}$, and the so-called \textit{abstract vocabulary}: a set of \textit{abstract constants} $\cons^{\Sigma}$, each assigned a type from $\types^{\Sigma}$.
The target instantation $\Tau$ provides another set of base types $\propcon^{\Tau}$, and constants $\cons^{\Tau}$ with types from $\types^{\Tau}$, called the \textit{object vocabulary}.
The map between them is once again a homomomorphism $\hm{.}$, defined on the basis of $\langle \eta_0, \theta_0\rangle$.
Not unlike before, $\eta_0$ is seen as implementing a mapping $\propcon^{\Sigma} \to \types^{\Tau}$, and $\theta_0$ a mapping $\cons^{\Sigma}\to \terms^{\Tau}$, both pointwise defined.
Their homomorphic extension is trivially obtained by recursively defining their actions on implicational types, function terms and $\lam$ abstractions, where they simply mimic the source type- and term- structure.
This formulation lends itself nicely to the notion of \textit{grammar composition}, if one is to use the object logic of a grammar as the abstract logic of another.
Each grammar is accompanied by two \textit{languages}; the abstract language, i.e. the set of terms (of some \textit{distinguished} type $p_d\in \propcon^{\Sigma}$) derivable in the source logic, and the object language, i.e. the set of object terms the abstract language maps into.
For the phenogrammar to tectogrammar picture to be made evident, the distinguished type needs to be mapped to the functional string type $p_{d} \mapsto \mathtt{str}$, forcing terms of the object language to evaluate to strings.
Note that, despite appearances, $\mathtt{str}$ is a first-order type $*\li *$ (where $*$ some arbitrary primitive) so as to permit the view of string concatenation as function composition, identical to~(\ref{equation:funcomp}):
\begin{equation}
+ := \lam \term{\vari^{\mathtt{str}}\varj^{\mathtt{str}}\vark^{*}.\vari~(\varj~\vark)}
\end{equation}


\subsubsection{Artificial Languages}
Abstract categorial grammars are characterized by two measures of complexity: the maximal order of source constants' types, and the maximal order of the codomain of $\eta_0$.
The two together constitute the grammar's \textit{class}, which concisely describes the sort of languages the grammar can model.
This can prove effective in revealing a more granular stratification underlying the Chomsky hierarchy of formal grammars, when the latter are embedded into abstract categorial equivalents; as such, the framework has found extensive use as a meta-language for the study and formalization of formal grammars (as done by \citet{de2004expressive}, \textit{inter alia}).
\footnote{There is a certain irony in formal grammars requiring or benefiting from formalization. If you're having trouble parsing this, consider that formal languages are essentially ad-hoc rules on strings; by formalization we mean giving these rules the type-theoretic treatment they deserve.}

To see this in practice, let's have some meta-fun pretty-printing the types of $\logic{(N)L}_{\diamond, \bx}$ by modeling their type formation rules (which constitute a context-free grammar) using an abstract categorial grammar.
First item on the agenda is the specification of our two logics $\Sigma$ and $\Tau$.
The source logic $\Sigma$ will provide the abstract backbone of the type grammar, containing a single base type, that of a well-formed ``type'' 
$\propcon^{\Sigma} := \{ \type[s] \}$.
The abstract vocabulary is then populated in Figure~\ref{figure:acg_nl_abstract_lex} by all abstract constants denoting base ``types''%
\footnote{In hindsight, that might have been an unfortunate choice of term\textsuperscript{\thefootnote} to overload.} and ``type'' constructors.
The target logic $\Tau$ will be our phenogrammatic printer tasked with translating abstract terms (``types'') to object terms (strings).
We will need a single object base type  $\propcon^{\Tau} := \{ * \}$, such that $\eta_0(\type[s]) = \mathtt{str}$, the type alias of $*\li *$.
Some auxiliary object constants are necessary before we proceed: opening and closing brackets, a diamond and a box the two implications, and a unique match for each unique \textit{constant} abstract constant (i.e. each abstract constant whose type is of order zero).
The above -- all of type $\mathtt{str}$, and underlined to distinguish from functional symbols -- are used by the abstract constant translation $\theta_0$ defined in Table~\ref{table:nl_tg_acg}: base constructors are mapped to their corresponding string representations, the two unary modalities simply concatenate their symbol to their single argument, whereas the two implications infix their arguments with the a slash or backslash, and wrap the result under brackets.

\begin{figure}
	\begin{align*}
	\cons^{\Sigma} := \{ 
		& 	\smallterm{n} :: \type[s],\ \smallterm{np} :: \type[s],\ \smallterm{pp} :: \type[s],\ \smallterm{np} :: \type[s],\\
		&	\smallterm{dia} :: \type[s]\li\type[s],\ \smallterm{box} :: \type[s]\li\type[s]\\
		&	\smallterm{ldiv} :: \type[s]\li\type[s]\li\type[s] \\
		&	\smallterm{rdiv} :: \type[s]\li\type[s]\li\type[s] \}
	\end{align*}
	\caption{Abstract lexicon for the language of $\logic{(N)L}_{\diamond, \bx}$ types.}
	\label{figure:acg_nl_abstract_lex}
\end{figure}

\begin{table}
	\centering
	\begin{tabularx}{0.75\textwidth}{@{}cc@{}}
		Abstract Constant							& Object Term\\
		\toprule
		$\smallterm{n}$								& $\underline{\smallprop{n}}$\\
		$\smallterm{np}$							& $\underline{\smallprop{np}}$\\
		$\smallterm{pp}$							& $\underline{\smallprop{pp}}$\\
		$\smallterm{s}$								& $\underline{\smallprop{s}}$\\
		$\smallterm{dia}$							& $\smallterm{\lam  \vari.\underline{\diamond} + \varj}$\\		
		$\smallterm{box}$							& $\smallterm{\lam  \vari.\underline{\bx} + \vark}$\\		
		$\smallterm{ldiv}$							& $\smallterm{\lam  \vari\varj.\underline{(} + \vari + \underline{\divleft} + \varj + \underline{)}}$\\
		$\smallterm{rdiv}$							& $\smallterm{\lam  \vari\varj.\underline{(} + \varj + \underline{\divright} + \vari + \underline{)}}$
	\end{tabularx}
	\caption{Object translation for the lexicon of Figure~\ref{figure:acg_nl_abstract_lex}.}
	\label{table:nl_tg_acg}
\end{table}

\begin{figure}
	{\smaller
	\[
		\infer[\li E]{\term{rdiv~(dia~(box~(ldiv~np~np)))~pp}:\type}{
			\infer{\type\li\type\li\type}{\term{rdiv}}
			&
			\infer[\li E]{\term{dia~(box~(ldiv~np~np))}: \type}{
				\infer{\type\li\type}{\term{dia}}
				&
				\infer[\li E]{\term{box~(ldiv~np~np)}: \type}{
					\infer{\type\li \type}{\term{box}}
					&
					\infer[\li E]{\term{ldiv~np~np}: \type}{
						\infer{\type \li \type \li \type}{\term{ldiv}}
						&
						\infer{\type}{\term{np}}
						&
						\infer{\type}{\term{np}}
					}					
				}
			}
			&
			\hspace{-45pt}
			\infer{\type}{\term{pp}}
		}
	\]
	}
	\caption{Constructing the type assignment of~(\ref{equation:litten}).}
	\label{figure:litten_acg_der}
\end{figure}

Figure~\ref{figure:litten_acg_der} presents the construction of the type previously assigned to \textex{litten}, $\diamond\bx (\np[s]\divleft\np[s])\divright\pp[s]$ (contexts are intentionally left empty and axioms replaced by abstract constants for brevity).
Applying the homomorphic translation to its abstract yields a printout in the form of the object term below (source function/argument brackets substituted with indendation levels for legibility):
\begin{equation}
	{
	\begin{aligned}
	& 			\hm{\term{rdiv~(dia~box~(ldiv~np~np)))~pp}}\\
	= \
	&				\smallterm{\lam  \vari\varj.\underline{(} + \varj + \underline{\divright} + \vari + \underline{)}}\\
	& \quad			\underline{\smallprop{pp}}\\
	& \quad 		\smallterm{\lam  \vark.\underline{\diamond} + \vark}\\
	& \qquad 		\smallterm{\lam  \varl.\underline{\bx} + \varl}\\
	& \qqquad		\smallterm{\lam  \Var{m}\Var{n}.\underline{(} + \Var{m} + \underline{\divleft} + \Var{n} + \underline{)}} \\
	& \qqqquad		\underline{\smallprop{np}}\\
	& \qqqquad		\underline{\smallprop{np}}\\
	\bredstar \
	&				\underline{(
						\diamond \bx ( \np \divleft \np )
						\divright \pp[s] 
						)}
	\end{aligned}
	}
\end{equation}
Six reduction steps later and... \textit{voil\`{a}} -- our pretty printer works!
The maximal order of the abstract constants is 1, and the maximal order of the translation is 2, making our grammar's complexity class (1, 2), a proper subset of the (2,2) that encapsulates context-free grammars.

\subsubsection{Human Languages}
Elegant and successful as they might be in their meta-theoretical enterprises, abstract categorial grammars have not fared as well with linguistic applications, in large part due to their computationally intractable nature.
On the one hand, they stand out from the rest of the categorial family in not being lexicalized by default.
The conceptual separation between lexicon and rules no longer holds: rules are fixed to the ones supplied by $\ILL_{\li}$, but inference is largely guided by the abstract constants.
Abstract constants may contain lexical items that make their way to the final (object) derivation, or simply compositional recipes that leave no imprint whatsoever.
At the same time, the framework is overly reliant on the constant map $\theta_0$ (defined on a per-item basis) for the translation into the object language to take effect.
Even in the lexicalized setup where the abstract lexicon is populated by words and words only, every abstract constant needs to be assigned both an abstract type and a unique object term for every phenogrammatic behavior it exhibits; two lexical dimensions, compared to the one of vanilla categorial grammars.
Enforcing grammaticality while blocking overgeneration of the object language similarly requires a careful, parallel finetuning of both the abstract language and the translation --  gone is the adage of words carrying their combinatorics on their sleeves. 
What's worse, words triggering higher-order tectogrammatic phenomena will then need object translations of an even higher order for their surface forms, making the design and population of a strict tectogrammatic translation $\hm{.}$ practically unfeasible.
This part could in principle be partially mitigated by flattening complex syntactic phenomena into lower-order (alias) types in the source domain, and outsourcing their expansion to a parallel grammar for concrete semantics -- this is less of a solution and more of a deferral, though.
Beyond issues of practicality, there are also foundational problems at stake, as resorting to a lexical enumeration of phenogrammatic forms evidences inability to perform linguistic generalization -- what \citet{moot2014hybrid} calls a problem of \textit{descriptive inadequacy} revolving around \textit{any} abstract replacement to a Lambek higher-order type.
Last but not least, it is hard to imagine an abstract categrial grammar in action: it is unclear how to procure an abstract proof object from the \textit{evaluated} yield of its object translation (i.e. the string form we are most likely encounter in the open) using traditional proof-theoretic disciplines -- the two layers of function/argument structures (abstract- and object- level) and their interacting reductions would unnerve even the sturdiest of parsers (or so it seems).

As an artificial yet illustrative and down-to-earth example, let's brave the design of an abstract categorial grammar tasked with the production of an end-to-end linguistic example.
Once more, we start with the specification of our two logics $\Sigma$ and $\Tau$.
For efficacy and simplicity, we can have $\Sigma$ coincide with the derivational semantics logic of Section~\ref{subsubsection:ssi_tlg}, inheriting its terms and types for free.
Doing so requires of course that we assume some high-level equivalence between the representations of what used to be abstract semantics before, and what now we call deep syntax -- let's naively take this for granted.
The object logic $\Tau$, being responsible for the surface materialization of our derivational proofs, will again need to be a logic of strings and is thus populated by a single atomic type $\propcon^{\Tau} = \{ *\}$; all abstract atoms atoms are then sent to $\mathtt{str}$.
For each word, we will need an abstract constant $\w{c} \in \cons^{\Sigma}$, and a corresponding object constant $\str{c} :: \mathtt{str} \in \cons^{\Tau}$, denoting the word's string form. 
Abstract constants will be sent to object terms via $\theta_0$, each of which must contain a single occurence of a term constant, in order to preserve lexicalism and respect lexical transparency.
Worth a special mention is the fact that the tectogrammatic image of words assigns them syntactic recipes, as they carry their own $\lam$ terms.
The story is summarized in Table~\ref{table:toy_acg_lexicon} (object types are ommitted for brevity -- simply substitute all atoms of the corresponding abstract types with $\mathtt{str}$ to obtain them).

\begin{table}[h]
	\centering
	\begin{tabularx}{0.85\textwidth}{@{}ccC@{}}
		Abstract Constant							& Abstract Type										& Object Term\\
		\toprule
		eye											& $\smalln$											& \str{eye}	\\
		suns										& $\smallnp$										& \str{suns} \\
		oceans										& $\smallnp$										& \str{ocreans} \\
		the											& $\smalln\li\smallnp$								& $\smallterm{\lam \vari.\str{the}+\vari}$\\
		opiate										& $\smallnp\li\smallnp$								& $\smallterm{\lam \vari.\str{opiate}+\vari}$\\
		poured										& $\smallnp\li\smalls$								& $\smallterm{\lam \vari.\vari + \str{poured}}$\\
		behold										& $\smallnp\li\smallnp\li\smalls$					& $\smallterm{\lam \vari\varj.\varj+\str{behold}+\vari }$\\
		there										& $(\smallnp\li\smalls)\li\smallnp\li\smalls$		& $\smallterm{\lam \vari \varj.(\vari~\varj) + \str{there}}$\\
		never										& $(\smallnp\li\smalls)\li\smallnp\li\smalls$		& $\smallterm{\lam \vari \varj.\str{never}+(\vari ~\varj)}$\\
		may											& $(\smallnp\li\smalls)\li\smallnp\li\smalls$		& $\smallterm{\lam \vari \varj.\str{may}+(\vari ~\varj)}$\\
		by											& $\np\li\pp$										& $\smallterm{\lam \vari.\str{by}+ \vari}$\\
		litten 										& $\smallpp\li\smallnp\li\smallnp$					& $\smallterm{\lam \vari\varj.\varj + \str{litten}+ \vari}$\\
		that										& $(\smallnp\li\smalls)\li\smallnp\li\smallnp$		& $\smallterm{\lam \vari\varj. \varj + \str{that} + \vari(\str{\gap})}$
	\end{tabularx}
	\caption{Abstract lovecraftian lexicon abiding to the types of Table~\ref{table:toy_lambek_lexicon}.}
	\label{table:toy_acg_lexicon}
\end{table}


The equation below shows the computation of the homomorphic translation to the derivational term (\ref{equation:litten_derterm}) inspected earlier.

{\smaller
\begin{equation}\label{equation:acg_pheno}
	\begin{aligned}
	& 			\hm{\term{litten~(by~(that~(\lam \vari .(may~(never~(behold~\vari )))~(the~eye)))~(suns))}} \\
	= \ 
	&							(\lam \vari\varj.\varj + \str{litten}+ \vari) \\
	&\quad								(\lam \vark.\str{by}+ \vark)\\
	&\qquad									(\lam \varl\varm. \varm + \str{that} + \varl(\str{\gap}))\\
	&\qqquad								(\lam x_n. \\
	&\qqquad\quad										(\lam x_o x_p.\str{may}+(x_o ~x_p))\\
	&\qqquad\qquad											(\lam x_q x_r.\str{never}+(x_q~x_r))\\
	&\qqquad\qqquad												(\lam x_sx_t.x_t+\str{behold}+x_s)\\
	&\qqquad\qqqquad												x_n \\
	&\qqquad												((\lam x_u.\str{the}+x_u)~\str{eye})\\
	&\quad									\str{suns}\\
	\bredstar \ & \term{\lam \vari.\vari + \str{litten}\str{by}\str{suns}\str{that}\str{the}\str{eye}\str{may}\str{never}\str{behold}}\str{\gap}
	\end{aligned}
\end{equation}
}%
This already proves quite an endeavour; twelve reduction steps later, we are presented with a seemingly reasonable output.
Upon closer inspection, though, an issue pops up: the end term is of type $\mathtt{str}\li\mathtt{str}$, making it an ill-fit for the discontinuous application to the \textit{substring} \str{opiate}\str{oceans}, which we expect to find nested within the main clause \str{opiate}\str{oceans}\str{poured}\str{there}.
For the tectogrammatical form to be able to surface correctly, some drastic adjustments are necessary.
We could alter the derivational proof by including non-lexical abstract constants, or refine the source types to impose (or enable) structural variation, but either option would be undermining the cohesion between deep syntax and semantics we started from.
The only alternative that does not resort to contaminating the original term -- abstract and pure -- with vulgar restrictions of form would be to lift the complexity of the interpretation and design it anew.

It seems therefore that the appealing simplicity and elegance of the tectogrammatic logic is counterbalanced by an increasingy bulky and cumbersome transition to the (equally simple, yet far less elegant) phenogrammatic logic.
The problem is of course more pronounced for natural languages, which overstep the strict confines of their formal counterparts~\cite{moot2014hybrid}.
If only we had a way to keep just the good part of a type-driven and semantically transparent deep syntax, without having to get involved with all the tedious labour of its surface materialization or the translation to it...
Spoiler alert: we will in a bit.

\subsection{Other Formalisms}
Type-logical and abstract categorial grammars have monopolized our interest, yet are not the only members of the categorial grammar family.
For the sake of completeness and impartiality, we will briefly discuss two other major flavours and contrast them to the ones so far presented.

\subsubsection{Combinatory Categorial Grammars}
A deviant from the categorial tradition are the broadly adopted combinatory categorial grammars~\cite{ades1982order,szabolcsi1989bound, steedman2022combinatory}.
These stray from the norm by rejecting the very idea of the syntactic variable (and with it, hypothetical reasoning), citing reasons of cognitive plausibility and parsing complexity.
Obviously, a categorial grammar stripped of hypothetical reasoning would not amount to much on its own: it would only be able to resolve syntactically flat sentences.
To regain some of the lost expressivity (ideally, exactly and only as much as needed), combinatory categorial grammars incorporate a collection of rules lent from the combinatory logic of~\citet{curry1958combinatory}, albeit in restricted form.
The first such rule is morpholexical in nature; it allows lexical items to raise their types once, before administering them to the syntactic derivation, forcing a flip in the local function/argument structure and allowing different semantic scopes to take effect as/when needed.
The remaining rules are essentially four instances of function composition -- one for each unique pair of directional implications considered.
The absence of hypothetical reasoning means that these are no longer derivable theorems of some underlying type theory, but ad-hoc schemata, fixed \textit{a priori} to fit their designated purpose.
To counteract overgeneration, these rules are made available only to a pre-defined subset of the sum of lexical types, empirically specified.
With respect to the interface, a combinatory derivaton can be cast into a semantic $\lam$ term the usual way; by assigning to each rule a corresponding term constructor.
Note, however, that this procedure is a non-invertible \textit{transformation} rather than an isomorphic correspondence; the purity of the Curry-Howard correspondence is lost, traded away for the aforementioned decrease in parsing complexity.

In spite of their (non-minor) differences, the agendas of multimodal type-logical grammars and combinatory categorial grammars are quite aligned, at least at a high level: they both stipulate the presence of syntactic universals that guide structure formation, utilize them as a pathway to semantics \textit{\`{a} la} Montague, and acknowledge the need for language-specific syntactic fine-tuning; one exercising proof-theoretic control via unary type operators and structural rules, the other controlling the applicability of the so-called combinatory rules via lexical adjustment.
For better or worse, combinatory categorial grammars have taken the lion's share of the practitioners' focus: they boast an assortment of tools and annotated corpora across languages, the size of which far exceeds that of their less popular siblings -- to the point where the term categorial grammars has become an almost synonym of combinatory categorial grammars.%
\footnote{At least if one is to consider the reviewers I get assigned a reliable statistical sample of the NLP population.}
I hope that, by its end, this thesis will have slightly adjusted the scales towards a healthier epistemological pluralism.

\subsubsection{Hybrid Type-Logical Grammars}
In the lands between type-logical grammars and their abstract siblings, there lives the strangeness of hybrid type-logical grammars.
Originally proposed by~\citet{kubota2012gapping}, hybrid type-logical grammars utilize a combination of the two slashes of traditional Lambek calculi with the non-directional linear implication, to give birth to a type grammar that combines the good aspects of both approaches while purportedly suffering the restrictions of neither.
The types of a hybrid type-logical grammar are the result of two stages of induction.
The first stage creates standard Lambek types, as in~(\ref{equation:LC}).
The second stage is of the form 
\begin{equation}
	\prop{a}, \prop{b}, \prop{c} := \prop{l} \ | \ \prop{a} \li \prop{b}
\end{equation}
where $\smallprop{l}$ a valid Lambek type.
The term calculus of hybrid type-logical grammars requires a translation of logical types into so-called \textit{prosodic} types $\smallprop{st}$ (for structure or string), such that all stage 1 (Lambek) types are sent to $\smallprop{st}$, and stage 2 types are inductively translated as (higher-order) functions over $\smallprop{st}$. 
The term constructors assigned to the elimination (resp. introduction) of the Lambek connectives is that of structure concatenation (resp. separation), whereas the term constructors assigned to the linear connective are standard function application and variable abstraction.
The marriage of these two layers of abstraction in the same term calculus might seem unorthodox or at least aesthetically displeasing, but is not without merit.
Concetenative terms allow the framework to relax the lexical pressure of an abstract categorial grammar by preserving the canonical categorial grammar treatment of local syntactic phenomena.
Applicative terms constitute localized and controlled bursts of ad-hoc expressivity that can exceptionally allow the derivation of higher-order and non-local phenomena normally inacessible to the OG Lambek calculi.
Hybrid type-logical grammars are a relatively new addition to the family, and are subject to ongoing research, both from the linguistic and the proof-theoretic perspective~\cite{kubota2020type,moot2022logical}.
As to why one would choose this setup over alternatives, your guess is as good as mine; hybrid proponents proclaim the system less obscure and more fit for linguistic applications~\cite{kubota2020type} -- external validation is still pending.


\section{Key References \& Further Reading}
Key references for this chapter were the Stanford Encyclopedia of Phisolophy entry on type-logical grammars~\cite{sep-typelogical-grammar} and the tried-and-true extended introduction books on $\lambda$-calculi and type theories of \citet{sorensen2006lectures} and \citet{pierce2004advanced}.
Moral credit is owed to my once faithful travel companion, the categorial grammar bible of \citet{moot2012logic}; it provides an accessible yet detailed documentation of most of the concepts hinted at in this chapter.
Sections~\ref{section:simple_type_theory} and~\ref{section:linear_type_theory} draw heavily, both in content and in style, from the excellent tutorial paper of \citet{wadler1993taste} on linear type theory -- waning presentational influences might be discernible up to Section~\ref{section:modalities}.

If unhappy about this chapter ending, or unsatisfied with the exposition provided, here's some extra reading material to keep you company.
For a detailed inquiry on proof nets and their linguistic applications, or an exemplar of what an actual great dissertation looks like, take a look at my co-supervisor's one~\cite{moot2002proof}.
For a more mathematically eloquent presentation of modalities and their potential as tools of inferential and structural reasoning, refer to the (also superb) dissertation of~\citet{bernardi2002reasoning}.
For a slightly outdated but still very educative overview of abstract categorial grammars, the lecture notes of~\citet{kanazawa2009advances} should prove handy.
If your eco-conscious side was moved by linear logic, but you find yourself lacking the bravery of facing the original manuscript of \citet{girard1987linear}, the lecture notes of \citet{troelstra1991lectures} would make for a good alternative.
If on the other hand you were intrigued about the vast expanse of type theories beyond the tiny scope of this thesis, the entry point to the downwards descent into the rabbit hole should be the seminal work of~\citet{martin1982constructive}.
A convincingly easy-to-swallow application of such type theories in the formal semantics world is extensively summarized by \citet{chatzikyriakidis2020formal}.
If you do like formal semantics but big lambdas give you nausea, there's a broad selection of books to go for; I still find myself guiltily cross-checking definitions and examples with that of \citet{winter2016elements} at times.
Finally, if what caught your attention was the historical drama at the beginning of the chapter, you will enjoy reading about the history of constructivism by \citet{troelstra2011history}.


\bibliographystyle{abbrvnat}
\bibliography{bibliography}

{
\renewcommand{\thechapter}{\arabic{chapter}}
\setcounter{chapter}{-1}
\renewcommand{\bibname}{Papers this dissertation is based on}
\chapter{Preface}
\label{chapter:preface}

\hide[Greetings, reader.
This preface serves only to provide a summary of this thesis through a chapter breakdown and an enumeration of its contributions.
Find both below.]{
	Greetings, reader.
	Out of coincidence or some weird turn of events, I have written this dissertation to-be and you have stumbled upon it.
	Introductions would normally be in order, but this communication channel is asynchronous and unidirectional so I will be doing double duty for the both of us.
	
	So let's start with you.
	A range of scenarios are plausible as to how you came to be reading these letters.
	The safest bet (and easiest way to score some easy points on precision) is to assume this is some obligation of sorts, in which case you have my sympathy and gratitude.
	Also likely, you might be acquaintance of mine -- academic, social, or both -- driven by the curiosity to figure out what it is I spent 5 (and counting) years in Utrecht for; well, pretty much this.
	Less likely, you could just lazilly be scrolling through the opening pages, contemplating whether I'd be a good fit for some organization you are representing; if so, you should definitely go for me.%
	\footnote{Exceptionally, if this happens to be some big corp reptile den, scram -- and shame on you, future me.}
	Least likely,  could it even be that you are \textit{actually} interested in the subject matter of this thesis? 
	Exciting, but also slightly alarming; I feel somewhat conscious knowing that you'll be putting my words under a critical lens – I’ll do my best not to fail your expectations. 
	In the wildcard scenario where you do not fall in any of the above categories, excuse my lack of foresight and know that you are still very welcome, and I am happy to have you around.
	More realistically, if noone ever reads this (far), let this transmission be forever lost to the void.
	
	But enough with you, what about me? 
	At the time of writing, I am in my early thirties and call myself Kokos. 
	I had the enormous luck of crossing paths with my supervisor, Michael, during my first weeks of graduate studies in Utrecht. 
	The repercussions of this encounter were (and still are) unforeseeable. 
	Coming from an applied background with an innate repulsion to anything formal, his course offered me a glimpse of a whole new world. 
	I got to see that proofs are not irrelevant bureaucracies to avoid, but objects of interest in themselves, hidden in plain sight from the working hacker under the most common programming patterns. 
	If this naive revelation came as shock, you can imagine my almost mystical awe when I was shown how proof \& type theories also offer suitable tools and vocabulary for the analysis of human languages. 
	Despite my prior ignorance, the “holy trinity” between constructive logics, programming languages and natural languages has been (with its ups and downs) at the forefronts of theoretical research for well over a century. 
	This dissertation aims to be the tiniest of contribution to this line of work, conducted from the angle of a late convert, a theory-conscious hacker. 
	
	If all this sounds enticing and you plan on sticking around, at least for a bit longer, I think it would be beneficial if we set down the terms and conditions of what is to follow. 
	It is no secret that dissertations are often boring to read, and it can be easy to lose track of context in seemingly unending walls of text. 
	Striking a balance between being pedantic and making too many assumptions on background knowledge is no easy task: the only way to spare you unecessary headaches requires a mutual contract. 
	On my part, I will try to clearly communicate my intentions, both about the thesis in full, and its parts in isolation: the idea is to make this manuscript as self-contained as possible, but without nitpicking on details or taking detours unnecessary for the presentation of the few novelties I have to contribute.
	Of you, I ask to remain conscious of what you are reading and aware of my own biases and limitations. 
	The absence of feedback means that my mental model of you is a purely artificial construct of my imagination.
	I will inadvertently skip things that to me seem self-evident, and rant at length about others that you take for granted.
	So feel free to skip ahead when something reads trivial, and do not judge too harshly when you encounter an explanation you find insufficient.
	
	\paragraph{What is this about}
	The quote below was received almost verbatim as a review.
	Mean spirited as it may be, it provides an adequate high-level summary of this thesis' contents: 
	\begin{quote}
	[The paper] starts with Lambek Calculus, some how uses dependency labels in some of its semantic types, provides a parsing algorithm for it; there are neural networks and vectors used and some accuracy results provided, but I am still unsure about the contributions [of the paper] and their relevance.
	\begin{flushright} Unknown reviewer, 2019.\end{flushright}
	\end{quote}\goodbreak
	
	\noindent Thanks to this fellow scientist's earnest reviewing work, all I need to do here is first align the above summary with the manuscript through a chapter breakdown, and then explicate the thesis' contributions in a childproof way.
}

\paragraph{\hide[What this thesis is about]{Chapter Breakdown}}
\begin{enumerate}[labelindent=2pt, itemindent=30pt, labelsep=5pt, widest=Chapter III,align=right,itemsep=5pt]
\item[\textbf{Chapter~\ref{chapter:preface}}] \hide[is what you are currently reading]{greets the reader and provides a chapter summary, while also setting the tonal precedents for what is to follow. You are currently in it}.
\item[\textbf{Chapter~\ref{chapter:Introduction}}] is an attempt at a painless introduction to simple type theory and its substructural variants, \hide[and an allusion to]{with an emphasis on} their significance for linguistics.
We set things off in Section~\ref{section:simple_type_theory} with a crash course on simple type theory, a formal model of computation and logical deduction.
Removing the ability to erase or duplicate logical propositions, we transition to linear type theory in Section~\ref{section:linear_type_theory} -- a place where the motto is resource consciousness and truth is not for free.
Following along the same path, in Section~\ref{section:lambek_calculi} we take the extra step of making propositions immovable and bracket-bound to their surroundings, revealing the landscape of Lambek calculi $\logic{(N)L(P)}$.
To regain some of the expressivity lost in the passage, we call modalities to the aid in Section~\ref{section:modalities} -- these allow us to reinstate the implicit equivalences of before as explicit rules with limited and controllable applicability.
In Section~\ref{section:linguistics}, the theoretical wisdoms amassed through our substructural expedition find use in defining categorial grammars: type-driven frameworks formalizing the syntax and semantics of natural languages.
We discuss two relevant and related paradigms: multi-modal type-logical grammars and abstract categorial grammars; both use constraints imported by types to control grammatical composition, converting parsing to a process of logical deduction.
At long last, we have all the foundational knowledge needed to move on.
\item[\textbf{Chapter~\ref{chapter:chapter_2}}] offers a non-standard usecase for the structural control modalities of the multimodal Lambek family $\logic{(N)L(P)}_{\diamond, \bx}$.
We begin in Section~\ref{section:phrase_vs_dependecy} with a face-off between the two strands of grammar flavors that have dominated computational linguistics in the past decades -- namely constituency  and dependency grammars -- and see how they compare to categorial grammars.
Unsatisfied by the comparison, we move on to Section~\ref{section:modalities_for_dependency}, where we appropriate the modalities we resorted to earlier, repurposing them now as dependency domain demarcators.
\item[\textbf{Chapter~\ref{chapter:chapter_3}}] instantiates a type system based on this new envisaging of modalities, and employs it as a derivational semantics logic%
	\footnote{Or abstract syntax logic, depending on which side of the dividing line you stand at.}%
, in alignment with real-world corpus data. 
Section~\ref{section:preliminaries} sets the stage with a backstory motivating the design choices made and describing the source corpus.
We proceed to the real thing in Section~\ref{section:aethel}, where we illustrate how the corpus' analyses can be recast as proof-theoretic inhabitants in our framework.
We detail the extraction process, the resulting view of the corpus, and its practical evaluation as a stand-alone resource.
\item[\textbf{Chapter~\ref{chapter:chapter_4}}] makes for a drastic change of scenery, offering a collection of insights on the neural parsing of substructural grammar logics.
We first paint a picture of the archetypical categorial grammar parser in Section~\ref{section:parse}, pinpointing the tension points between abstract theory and applied practice and exposing the need for a disciplined merger between the formal and the informal.
In Section~\ref{section:supertagging} we trace along the history of supertagging: the statistical disambiguation process (and the machinery empowering it) through which a system can automatically infer the most likely type for a lexical item in context (i.e. a word in a sentence).
We start from its (not-so-ancient) origins and go all the way to today, motivating the abolition of a set-in-stone type vocabulary as a natural step in its evolutionary progress and providing two convincing implementations to that end.
For the grand finale, in Section~\ref{section:npn} we propose a neural operationalization for the proof nets of linear logic.
Invoking their bureaucracy-free format, we uncover a brand new paradigm for the statistical parsing of substructural grammar logics in the \logic{(N)L(P)} lineage.
\item[\textbf{Chapter~\ref{chapter:chapter_5}}] wraps things up and waves \hide[you]{the reader} goodbye.
\end{enumerate}

%\newpage
\paragraph{Contributions}
\hide{Contributions produced and presented in this thesis, organized in bullet points for your convenience and reading pleasure, include:}
\begin{itemize}
\item a \textbf{type-driven} model of compositional syntax that simultaneously captures \textbf{dependency-} and \textbf{function-argument- structures}; not a world first, but a close second by 30 years
\item a big, open-source, \textbf{well-typed dataset} of proof-derivations for written Dutch
\item the \textbf{first supertagger} to correctly \textbf{construct novel type assignments}, operating without a fixed lexicon
\item the current \textbf{state of the art} supertagger that outperforms accuracy benchmarks \textbf{across grammar frameworks}, without foregoing the ability to predict rare and unseen assignments -- essentially an assurance that sparse and elaborated categorial grammars are of practical use, despite prior disdains
\item a \textbf{neural operationalization} of linear logic's \textbf{proof nets} into a massively parallel, differentiable and hyper-performant proof search engine -- essentially a reconciliation between the modern neural toolbox and the Lambekian tradition, and a call back to typing discipline for categorial practitioners
\end{itemize}

\paragraph{How to README}
The thesis is best read in the order presented; each hapter is (weakly) dependent on its predecessors.
That said, you can skip Chapter~\ref{chapter:Introduction} if already familiar with type theory and grammar logics -- you can always refer back to it later in case of emergency.
Chapter~\ref{chapter:chapter_2} is self-standing and provides background that is necessary for Chapter~\ref{chapter:chapter_3} to make sense -- skipping it is ill-advised.
But while Chapter~\ref{chapter:chapter_3} should prove helpful in appreciating the empirical results of Chapter~\ref{chapter:chapter_4}, it is by no means a prerequisite.
If you are indifferent to the dataset and its construction, and only interested in the neural stuff from a high-level perspective, you could skip through straight to Chapter~\ref{chapter:chapter_4} (use your power of imagination to fill in any gaps).
If you're not interested in the neural stuff either, you probably downloaded the wrong document and may as well stop reading now.
Assuming you're still with me, I wish you a pleasant reading.

\paragraph{Publications}
Chapter~\ref{chapter:chapter_2} is a novel, extended collage of work taking secondary role in \citet{kogkalidis-etal-2020-aethel} and \citet{rouss}.
Chapter~\ref{chapter:chapter_3} is an extended version of \citet{kogkalidis-etal-2020-aethel}.
Chapter~\ref{chapter:chapter_4} is based on \citet{kogkalidis-etal-2019-constructive,kogkalidis-etal-2020-neural} and \citet[preprint]{kogkalidis2022geometryaware} -- a practical summary is compiled in \citet{spindle}.
The whole manuscript is a bigger, better, faster, stronger (or so I'd like to think) version of early work delivered as part of my master's thesis~\cite{https://doi.org/10.48550/arxiv.1909.02955}.


\bibliographystyle{abbrvnat}
\relax\bibliography{bibliography}
}
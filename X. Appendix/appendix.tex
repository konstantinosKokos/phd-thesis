\chapter{Appendix}
\addtocounter{chapter}{1}

%\section{Abbreviations}
%\label{sec:abbrevations}
%
%\begin{table}[ht]
%	\hfil
%	\begin{minipage}{0.45\textwidth}
%	\begin{tabularx}{1\textwidth}{@{}cc}
%	\textbf{Abbreviation} & \textbf{Meaning}\\
%	\toprule
%	\abbrv{1} & first person\\
%	\abbrv{3} & 3rd person\\
%	\abbrv{acc} & accusative\\
%	\abbrv{adv} & adverbial\\
%	\abbrv{def} & definitive\\
%	\abbrv{dim} & neuter \\
%	\abbrv{f} & feminine\\
%	\abbrv{imp} & imperative\\
%	\abbrv{inf} & infinitive\\
%	\abbrv{m} & masculine\\
%	\abbrv{n} & neuter\\
%%	\abbrv{nmlz} & nominalization\\
%%	\abbrv{nn} & non-neuter\\
%%	\abbrv{nom} & nominative\\
%%	\abbrv{pl} & plural\\
%%	\abbrv{ptcp} & participle\\
%%	\abbrv{ptv} & partitive\\
%%	\abbrv{prs} & present\\
%%	\abbrv{pst} & past\\
%%	\abbrv{sg} & singular\\
%%	\abbrv{sup} & superlative\\
%	\end{tabularx}
%	\end{minipage}%
%	\begin{minipage}{0.45\textwidth}
%	\begin{tabularx}{1\textwidth}{cc@{}}
%	\textbf{Abbreviation} & \textbf{Meaning}\\
%	\toprule
%%	\abbrv{1} & first person\\
%%	\abbrv{3} & 3rd person\\
%%	\abbrv{acc} & accusative\\
%%	\abbrv{adv} & adverbial\\
%%	\abbrv{def} & definitive\\
%%	\abbrv{dim}\\
%%	\abbrv{f} & feminine\\
%%	\abbrv{imp} & imperative\\
%%	\abbrv{inf} & infinitive\\
%%	\abbrv{m} & masculine\\
%%	\abbrv{n} & neuter\\
%	\abbrv{nmlz} & nominalization\\
%	\abbrv{nn} & non-neuter\\
%	\abbrv{nom} & nominative\\
%	\abbrv{pl} & plural\\
%	\abbrv{ptcp} & participle\\
%	\abbrv{ptv} & partitive\\
%	\abbrv{prs} & present\\
%	\abbrv{pst} & past\\
%	\abbrv{sg} & singular\\
%	\abbrv{sup} & superlative\\
%	\\
%	\end{tabularx}
%	\end{minipage}
%	\caption{Gloss abbreviations.}
%	\label{table:gloss_abbreviations}
%\end{table}

\section{Implementation Notes}
\label{appendix:implementation_notes}
The acronym \AE thel has two readings, depending on the aspect of the E.
Read as automatically extract\textit{ed} theorems from Lassy, it is an elaborate dataset of type-checked derivations of Dutch in $\NLPplus{}$.
Read as automatically extract\textit{ing} theorems from Lassy, it is a Python library for extracting, processing and representing these derivations.
The two are made for one another; even though they can live independently, they are best presented together.
We'll start from the second aspect and move towards the first.
The exposition is honest but not exhaustive with respect to the actual code -- the intention is not to write a full API reference manual but rather just a quick how-to guide, with occasional commentary motivating design decisions.
The full source code is available at \url{https://github.com/konstantinosKokos/aethel}.%
	\footnote{Latest commit is 7e9e4c472df22582708ff03a35cf90718c17c60e.}
For up-to-date user instructions and data downloads please refer to the repository, which shall remain active and accessible for the foreseeable future.

\subsection{\NLPplus{} with Python}
\label{subappendix:nlp}
Implementing a type system in an untyped language is a perversity of nature, but enables us at least to poke fun at the run-of-the-mill medium-hosted blog post.
More than providing comedic value, it makes the code easy to integrate with standard machine learning libraries, for which Python is the de facto choice -- we're just planning ahead.
On the less fun side, the absence of first-class inductive types in Python means we'll have to use the object-oriented  abstract class pattern instead -- I apologize in advance for the mandatory eye bleach to follow.
In spite of the impediments, our implementation of \NLPplus{} proofs is as faithful as possible to the decomposition of Chapter~\ref{chapter:Introduction}.
We disentangle proofs and terms into distinct entities, following our observations that the latter can be less informative than the former (e.g. hiding structural brackets and structural rules, being equivalent under order variations, etc.).
Practically, proofs and terms are functionally related but not equivalent, i.e. each proof \textit{induces} or \textit{has} a term, but a term is \textit{not} a proof -- it might be either many or none.
To carve a path to our objects of interest, we will go from small to big, starting with the basic definitions of structures, types, structural primitives, terms and rules.


\subsubsection{Structures}
Structures are implemented as an abstract class, parameterizable with respect to their contents \py{T}.
Structures must be traversable, representable and pairwise comparable, and we must be able to check whether a structure contains an object, where the notion of containment changes depending on the concrete structure under scrutiny.

\begin{minted}{python}
class Structure(abc.ABC, typing.Generic[T]):
	def __repr__(self) -> str: ...
	def __eq__(self, other) -> bool: ...
	def __contains__(self, item) -> bool: ...
\end{minted}

\noindent\NLP{} structures are multisets; yet in our use case we do care about order (even if only in the representational sense) -- we can treat them as \textit{sequences} instead (altering the notion of equality to account for permutation invariance if/when necessary).
The $\diamond$, $\bx$ modalities impose structure in the form of brackets -- we can treat them as \textit{unary} containers.
A container structure will carry a name for its brackets; strictly speaking a tagged union object, but implemented as a string.
To canonicalize ambiguous representations and remain faithful to the absence of tree-like recursion as described in Section~\ref{subsubsection:dependency_trees}, we impose that a unary must necessarily contain a sequence (may as well be a singleton), while a sequence can contain either unary structures or elementary objects (i.e. no sequences of sequences).
With this simple mutual induction in mind, we arrive at the definitions below (the type hints serve as little more than mental notes).

\begin{minted}{python}
class Unary(Structure[T]):
	content:  Sequence[T]
	brackets: str

class Sequence(Structure[T], typing.Sequence[T]):
	structures: tuple[T | Unary[T], ...]
\end{minted}

\subsubsection{Types}
Next, we take the inductive type grammar and break it into multiple classes, starting with the abstract class that all concrete patterns inherit from.%
	\footnote{Older implementations had \py{Type} be an abstract factory pattern, with each type-pattern being a concrete factory, with the intention of dynamically constructing types that are indeed distinct python ``types''.
	This would then allow the native creation of terms of the appropriate ``type''. 
	In hindsight, the extra complexity was far from worth it -- tangling up $\NLPplus{}$ types and Python ``types'' offers little practical value aside the cheap thrill of calling \py{type()} on some proof object and reading back an actual formula.
	The two implementations are mutually compatible, though.}
Types must be representable and implement equality, while also providing auxiliary functionalities like back-and-forth translations between prefix and infix notation, computing order, stripping modalities, etc.

\begin{minted}[texcomments]{python}
class Type(abc.ABC): 
	def __repr__(self) -> str: ...
	def __eq__(self, other) -> bool: ...
	def __abs__(self) -> Type: ... # removes modalities
	def order(self) -> int: ...    # see \ref{equation:type_order}
	def prefix(self) -> str: ...  
	
	@staticmethod
	def parse_prefix(prefix: str) -> Type: ...  
\end{minted}

\noindent Different type patterns are then defined as different concrete classes.
Atoms are simple: they just contain a sign that allows their identification.
Functors are defined coordinate-wise (having an argument to the left of the arrow and a result to the right).
Modal quantifications obey the same abstraction (having a content and a decoration), differing only in whether they are a diamond or a box.

\begin{minted}{python}
class Atom(Type):
	sign: str

class Functor(Type):
	argument: Type
	result:   Type
	
class Modal(Type, ABC):
	content:    Type
	decoration: str
	
class Box(Modal):
	...
	
class Diamond(Modal):
	...
\end{minted}

\noindent By inheritance, concrete type objects are instances of both their respective construction patterns and the abstract class \py{Type}.
On the basis of the above, we implement a tiny calculator responsible for performing operations on types and asserting their validity.

\begin{minted}{python}
class TypeInference:
    class TypeCheckError(Exception): ...

    @staticmethod
    def assert_equal(a: Type, b: Type) -> None: ...
    @staticmethod
    def arrow_elim(functor: Type, argument: Type) -> Type: ...
    @staticmethod
    def box_elim(inner: Type, box: str | None) -> tuple[Type, str]: ...
    @staticmethod
    def dia_elim(inner: Type, dia: str | None) -> tuple[Type, str]: ...
\end{minted}

\subsubsection{Terms}
In the exact same vein, we have to define a painstaking number of different classes to capture all the ways we can cook up a term.
We'll adorn terms with types to ensure a first layer of well-typedness pertaining to logical constraints.
Rather than redundantly transcribe the type of all complex terms, we compute it dynamically on the basis of their primitive parts and the operations that bind them.

\begin{minted}{python}
class Term(abc.ABC):
    def __repr__(self) -> str: ...
    def __eq__(self, other) -> bool: ...
    def vars(self) -> Iterable[Variable]: ...
    def constants(self) -> Iterable[Constant]: ...

    @property
    @abstractmethod
    def type(self) -> Type: ...
\end{minted}

\noindent On top of an index allowing their identification, variables and constants must then also carry their types on their sleeve.

\begin{minted}{python}
class Variable(Term):
    type:  Type
    index: int
    
class Constant(Term):
    type:  Type
    index: int
\end{minted}

\noindent All other terms consist of subterms which allow the inductive computation of their type.
This is also used at instantiation time to assert that the term is well-formed.

\begin{minted}{python}
class ArrowElimination(Term):
    function: Term
    argument: Term

class ArrowIntroduction(Term):
    abstraction: Variable
    body:        Term

class DiamondIntroduction(Term):
    decoration: str
    body:       Term

class BoxElimination(Term):
    decoration: str
    body:       Term

class BoxIntroduction(Term):
    decoration: str
    body:       Term
\end{minted}

\noindent Since we are encoding term patterns rather than proofs, it makes sense to decompose the term rewrite prescribed by the $\diamond E$ rule into the two different patterns it involves: one for the actual removal of a diamond, done retroactively, and one for the substitution, done locally.

\begin{minted}{python}
class DiamondElimination(Term):
    decoration: str
    body:       Term

class CaseOf(Term):
    becomes:  Term
    where:    Term
    original: Term
\end{minted}

\noindent This is already showing how proofs and terms diverge.
A valid (sub-)term is not necessarily a valid proof:  its validity can only be asserted given some external context (and under structural conditions it is blind to), in turn relying on our definition of proof.

\subsubsection{Proofs}
To close the circle, we start by mimicking the definition of a judgement as an antecedent structure of variables and constants and a succedent term (which carries a type).
As before, we restrict the assumptions to being a \py{Sequence} for the sake of canonicalization.

\begin{minted}{python}
class Judgement:
    assumptions: Sequence[Variable | Constant]
    term:        Term
\end{minted}

\noindent Proof constructors are the logic's rules, which for the most part overlap with term patterns.
Exactly because of the exceptional cases where they don't, we need to actually implement them anew.

\begin{minted}{python}
class Rule(enum.Enum):
    def __repr__(self) -> str: ...
    def __str__(self) -> str: ...
    def __call__(self, *args, **kwargs) -> Proof: ...
\end{minted}

\noindent Rules are essentially implemented as enumerated types mapped to dynamically checked operations on proofs.
An organizational distinction is made between logical rules and the sole structural rule.

\begin{minted}{python}
class Logical(Rule):
    Variable = ...
    Constant = ...
    ArrowElimination = ...
    ArrowIntroduction = ...
    DiamondIntroduction = ...
    BoxElimination = ...
    BoxIntroduction = ...
    DiamondElimination = ...

class Structural(Rule):
    Extract = ...
\end{minted}

\noindent At long last, we have all the components necessary to define a proof.
A proof is a record of zero or more premises (themselves proofs), a conclusion (a verified judgement), the last rule of inference used to bind the premises together (an identifier of the previous enumeration), and maybe a variable under focus (used to tell which variable is abstracted over or substituted, for rules $\li I$ and $\diamond E$ respectively).
A proof has a structure (that of its conclusion's antecedent), a term (that of its conclusion succedent) and a type (that of its term's).
Other than being comparable, representable and yada yada, and on top of some proof-theoretic utilities, proof objects provide instance-level access to the compositional operations implemented by rules, allowing (relatively) easy bottom-up synthesis.

\begin{minted}{python}
class Proof:
    premises:   tuple[Proof, ...]
    conclusion: Judgement
    rule:       Rule
    focus:      Variable | None
    
    def __repr__(self) -> str: ...
    def __str__(self) -> str: ...
    def __eq__(self, other) -> bool: ...
    
    # self-applied rule shortcuts
    def apply(self, other: Proof) -> Proof: ...
    def diamond(self, diamond: str) -> Proof: ...
    def box(self, box: str) -> Proof: ...
    def unbox(self, box: str | None) -> Proof: ...
    def undiamond(self, where: Variable, becomes: Proof) -> Proof: ... 
    def abstract(self, var: Variable) -> Proof: ...
    def extract(self, var: Variable) -> Proof: ...

    def standardize_vars(self) -> Proof: ...
    def eta_norm(self) -> Proof: ...
    def beta_norm(self) -> Proof: ...
    def is_linear(self) -> bool: ...
    def subproofs(self) -> Iterator[Proof]: ...

\end{minted}

\noindent Since we now have access to the full picture, rule applications are sufficiently informed to assert all validity checks necessary, both logical and structural.

\subsubsection{Examples}
To see this in action, the snippets below showcase the construction of simple proofs seen through Chapter~\ref{chapter:Introduction}.
But first, some type shortcuts to make our lifes easier:

\begin{minted}{pycon}
>>> A = Atom('A')             # $\smallprop{a}$
>>> B = Atom('B')             # $\smallprop{b}$
>>> C = Atom('C')             # $\smallprop{c}$
>>> bA = Box('a', A)          # $\dbox{a}\smallprop{a}$
>>> dbA = Diamond('a', bA)    # $\ddia{a}\dbox{a}\smallprop{a}$
>>> dA = Diamond('a', A)      # $\ddia{a}\smallprop{a}$
>>> bdA = Box('a', dA)        # $\dbox{a}\ddia{a}\smallprop{a}$
\end{minted}

\noindent Then a proof pattern shortcut that from a type and an index creates the identity proof of the corresponding variable:
\begin{minted}{pycon}
>>> def var(t: Type, i: int) -> Proof:
>>>    return Logical.Variable(Variable(_type=t, index=i))
\end{minted}

\noindent With these, we can create our first toy proofs, like the axiom of identity for some type \smallprop{a} or the function composition of $\smallprop{a}\li\smallprop{b}$ and $\smallprop{b}\li\smallprop{c}$:
\begin{minted}{pycon}
>>> (x := var(A, 0)).abstract(x.term)
⊢ (λx0.x0) : A⟶A
>>> x = var(A, 0)
>>> f = var(Functor(A, B), 1)
>>> g = var(Functor(B, C), 2)
>>> g.apply(f.apply(x)).abstract(x.term)
x2, x1 ⊢ (λx0.x2 (x1 x0)) : A⟶C
\end{minted}

\noindent The story is no different for the modalities; here's deriving the interior and closure operators:
\begin{minted}{pycon}
>>> var(A, 0).diamond('a').box('a')
x0 ⊢ ▴a(▵a(x0)) : □a(◇a(A))
>>> (x := var(bA, 0)).unbox().undiamond(where=x.term, 
                                        becomes=var(dbA, 1))
x1 ⊢ case ▿a(x1) of x0 in ▾a(x0) : A
\end{minted}

\noindent As long as we refrain from initializing proof objects manually or mutating their values, rules will block us from making illegal moves:
\begin{minted}{pycon}
>>> var(Functor(A, B), 0).apply(var(B, 1)
TypeCheckError: A⟶B is not a functor of B
>>> var(A, 0).box('a')
ProofError: x0 : A is not a singleton containing a unary
\end{minted}

\subsection{Manipulating \AE thel}
\label{subappendix:aethel}
To allow the incorporation of extra-theoretical information, proofs are repackaged with the sentence into a \py{Sample} record, containing also a name (the Lassy identifier of the source file, plus a node identifier indicating the pruning point) and a subset specification, imposing a canonical train/dev/test split.

\begin{minted}{python}
class Sample:
    lexical_phrases:    tuple[LexicalPhrase, ...]
    proof:              Proof
    name:               str
    subset:             str

	def __len(self)__ -> int: ...
	def __repr__(self) -> str: ...
    def show_term(self, show_types: bool, show_words: bool) -> str: ...
    
    @property
    def sentence(self) -> str: ...
\end{minted}

\noindent Tokenization and token-level annotations are provided as a record field, populated by breaking the sentence apart into a variadic tuple of lexical phrases.
Each lexical phrase consists of one or more lexical items, but is given a single type assignment and enacts a singularly indexed proof constant.
This organization is in line with the more liberal type lexicon demanded by multiword units, and allows us to preserve lexical information that would be lost if we were to simply just squeeze them into a single ``word''.
Multiwords aside, it permits faithfully presenting a sentence together with its punctuation marks, despite them not (usually) appearing in the compositional analysis.
Finally, it disassociates proofs from the concrete lexical constants justifying them, allowing us to easily compare, filter and aggregate proofs detached from the sentences they were assigned to.

\begin{minted}{python}
class LexicalPhrase:
    items:  tuple[LexicalItem, ...]
    type:   Type

    @property
    def string(self) -> str: ...
    def __repr__(self) -> str: ...
    def __len__(self) -> int: ...

class LexicalItem:
    word:   str
    pos:    str
    pt:     str
    lemma:  str
\end{minted}

\noindent Finally, the entire dataset is packaged into a \py{ProofBank} record; practically a list of samples with some extra niceties on top, including indexing utilities and a version field used to tell different temporal instances apart.
Loading from a binarized dump is done by a convenience static function.

\begin{minted}{python}
class ProofBank:
    version: str
    samples: list[Sample]

    def __getitem__(self, item: int) -> Sample: ...
    def __len__(self) -> int: ...
    def find_by_name(self, name: str) -> list[Sample]: ...
    def __repr__(self) -> str: ...

    @staticmethod
    def load_data(path: str) -> ProofBank: ...
\end{minted}

\subsubsection{User Interface}
User interface is barebones but practical.
The user procures (some version of) the source code from the official repository and a compatible binarized dump of the dataset (download links are also provided there).
From then on, the dataset can easily be loaded as follows:

\begin{minted}{pycon}
>>> from LassyExtraction import ProofBank
>>> aethel = ProofBank.load_data('path/to/dump')
Loading and verifying path/to/dump...
Loaded æthel version 1.x.x containing xxxxx samples.
\end{minted}

\noindent Individual samples can be indexed numerically, or found by name, and their attributes can be explored according to the earlier specifications; here's inspecting the sample of Figure~\ref{figure:simple_abstraction}, for instance:

\begin{minted}{pycon}
>>> sample = aethel.find_by_name('WS-U-E-A-0000000016.p.37.s.1(3)')[0]
>>> sample.sentence
"Auto's die niet starten"
>>> sample.lexical_phrases
(LexicalPhrase(string=Auto's, type=NP, len=1),
 LexicalPhrase(string=die, type=(◇relcl(◇su(VNW)⟶SSUB))⟶□mod(NP⟶NP), len=1),
 LexicalPhrase(string=niet, type=□mod(SSUB⟶SSUB), len=1),
 LexicalPhrase(string=starten, type=◇su(VNW)⟶SSUB, len=1))
>>> sample.lexical_phrases[0].items[0]
LexicalItem(word="Auto's", pos='noun', pt='n', lemma='auto')
>>> sample.proof.structure
〈c1, 〈〈c2〉mod, c3〉relcl〉mod, c0
>>> sample.proof.term
▾mod(c1 ▵relcl((λx0.▾mod(c2) (c3 x0)))) c0 : NP
\end{minted}

\subsubsection{Visualization}
Proofs and samples can be converted to pdf format for easier inspection; the conversion utility simply follows along the inductive definitions of proofs and terms, casting them into corresponding \LaTeX{} code.
All proofs rendered in this document were built this way.
Assuming you have pdflatex installed, you can replicate this by running:
\begin{minted}{pycon}
>>> from LassyExraction.utils.tex import compile_tex, sample_to_tex
>>> tex_code = sample_to_tex(sample)
>>> compile_tex(tex_code, './output.pdf') # see Figure \ref{figure:simple_abstraction}
\end{minted}

\subsubsection{Corpus Search}
To facilitate corpus exploration, we provide search utilities in the form of composable queries and a lazy search function.
New queries can be made bottom-up from custom boolean predicates applied to samples, or point-free composed from existing queries using standard boolean operations.
Preimplemented queries include searching for samples utilizing only (a subset of) some rules, enumerating a specific number of constants, having some specific word or lemma or being of a specific type, etc.

\begin{minted}{pycon}
>>> from scripts.search import (search, contains_word,
>>>                             length_between, of_type)
>>> x = next(search(aethel, contains_word('vuur')  
>>>                         & length_between(4, 7)))
>>> x.name
'WS-U-E-A-0000000004.p.28.s.3(1).xml'
>>> x.sentence
'Het vuur greep snel om zich heen .'
\end{minted}

\noindent And here's an example of what the creation of a primitive query looks like; we start from the creation of a boolean predicate on samples that tells us whether the opening letters of each word follow a reverse alphanumeric order:
\begin{minted}{python}
def alphanumerically_ordered(sample: Sample) -> bool:
    first_letters = [lp.string[0].lower() 
                     for lp in sample.lexical_phrases]
    return first_letters == sorted(set(first_letters), reverse=True)
\end{minted}

\noindent which we can then wrap into a \py{Query} and compose it with a type condition to find the longest sentence with alphanumerically descending initial word letters:

\begin{minted}{pycon}
>>> sample = sorted(search(aethel, 
>>>                        Query(alphanumerically_ordered)
>>>                        & of_type(Atom('SMAIN')),
>>>                 key=len,
>>>                 reverse=True))[0]
>>> sample.sentence
'Ze vinden nog een bombrief ...'
\end{minted}

\noindent Less (or more) ad-hoc searches can be similarly written, predicating over any of the properties enclosed within a \py{Sample}; i.e. proof depth, maximal type order, variable count, nestedness of verbal complements, presence of a part of speech tag, or anything of the sort.

\subsection{Neural Interfacing: Spind$\lambda$e}
\label{subappendix:spindle}
A practical user-facing front integrating the neural proof search engine described in Section~\ref{section:npn} and the type system's implementation can be found at
\url{https://github.com/konstantinosKokos/spindle}. 
To install, refer to the instructions provided online; they invole downloading the source code, installing prerequisite packages and obtaining a copy of the pretrained model's parameters.
Aftewards, the model can be invoked using:

\begin{minted}{pycon}
>>> from inference import InferenceWrapper
>>> inferer = InferenceWrapper(weight_path='/path/to/model/weights')
\end{minted}

\noindent and analyses requisitioned with:
\begin{minted}{pycon}
>>> analyses = inferer.analyze(
>>>     ['omdat ik Henk haar de nijlpaarden zag helpen voeren',
>>>      'Wat is de lambda term van deze voorbeeldzin?'])
\end{minted}

\noindent What we get back is a list of Python objects that partially abide by the \py{Sample} protocol, each containing a field for lexical phrases and a proof, one such object per input sentence.
Lexical phrases are chunked by the supertagger, but lemma and part of speech information are not provided.
The compatibility between parser-produced analyses and \AE thel samples allows us to use on the former methods originally intended for the latter.
More importantly, it asserts that what we get back is not a cheap, duck-typed imitation of a proof, but a \sout{duck} proof proper. 
The parser is by construction bound to either give us some proof, or, if it fails, the reason for its failure. 

\begin{minted}[breaklines]{pycon}
>>> analyses[0].lexical_phrases
(LexicalPhrase(string=omdat, type=◇cmpbody(SSUB)⟶CP, len=1),
 LexicalPhrase(string=ik, type=VNW, len=1),
 LexicalPhrase(string=Henk, type=NP, len=1),
 LexicalPhrase(string=haar, type=VNW, len=1),
 LexicalPhrase(string=de, type=□det(N⟶NP), len=1),
 LexicalPhrase(string=nijlpaarden, type=N, len=1),
 LexicalPhrase(string=zag, type=◇vc(INF)⟶◇obj1(NP)⟶◇su(VNW)⟶SSUB, len=1),
 LexicalPhrase(string=helpen, type=◇vc(INF)⟶◇obj1(VNW)⟶INF, len=1),
 LexicalPhrase(string=voeren, type=◇obj1(NP)⟶INF, len=1))
>>> analyses[0].proof.term
c0 ▵cmpbody(c6 ▵vc(c7 ▵vc(c8 ▵obj1(▾det(c4) c5)) ▵obj1(c3)) ▵obj1(c2) ▵su(c1)) : CP
>>> analyses[1].lexical_phrases
(LexicalPhrase(string=Wat, type=(◇whbody(◇predc(VNW)⟶SV1))⟶WHQ, len=1),
 LexicalPhrase(string=is, type=◇predc(VNW)⟶◇su(NP)⟶SV1, len=1),
 LexicalPhrase(string=de, type=□det(N⟶NP), len=1),
 LexicalPhrase(string=lambda, type=□mod(N⟶N), len=1),
 LexicalPhrase(string=term, type=N, len=1),
 LexicalPhrase(string=van, type=◇obj1(NP)⟶□mod(NP⟶NP), len=1),
 LexicalPhrase(string=deze, type=□det(N⟶NP), len=1),
 LexicalPhrase(string=voorbeeldzin, type=N, len=1),
 LexicalPhrase(string=?, type=PUNCT, len=1))
>>> analyses[1].proof.term
c0 ▵whbody((λx0.c1 x0 ▵su(▾mod(c5 ▵obj1(▾det(c6) c7)) (▾det(c2) (▾mod(c3) c4))))) : WHQ
\end{minted}


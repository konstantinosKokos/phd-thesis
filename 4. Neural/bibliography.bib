@article{bangalore1999supertagging,
  title={Supertagging: An approach to almost parsing},
  author={Bangalore, Srinivas and Joshi, Aravind},
  journal={Computational linguistics},
  volume={25},
  number={2},
  pages={237--265},
  year={1999}
}
@inproceedings{joshi1994disambiguation,
  title={Disambiguation of super parts of speech (or supertags) almost parsing},
  author={Joshi, Aravind K and Bangalore, Srinivas},
  booktitle={Proceedings of the 15th conference on Computational linguistics-Volume 1},
  pages={154--160},
  year={1994}
}
@article{10.1162/tacl_a_00186,
    author = {Lewis, Mike and Steedman, Mark},
    title = "{Improved CCG Parsing with Semi-supervised
                    Supertagging}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {2},
    pages = {327-338},
    year = {2014},
    month = {10},
    abstract = "{Current supervised parsers are limited by the size of their labelled training
                    data, making improving them with unlabelled data an important goal. We show how
                    a state-of-the-art CCG parser can be enhanced, by predicting lexical categories
                    using unsupervised vector-space embeddings of words. The use of word embeddings
                    enables our model to better generalize from the labelled data, and allows us to
                    accurately assign lexical categories without depending on a POS-tagger. Our
                    approach leads to substantial improvements in dependency parsing results over
                    the standard supervised CCG parser when evaluated on Wall Street Journal (0.8\\%),
                    Wikipedia (1.8\\%) and biomedical (3.4\\%) text. We compare the performance of two
                    recently proposed approaches for classification using a wide variety of word
                    embeddings. We also give a detailed error analysis demonstrating where using
                    embeddings outperforms traditional feature sets, and showing how including POS
                    features can decrease accuracy.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00186},
    url = {https://doi.org/10.1162/tacl\_a\_00186},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00186/1566917/tacl\_a\_00186.pdf},
}
@inproceedings{turian-etal-2010-word,
    title = "Word Representations: A Simple and General Method for Semi-Supervised Learning",
    author = "Turian, Joseph  and
      Ratinov, Lev-Arie  and
      Bengio, Yoshua",
    booktitle = "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2010",
    address = "Uppsala, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P10-1040",
    pages = "384--394",
}
@inproceedings{curran-clark-2003-investigating,
    title = "Investigating {GIS} and Smoothing for Maximum Entropy Taggers",
    author = "Curran, James R.  and
      Clark, Stephen",
    booktitle = "10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2003",
    address = "Budapest, Hungary",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E03-1071",
}
@inproceedings{clark2002supertagging,
  title={Supertagging for combinatory categorial grammar},
  author={Clark, Stephen},
  booktitle={Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+ 6)},
  pages={19--24},
  year={2002}
}
@inproceedings{clark-curran-2004-importance,
    title = "The Importance of Supertagging for Wide-Coverage {CCG} Parsing",
    author = "Clark, Stephen  and
      Curran, James R.",
    booktitle = "{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics",
    month = "aug 23{--}aug 27",
    year = "2004",
    address = "Geneva, Switzerland",
    publisher = "COLING",
    url = "https://aclanthology.org/C04-1041",
    pages = "282--288",
}
@inproceedings{curran2006multi,
  title={Multi-tagging for lexicalized-grammar parsing},
  author={Curran, James R and Clark, Stephen and Vadas, David},
  booktitle={Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics},
  pages={697--704},
  year={2006}
}
@article{clark2007wide,
  title={Wide-coverage efficient statistical parsing with CCG and log-linear models},
  author={Clark, Stephen and Curran, James R},
  journal={Computational Linguistics},
  volume={33},
  number={4},
  pages={493--552},
  year={2007},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}
@inproceedings{xu-etal-2015-ccg,
    title = "{CCG} Supertagging with a Recurrent Neural Network",
    author = "Xu, Wenduan  and
      Auli, Michael  and
      Clark, Stephen",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-2041",
    doi = "10.3115/v1/P15-2041",
    pages = "250--255",
}
@inproceedings{vaswani-etal-2016-supertagging,
    title = "Supertagging With {LSTM}s",
    author = "Vaswani, Ashish  and
      Bisk, Yonatan  and
      Sagae, Kenji  and
      Musa, Ryan",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1027",
    doi = "10.18653/v1/N16-1027",
    pages = "232--237",
}
@inproceedings{lewis-etal-2016-lstm,
    title = "{LSTM} {CCG} Parsing",
    author = "Lewis, Mike  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1026",
    doi = "10.18653/v1/N16-1026",
    pages = "221--231",
}
@inproceedings{xu-etal-2016-expected,
    title = "Expected {F}-Measure Training for Shift-Reduce Parsing with Recurrent Neural Networks",
    author = "Xu, Wenduan  and
      Auli, Michael  and
      Clark, Stephen",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1025",
    doi = "10.18653/v1/N16-1025",
    pages = "210--220",
}
@inproceedings{ling-etal-2015-finding,
    title = "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation",
    author = "Ling, Wang  and
      Dyer, Chris  and
      Black, Alan W  and
      Trancoso, Isabel  and
      Fermandez, Ram{\'o}n  and
      Amir, Silvio  and
      Marujo, Lu{\'\i}s  and
      Lu{\'\i}s, Tiago",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1176",
    doi = "10.18653/v1/D15-1176",
    pages = "1520--1530",
}
@inproceedings{clark-etal-2018-semi,
    title = "Semi-Supervised Sequence Modeling with Cross-View Training",
    author = "Clark, Kevin  and
      Luong, Minh-Thang  and
      Manning, Christopher D.  and
      Le, Quoc",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1217",
    doi = "10.18653/v1/D18-1217",
    pages = "1914--1925",
    abstract = "Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text. However, the supervised models only learn from task-specific labeled data during the main training phase. We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data. On labeled examples, standard supervised learning is used. On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input. Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model. Moreover, we show that CVT is particularly effective when combined with multi-task learning. We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.",
}
@inproceedings{deoskar-etal-2011-learning,
    title = "Learning Structural Dependencies of Words in the {Z}ipfian Tail",
    author = "Deoskar, Tejaswini  and
      Mylonakis, Markos  and
      Sima{'}an, Khalil",
    booktitle = "Proceedings of the 12th International Conference on Parsing Technologies",
    month = oct,
    year = "2011",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W11-2911",
    pages = "80--91",
}
@inproceedings{deoskar2014generalizing,
  title={Generalizing a strongly lexicalized parser using unlabeled data},
  author={Deoskar, Tejaswini and Christodoulopoulos, Christos and Birch, Alexandra and Steedman, Mark},
  booktitle={Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={126--134},
  year={2014}
}
@inproceedings{thomforde-steedman-2011-semi,
    title = "Semi-supervised {CCG} Lexicon Extension",
    author = "Thomforde, Emily  and
      Steedman, Mark",
    booktitle = "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland, UK.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D11-1115",
    pages = "1246--1256",
}
@inproceedings{kogkalidis-etal-2019-constructive,
    title = "Constructive Type-Logical Supertagging With Self-Attention Networks",
    author = "Kogkalidis, Konstantinos  and
      Moortgat, Michael  and
      Deoskar, Tejaswini",
    booktitle = "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4314",
    doi = "10.18653/v1/W19-4314",
    pages = "113--123",
    abstract = "We propose a novel application of self-attention networks towards grammar induction. We present an attention-based supertagger for a refined type-logical grammar, trained on constructing types inductively. In addition to achieving a high overall type accuracy, our model is able to learn the syntax of the grammar{'}s type system along with its denotational semantics. This lifts the closed world assumption commonly made by lexicalized grammar supertaggers, greatly enhancing its generalization potential. This is evidenced both by its adequate accuracy over sparse word types and its ability to correctly construct complex types never seen during training, which, to the best of our knowledge, was as of yet unaccomplished.",
}
@inproceedings{bahdanau2015neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyung Hyun and Bengio, Yoshua},
  booktitle={3rd International Conference on Learning Representations, ICLR 2015},
  year={2015}
}
@inproceedings{NIPS2014_a14ac55a,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sequence to Sequence Learning with Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf},
 volume = {27},
 year = {2014}
}
@article{vinyals2015grammar,
  title={Grammar as a foreign language},
  author={Vinyals, Oriol and Kaiser, {\L}ukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}
@inproceedings{NIPS2014_09c6c378,
 author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and kavukcuoglu, koray},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Recurrent Models of Visual Attention},
 url = {https://proceedings.neurips.cc/paper/2014/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf},
 volume = {27},
 year = {2014}
}
@article{larochelle2010learning,
  title={Learning to combine foveal glimpses with a third-order Boltzmann machine},
  author={Larochelle, Hugo and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}
@article{cho2014properties,
  title={On the properties of neural machine translation: Encoder-decoder approaches},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.1259},
  year={2014}
}
@inproceedings{kalchbrenner2013recurrent,
  title={Recurrent continuous translation models},
  author={Kalchbrenner, Nal and Blunsom, Phil},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1700--1709},
  year={2013}
}
@inproceedings{cho2014learning,
  title={Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation},
  author={Cho, Kyunghyun and van Merrienboer, Bart and G{\"u}l{\c{c}}ehre, {\c{C}}aglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  booktitle={EMNLP},
  year={2014}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}
@inproceedings{che-etal-2018-towards,
    title = "Towards Better {UD} Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation",
    author = "Che, Wanxiang  and
      Liu, Yijia  and
      Wang, Yuxuan  and
      Zheng, Bo  and
      Liu, Ting",
    booktitle = "Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K18-2005",
    doi = "10.18653/v1/K18-2005",
    pages = "55--64",
    abstract = "This paper describes our system (HIT-SCIR) submitted to the CoNLL 2018 shared task on Multilingual Parsing from Raw Text to Universal Dependencies. We base our submission on Stanford{'}s winning system for the CoNLL 2017 shared task and make two effective extensions: 1) incorporating deep contextualized word embeddings into both the part of speech tagger and parser; 2) ensembling parsers trained with different initialization. We also explore different ways of concatenating treebanks for further improvements. Experimental results on the development data show the effectiveness of our methods. In the final evaluation, our system was ranked first according to LAS (75.84{\%}) and outperformed the other systems by a large margin.",
}
@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}
@inproceedings{kasai-etal-2017-tag,
    title = "{TAG} Parsing with Neural Networks and Vector Representations of Supertags",
    author = "Kasai, Jungo  and
      Frank, Bob  and
      McCoy, Tom  and
      Rambow, Owen  and
      Nasr, Alexis",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1180",
    doi = "10.18653/v1/D17-1180",
    pages = "1712--1722",
    abstract = "We present supertagging-based models for Tree Adjoining Grammar parsing that use neural network architectures and dense vector representation of supertags (elementary trees) to achieve state-of-the-art performance in unlabeled and labeled attachment scores. The shift-reduce parsing model eschews lexical information entirely, and uses only the 1-best supertags to parse a sentence, providing further support for the claim that supertagging is {``}almost parsing.{''} We demonstrate that the embedding vector representations the parser induces for supertags possess linguistically interpretable structure, supporting analogies between grammatical structures like those familiar from recent work in distributional semantics. This dense representation of supertags overcomes the drawbacks for statistical models of TAG as compared to CCG parsing, raising the possibility that TAG is a viable alternative for NLP tasks that require the assignment of richer structural descriptions to sentences.",
}
@article{prange-etal-2021-supertagging,
    title = "Supertagging the Long Tail with Tree-Structured Decoding of Complex Categories",
    author = "Prange, Jakob  and
      Schneider, Nathan  and
      Srikumar, Vivek",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.15",
    doi = "10.1162/tacl_a_00364",
    pages = "243--260",
    abstract = "Abstract Although current CCG supertaggers achieve high accuracy on the standard WSJ test set, few systems make use of the categories{'} internal structure that will drive the syntactic derivation during parsing. The tagset is traditionally truncated, discarding the many rare and complex category types in the long tail. However, supertags are themselves trees. Rather than give up on rare tags, we investigate constructive models that account for their internal structure, including novel methods for tree-structured prediction. Our best tagger is capable of recovering a sizeable fraction of the long-tail supertags and even generates CCG categories that have never been seen in training, while approximating the prior state of the art in overall tag accuracy with fewer parameters. We further investigate how well different approaches generalize to out-of-domain evaluation sets.",
}
@inproceedings{alvarez-melis2017treestructured,
	title={Tree-structured decoding with doubly-recurrent neural networks},
	author={David Alvarez-Melis and Tommi S. Jaakkola},
	booktitle={International Conference on Learning Representations},
	year={2017},
	url={https://openreview.net/forum?id=HkYhZDqxg}
}
@misc{kogkalidis2022geometryaware,
    title={Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions},
    author={Konstantinos Kogkalidis and Michael Moortgat},
    year={2022},
    eprint={2203.12235},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@article{bader2019computing,
  title={Computing the matrix exponential with an optimized {T}aylor polynomial approximation},
  author={Bader, Philipp and Blanes, Sergio and Casas, Fernando},
  journal={Mathematics},
  volume={7},
  number={12},
  pages={1174},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{lezcano2019trivializations,
  title={Trivializations for gradient-based optimization on manifolds},
  author={Lezcano Casado, Mario},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{bernardy2022assessing,
  title={Assessing the Unitary RNN as an End-to-End Compositional Model of Syntax},
  author={Bernardy, Jean-Philippe and Lappin, Shalom},
  journal={arXiv preprint arXiv:2208.05719},
  year={2022}
}
@inproceedings{arjovsky2016unitary,
  title={Unitary evolution recurrent neural networks},
  author={Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1120--1128},
  year={2016},
  organization={PMLR}
}

@article{bangalore1999supertagging,
  title={Supertagging: An approach to almost parsing},
  author={Bangalore, Srinivas and Joshi, Aravind},
  journal={Computational linguistics},
  volume={25},
  number={2},
  pages={237--265},
  year={1999}
}
@inproceedings{joshi1994disambiguation,
  title={Disambiguation of super parts of speech (or supertags) almost parsing},
  author={Joshi, Aravind K and Bangalore, Srinivas},
  booktitle={Proceedings of the 15th conference on Computational linguistics-Volume 1},
  pages={154--160},
  year={1994}
}
@article{10.1162/tacl_a_00186,
    author = {Lewis, Mike and Steedman, Mark},
    title = "{Improved CCG Parsing with Semi-supervised
                    Supertagging}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {2},
    pages = {327-338},
    year = {2014},
    month = {10},
    abstract = "{Current supervised parsers are limited by the size of their labelled training
                    data, making improving them with unlabelled data an important goal. We show how
                    a state-of-the-art CCG parser can be enhanced, by predicting lexical categories
                    using unsupervised vector-space embeddings of words. The use of word embeddings
                    enables our model to better generalize from the labelled data, and allows us to
                    accurately assign lexical categories without depending on a POS-tagger. Our
                    approach leads to substantial improvements in dependency parsing results over
                    the standard supervised CCG parser when evaluated on Wall Street Journal (0.8\\%),
                    Wikipedia (1.8\\%) and biomedical (3.4\\%) text. We compare the performance of two
                    recently proposed approaches for classification using a wide variety of word
                    embeddings. We also give a detailed error analysis demonstrating where using
                    embeddings outperforms traditional feature sets, and showing how including POS
                    features can decrease accuracy.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00186},
    url = {https://doi.org/10.1162/tacl\_a\_00186},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00186/1566917/tacl\_a\_00186.pdf},
}
@inproceedings{turian-etal-2010-word,
    title = "Word Representations: A Simple and General Method for Semi-Supervised Learning",
    author = "Turian, Joseph  and
      Ratinov, Lev-Arie  and
      Bengio, Yoshua",
    booktitle = "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2010",
    address = "Uppsala, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P10-1040",
    pages = "384--394",
}
@inproceedings{curran-clark-2003-investigating,
    title = "Investigating {GIS} and Smoothing for Maximum Entropy Taggers",
    author = "Curran, James R.  and
      Clark, Stephen",
    booktitle = "10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2003",
    address = "Budapest, Hungary",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E03-1071",
}
@inproceedings{clark2002supertagging,
  title={Supertagging for combinatory categorial grammar},
  author={Clark, Stephen},
  booktitle={Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+ 6)},
  pages={19--24},
  year={2002}
}
@inproceedings{clark-curran-2004-importance,
    title = "The Importance of Supertagging for Wide-Coverage {CCG} Parsing",
    author = "Clark, Stephen  and
      Curran, James R.",
    booktitle = "{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics",
    month = "aug 23{--}aug 27",
    year = "2004",
    address = "Geneva, Switzerland",
    publisher = "COLING",
    url = "https://aclanthology.org/C04-1041",
    pages = "282--288",
}
@inproceedings{curran2006multi,
  title={Multi-tagging for lexicalized-grammar parsing},
  author={Curran, James R and Clark, Stephen and Vadas, David},
  booktitle={Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics},
  pages={697--704},
  year={2006}
}
@article{clark2007wide,
  title={Wide-coverage efficient statistical parsing with CCG and log-linear models},
  author={Clark, Stephen and Curran, James R},
  journal={Computational Linguistics},
  volume={33},
  number={4},
  pages={493--552},
  year={2007},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}
@inproceedings{xu-etal-2015-ccg,
    title = "{CCG} Supertagging with a Recurrent Neural Network",
    author = "Xu, Wenduan  and
      Auli, Michael  and
      Clark, Stephen",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-2041",
    doi = "10.3115/v1/P15-2041",
    pages = "250--255",
}
@inproceedings{vaswani-etal-2016-supertagging,
    title = "Supertagging With {LSTM}s",
    author = "Vaswani, Ashish  and
      Bisk, Yonatan  and
      Sagae, Kenji  and
      Musa, Ryan",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1027",
    doi = "10.18653/v1/N16-1027",
    pages = "232--237",
}
@inproceedings{lewis-etal-2016-lstm,
    title = "{LSTM} {CCG} Parsing",
    author = "Lewis, Mike  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1026",
    doi = "10.18653/v1/N16-1026",
    pages = "221--231",
}
@inproceedings{xu-etal-2016-expected,
    title = "Expected {F}-Measure Training for Shift-Reduce Parsing with Recurrent Neural Networks",
    author = "Xu, Wenduan  and
      Auli, Michael  and
      Clark, Stephen",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1025",
    doi = "10.18653/v1/N16-1025",
    pages = "210--220",
}
@inproceedings{ling-etal-2015-finding,
    title = "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation",
    author = "Ling, Wang  and
      Dyer, Chris  and
      Black, Alan W  and
      Trancoso, Isabel  and
      Fermandez, Ram{\'o}n  and
      Amir, Silvio  and
      Marujo, Lu{\'\i}s  and
      Lu{\'\i}s, Tiago",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1176",
    doi = "10.18653/v1/D15-1176",
    pages = "1520--1530",
}
@inproceedings{clark-etal-2018-semi,
    title = "Semi-Supervised Sequence Modeling with Cross-View Training",
    author = "Clark, Kevin  and
      Luong, Minh-Thang  and
      Manning, Christopher D.  and
      Le, Quoc",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1217",
    doi = "10.18653/v1/D18-1217",
    pages = "1914--1925",
    abstract = "Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text. However, the supervised models only learn from task-specific labeled data during the main training phase. We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data. On labeled examples, standard supervised learning is used. On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input. Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model. Moreover, we show that CVT is particularly effective when combined with multi-task learning. We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.",
}
@inproceedings{deoskar-etal-2011-learning,
    title = "Learning Structural Dependencies of Words in the {Z}ipfian Tail",
    author = "Deoskar, Tejaswini  and
      Mylonakis, Markos  and
      Sima{'}an, Khalil",
    booktitle = "Proceedings of the 12th International Conference on Parsing Technologies",
    month = oct,
    year = "2011",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W11-2911",
    pages = "80--91",
}
@inproceedings{deoskar2014generalizing,
  title={Generalizing a strongly lexicalized parser using unlabeled data},
  author={Deoskar, Tejaswini and Christodoulopoulos, Christos and Birch, Alexandra and Steedman, Mark},
  booktitle={Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={126--134},
  year={2014}
}
@inproceedings{thomforde-steedman-2011-semi,
    title = "Semi-supervised {CCG} Lexicon Extension",
    author = "Thomforde, Emily  and
      Steedman, Mark",
    booktitle = "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland, UK.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D11-1115",
    pages = "1246--1256",
}
@inproceedings{kogkalidis-etal-2019-constructive,
    title = "Constructive Type-Logical Supertagging With Self-Attention Networks",
    author = "Kogkalidis, Konstantinos  and
      Moortgat, Michael  and
      Deoskar, Tejaswini",
    booktitle = "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4314",
    doi = "10.18653/v1/W19-4314",
    pages = "113--123",
    abstract = "We propose a novel application of self-attention networks towards grammar induction. We present an attention-based supertagger for a refined type-logical grammar, trained on constructing types inductively. In addition to achieving a high overall type accuracy, our model is able to learn the syntax of the grammar{'}s type system along with its denotational semantics. This lifts the closed world assumption commonly made by lexicalized grammar supertaggers, greatly enhancing its generalization potential. This is evidenced both by its adequate accuracy over sparse word types and its ability to correctly construct complex types never seen during training, which, to the best of our knowledge, was as of yet unaccomplished.",
}
@inproceedings{bahdanau2015neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyung Hyun and Bengio, Yoshua},
  booktitle={3rd International Conference on Learning Representations, ICLR 2015},
  year={2015}
}
@inproceedings{NIPS2014_a14ac55a,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sequence to Sequence Learning with Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf},
 volume = {27},
 year = {2014}
}
@article{vinyals2015grammar,
  title={Grammar as a foreign language},
  author={Vinyals, Oriol and Kaiser, {\L}ukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}
@inproceedings{NIPS2014_09c6c378,
 author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and kavukcuoglu, koray},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Recurrent Models of Visual Attention},
 url = {https://proceedings.neurips.cc/paper/2014/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf},
 volume = {27},
 year = {2014}
}
@article{larochelle2010learning,
  title={Learning to combine foveal glimpses with a third-order Boltzmann machine},
  author={Larochelle, Hugo and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}
@article{cho2014properties,
  title={On the properties of neural machine translation: Encoder-decoder approaches},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.1259},
  year={2014}
}
@inproceedings{kalchbrenner2013recurrent,
  title={Recurrent continuous translation models},
  author={Kalchbrenner, Nal and Blunsom, Phil},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1700--1709},
  year={2013}
}
@inproceedings{cho2014learning,
  title={Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation},
  author={Cho, Kyunghyun and van Merrienboer, Bart and G{\"u}l{\c{c}}ehre, {\c{C}}aglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  booktitle={EMNLP},
  year={2014}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}
@inproceedings{che-etal-2018-towards,
    title = "Towards Better {UD} Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation",
    author = "Che, Wanxiang  and
      Liu, Yijia  and
      Wang, Yuxuan  and
      Zheng, Bo  and
      Liu, Ting",
    booktitle = "Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K18-2005",
    doi = "10.18653/v1/K18-2005",
    pages = "55--64",
    abstract = "This paper describes our system (HIT-SCIR) submitted to the CoNLL 2018 shared task on Multilingual Parsing from Raw Text to Universal Dependencies. We base our submission on Stanford{'}s winning system for the CoNLL 2017 shared task and make two effective extensions: 1) incorporating deep contextualized word embeddings into both the part of speech tagger and parser; 2) ensembling parsers trained with different initialization. We also explore different ways of concatenating treebanks for further improvements. Experimental results on the development data show the effectiveness of our methods. In the final evaluation, our system was ranked first according to LAS (75.84{\%}) and outperformed the other systems by a large margin.",
}
@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}
@inproceedings{kasai-etal-2017-tag,
    title = "{TAG} Parsing with Neural Networks and Vector Representations of Supertags",
    author = "Kasai, Jungo  and
      Frank, Bob  and
      McCoy, Tom  and
      Rambow, Owen  and
      Nasr, Alexis",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1180",
    doi = "10.18653/v1/D17-1180",
    pages = "1712--1722",
    abstract = "We present supertagging-based models for Tree Adjoining Grammar parsing that use neural network architectures and dense vector representation of supertags (elementary trees) to achieve state-of-the-art performance in unlabeled and labeled attachment scores. The shift-reduce parsing model eschews lexical information entirely, and uses only the 1-best supertags to parse a sentence, providing further support for the claim that supertagging is {``}almost parsing.{''} We demonstrate that the embedding vector representations the parser induces for supertags possess linguistically interpretable structure, supporting analogies between grammatical structures like those familiar from recent work in distributional semantics. This dense representation of supertags overcomes the drawbacks for statistical models of TAG as compared to CCG parsing, raising the possibility that TAG is a viable alternative for NLP tasks that require the assignment of richer structural descriptions to sentences.",
}
@article{prange-etal-2021-supertagging,
    title = "Supertagging the Long Tail with Tree-Structured Decoding of Complex Categories",
    author = "Prange, Jakob  and
      Schneider, Nathan  and
      Srikumar, Vivek",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.15",
    doi = "10.1162/tacl_a_00364",
    pages = "243--260",
    abstract = "Abstract Although current CCG supertaggers achieve high accuracy on the standard WSJ test set, few systems make use of the categories{'} internal structure that will drive the syntactic derivation during parsing. The tagset is traditionally truncated, discarding the many rare and complex category types in the long tail. However, supertags are themselves trees. Rather than give up on rare tags, we investigate constructive models that account for their internal structure, including novel methods for tree-structured prediction. Our best tagger is capable of recovering a sizeable fraction of the long-tail supertags and even generates CCG categories that have never been seen in training, while approximating the prior state of the art in overall tag accuracy with fewer parameters. We further investigate how well different approaches generalize to out-of-domain evaluation sets.",
}
@inproceedings{alvarez-melis2017treestructured,
	title={Tree-structured decoding with doubly-recurrent neural networks},
	author={David Alvarez-Melis and Tommi S. Jaakkola},
	booktitle={International Conference on Learning Representations},
	year={2017},
	url={https://openreview.net/forum?id=HkYhZDqxg}
}
@misc{kogkalidis2022geometryaware,
    title={Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions},
    author={Konstantinos Kogkalidis and Michael Moortgat},
    year={2022},
    eprint={2203.12235},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@article{bader2019computing,
  title={Computing the matrix exponential with an optimized {T}aylor polynomial approximation},
  author={Bader, Philipp and Blanes, Sergio and Casas, Fernando},
  journal={Mathematics},
  volume={7},
  number={12},
  pages={1174},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{lezcano2019trivializations,
  title={Trivializations for gradient-based optimization on manifolds},
  author={Lezcano Casado, Mario},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{bernardy2022assessing,
  title={Assessing the Unitary RNN as an End-to-End Compositional Model of Syntax},
  author={Bernardy, Jean-Philippe and Lappin, Shalom},
  journal={arXiv preprint arXiv:2208.05719},
  year={2022}
}
@inproceedings{arjovsky2016unitary,
  title={Unitary evolution recurrent neural networks},
  author={Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1120--1128},
  year={2016},
  organization={PMLR}
}
@inproceedings{velivckovic2018graph,
  title={Graph Attention Networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@inproceedings{brody2021attentive,
  title={How attentive are graph attention networks?},
  author={Brody, Shaked and Alon, Uri and Yahav, Eran},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@inproceedings{press-wolf-2017-using,
    title = "Using the Output Embedding to Improve Language Models",
    author = "Press, Ofir  and
      Wolf, Lior",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-2025",
    pages = "157--163",
    abstract = "We study the topmost weight matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural network language models. Finally, we show that weight tying can reduce the size of neural translation models to less than half of their original size without harming their performance.",
}
@article{shazeer2020glu,
  title={{GLU} variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}
@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={933--941},
  year={2017}
}
@inproceedings{shaw-etal-2018-self,
    title = "Self-Attention with Relative Position Representations",
    author = "Shaw, Peter  and
      Uszkoreit, Jakob  and
      Vaswani, Ashish",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2074",
    doi = "10.18653/v1/N18-2074",
    pages = "464--468",
    abstract = "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.",
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{li2016gated,
  title={Gated Graph Sequence Neural Networks},
  author={Li, Yujia and Zemel, Richard and Brockschmidt, Marc and Tarlow, Daniel},
  booktitle={Proceedings of ICLR'16},
  year={2016}
}
@article{bai2019deep,
  title={Deep equilibrium models},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{dehghani2018universal,
  title={Universal Transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@article{yun2019graph,
  title={Graph transformer networks},
  author={Yun, Seongjun and Jeong, Minbyul and Kim, Raehyun and Kang, Jaewoo and Kim, Hyunwoo J},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{ying2021transformers,
  title={Do Transformers Really Perform Badly for Graph Representation?},
  author={Ying, Chengxuan and Cai, Tianle and Luo, Shengjie and Zheng, Shuxin and Ke, Guolin and He, Di and Shen, Yanming and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{liao2019efficient,
  title={Efficient graph generation with graph recurrent attention networks},
  author={Liao, Renjie and Li, Yujia and Song, Yang and Wang, Shenlong and Hamilton, Will and Duvenaud, David K and Urtasun, Raquel and Zemel, Richard},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{pareja2020evolvegcn,
  title={Evolve{GCN}: Evolving graph convolutional networks for dynamic graphs},
  author={Pareja, Aldo and Domeniconi, Giacomo and Chen, Jie and Ma, Tengfei and Suzumura, Toyotaro and Kanezashi, Hiroki and Kaler, Tim and Schardl, Tao and Leiserson, Charles},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={5363--5370},
  year={2020}
}
@inproceedings{cai2020graph,
  title={Graph transformer for graph-to-sequence learning},
  author={Cai, Deng and Lam, Wai},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={7464--7471},
  year={2020}
}
@inproceedings{zhu-etal-2019-modeling,
    title = "Modeling Graph Structure in Transformer for Better {AMR}-to-Text Generation",
    author = "Zhu, Jie  and
      Li, Junhui  and
      Zhu, Muhua  and
      Qian, Longhua  and
      Zhang, Min  and
      Zhou, Guodong",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1548",
    doi = "10.18653/v1/D19-1548",
    pages = "5459--5468",
    abstract = "Recent studies on AMR-to-text generation often formalize the task as a sequence-to-sequence (seq2seq) learning problem by converting an Abstract Meaning Representation (AMR) graph into a word sequences. Graph structures are further modeled into the seq2seq framework in order to utilize the structural information in the AMR graphs. However, previous approaches only consider the relations between directly connected concepts while ignoring the rich structure in AMR graphs. In this paper we eliminate such a strong limitation and propose a novel structure-aware self-attention approach to better model the relations between indirectly connected concepts in the state-of-the-art seq2seq model, i.e. the Transformer. In particular, a few different methods are explored to learn structural representations between two concepts. Experimental results on English AMR benchmark datasets show that our approach significantly outperforms the state-of-the-art with 29.66 and 31.82 BLEU scores on LDC2015E86 and LDC2017T10, respectively. To the best of our knowledge, these are the best results achieved so far by supervised models on the benchmarks.",
}
@inproceedings{honnibal2010rebanking,
  title={Rebanking CCGbank for improved NP interpretation},
  author={Honnibal, Matthew and Curran, James R and Bos, Johan},
  booktitle={Proceedings of the 48th annual meeting of the association for computational linguistics},
  pages={207--215},
  year={2010}
}
@inproceedings{
	loshchilov2018decoupled,
	title={Decoupled Weight Decay Regularization},
	author={Ilya Loshchilov and Frank Hutter},
	booktitle={International Conference on Learning Representations},
	year={2019},
	url={https://openreview.net/forum?id=Bkg6RiCqY7},
}
@inproceedings{delobelle2020robbert,
    title = "{R}ob{BERT}: a {D}utch {R}o{BERT}a-based {L}anguage {M}odel",
    author = "Delobelle, Pieter  and
      Winters, Thomas  and
      Berendt, Bettina",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.292",
    doi = "10.18653/v1/2020.findings-emnlp.292",
    pages = "3255--3265"
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@inproceedings{martin2020camembert,
  title={CamemBERT: a Tasty French Language Model},
  author={Martin, Louis and Muller, Benjamin and Su{\'a}rez, Pedro Javier Ortiz and Dupont, Yoann and Romary, Laurent and de la Clergerie, {\'E}ric Villemonte and Seddah, Djam{\'e} and Sagot, Beno{\^\i}t},
  booktitle={ACL 2020-58th Annual Meeting of the Association for Computational Linguistics},
  year={2020}
}
@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}
@article{Liu_Ji_Wu_Lan_2021,
 title={Generating CCG Categories},
  volume={35},
   url={https://ojs.aaai.org/index.php/AAAI/article/view/17586}, DOI={10.1609/aaai.v35i15.17586},
   abstractNote={Previous CCG supertaggers usually predict categories using multi-class classification. Despite their simplicity, internal structures of categories are usually ignored. The rich semantics inside these structures may help us to better handle relations among categories and bring more robustness into existing supertaggers. In this work, we propose to generate categories rather than classify them: each category is decomposed into a sequence of smaller atomic tags, and the tagger aims to generate the correct sequence. We show that with this finer view on categories, annotations of different categories could be shared and interactions with sentence contexts could be enhanced. The proposed category generator is able to achieve state-of-the-art tagging (95.5% accuracy) and parsing (89.8% labeled F1) performances on the standard CCGBank . Further-more, its performances on infrequent (even unseen) categories, out-of-domain texts and low resource language give promising results on introducing generation models to the general CCG analyses.}, number={15}, journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    author={Liu, Yufang and Ji, Tao and Wu, Yuanbin and Lan, Man},
    year={2021}, 
    month={May}, 
    pages={13443-13451}
}
@inproceedings{tian2020supertagging,
  title={Supertagging Combinatory Categorial Grammar with Attentive Graph Convolutional Networks},
  author={Tian, Yuanhe and Song, Yan and Xia, Fei},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={6037--6044},
  year={2020}
}
@misc{moot2019,
  title={Reconciling vectors with proofs for natural language processing},
  author={Moot, Richard},
  howpublished={Compositionality in formal and distributional models of natural language semantics, 26th Workshop on Logic, Language, Information and Computation (WoLLIC 2019)},
  month={July},
  year={2019},
  note={Retrieved from \url{https://richardmoot.github.io/Slides/WoLLIC2019.pdf}}
}
@inproceedings{kogkalidis-etal-2020-neural,
    title = "Neural Proof Nets",
    author = "Kogkalidis, Konstantinos  and
      Moortgat, Michael  and
      Moot, Richard",
    booktitle = "Proceedings of the 24th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.conll-1.3",
    doi = "10.18653/v1/2020.conll-1.3",
    pages = "26--40",
    abstract = "Linear logic and the linear Î»-calculus have a long standing tradition in the study of natural language form and meaning. Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation of derivations that is unburdened by the bureaucratic complications of conventional prooftheoretic formats. Building on recent advances in set-theoretic learning, we propose a neural variant of proof nets based on Sinkhorn networks, which allows us to translate parsing as the problem of extracting syntactic primitives and permuting them into alignment. Our methodology induces a batch-efficient, end-to-end differentiable architecture that actualizes a formally grounded yet highly efficient neuro-symbolic parser. We test our approach on {\AE}Thel, a dataset of type-logical derivations for written Dutch, where it manages to correctly transcribe raw text sentences into proofs and terms of the linear Î»-calculus with an accuracy of as high as 70{\%}.",
}
@article{fey2019fast,
  title={Fast graph representation learning with PyTorch Geometric},
  author={Fey, Matthias and Lenssen, Jan Eric},
  journal={arXiv preprint arXiv:1903.02428},
  year={2019}
}


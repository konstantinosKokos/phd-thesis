\chapter{Learning to Prove}
\label{chapter:chapter_4}


\chapabstract{\todo}


Reflections are normally reserved for the end of a chapter, but I'm in a pensive mood so we'll have them here instead.
This thesis was originally envisaged as part of a broader whole, a stepping stone toward an integrated approach at structural reasoning and meaning representation; the end goal was to be a prototype for a type-driven composition calculus for vector-based semantic modeling.
In the project's days of inception, that goal was not just noble but also technically feasible; distributional semantics and word vectors were in their heyday, machine learning was rapidly taking off, parsers all of a sudden started becoming reliable.
Everything seemed to point towards the imminent bloom of a new era in natural language processing (NLP), an era where the wisdoms of old would meet the machines of today, hinting at a bright and prosperous future for neurosymbolic structure-aware semantic composition models.
And things did seem to go that way, at least for a few years.
But, unbeknownst to all, the new era in the evolutionary history of NLP would actually turn out to also be the dumbest (yet).
The advent of data efficient neural architectures -- combined with their immense potential for commercial applications, and its allure to big corporations -- brought large language models into the game.
These are unsophisticated, profanely over-parameterized, general purpose systems, fed unprocessed texts for weeks on end until they'd learn to convincingly imitate its use.
With their sheer strength, large language models usurped the heir apparent and condemned structure-aware semantic computation to obscurity; the future of computational semantics is to be opaque, boring, exclusive to big tech and provided as-a-service instead.
My thesis got caught in the blast of this change of power, necessitating a clear positioning in the current state of affairs, and a careful motivation for the chapter to follow; anything and everything done as -- or in the name of -- science requires justification after all.
So here goes.

Parsing is good.
Converting raw signals intro structured representations thereof allows us to standardize their machine processing, and elevates automated reasoning away from form and into substance.
The more well-behaved the representational format chosen, the more powerful, transparent and verifiable the reasoning can be.
The less localized and problem-specific the representational format chosen, the more adaptive and better understood the reasoning can be.
On the basis of these observations, $\lambda$ calculi make for an ideal representation format.
Choice of format aside, a formal system operating on formal representations is not prone to implicit biases, latent variables, ambiguity or inconsistency: erroneous outputs are the result of bad input or bad programming; there's always someone to blame.
Specifically in the natural language domain, advancing the conversion of text into formal representations is promoting accountable automation of textual processing, and eliminates the anthromorphic delusion of the ghost in the machine.
Parsing is therefore only superficially in competition with large language models, and its seeming obsoletion is just a by-product of the ephemeral and rapidly shifting pop science trends of the ``AI'' race.

That said, machine learning is not bad.
Shifting the focus away from the algorithm and toward the data can often be a reasonable concession in the automation of complex or labor-intensive tasks, provided that the task is not risk critical and that no intelligence is attributed to the end system.
This is especially the case if ``almost correct'' is almost as good as correct, or the problem being modeled is intractable, making an approximation the best we can hope for.
But employing machine learning has to be thought of as either a shortcut, or an admission of defeat.
In opting for a machine learning solution, one assigns more faith to a generic data cruncher in solving a problem, than to oneself in designing a solution to that very problem.

Interweaving symbolic and subsymbolic reasoning is then the responsible engineer's out.
A bipartite disassembly promotes the selective expenditure of formal effort where it really is needed, while still benefiting from the high horsepower of brute force statistical machinery.
Complicated but decipherable components, rich in hierarchical or recursive structure, and requiring or greatly benefiting from formal transparency are to be tackled explicitly.
Components that are laborious but uninteresting, data intensive or intractable are to be isolated and outsourced to a machine worker.
In this here context, I'm claiming that large language models should be treated not as a substitute, but as complementary to logic-based systems.
This is exactly the route we'll follow in this chapter, where we'll go through the hoops of designing and implementing a formally disciplined but accurate and robust wide-coverage parser, a novel neurosymbolic architecture aimed at substructural logics of the linear lineage, instantiated here for \NLPplus{} and trained on \AE thel.

\section{The Categorial Parser}
We'll start with a high-level conceptualization of the categorial grammar parser.
In the infancy of categorial grammars, the parser would be thought of as nothing other than a lexicon and a theorem prover: the lexicon enumerating any and all the possible type assignments for each word, the theorem prover exhaustively iterating the combinatorial space of assignments to produce all possible proofs for each possible assignment (Figure~\ref{figure:archetypical_parser}).

\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}[t/.style={text height=1.5ex, text depth=.25ex, rectangle, outer sep=0pt}, node distance=10pt]
	\node[t] (w1) 			at (0, 0) {w\textsubscript{0}};
	\node[t] (wdots)		at (2, 0) {\dots};		
	\node[t] (wn) 			at (4, 0) {w\textsubscript{n}};
	\node[rectangle,draw=black, minimum width=120pt,minimum height=20pt] (lex)
						 	at (2, -1.5) {Lexicon};
	\draw[->]  (w1) -- ++ (0.6, -1);
	\draw[->] (wn) -- ++ (-0.6, -1);
	\node[t] (t1)			at (0.2, -3.2) {$\text{t}_0^0 \ | \ \text{t}_0^1 \ ... \text{t}_0^p$};
	\node[t] (tdots)		at (2, -3.2) 	  {\dots};
	\node[t] (tn)			at (3.8, -3.2) {$\text{t}_n^0 \ | \ \text{t}_n^1 \ ... \text{t}_n^q$};
	\draw[->] ($(w1) + (0.6, -2)$) -- ($(t1.north) + (0, 0.1)$);
	\draw[->] ($(wn) + (-0.6, -2)$) -- ($(tn.north) + (0, 0.1)$);
	\draw [thick,decorate,decoration={brace,aspect=0.2,amplitude=10pt,mirror}] (-0.5,-3.5) -- (4.5,-3.5) 
			node[black,xshift=-113.5pt,yshift=-0.6cm] {\footnotesize $\times$};
	\node[t] (c11)			at (1.5, -5) {$\text{t}^0_0$};
	\node[t] 				at (2.75, -5) {\dots};
	\node[t] (cn1)			at (4, -5) {$\text{t}^0_n$};
	\node[t] (c12)			at (1.5, -6) {$\text{t}^0_0$};
	\node[t] 				at (2.75, -6) {\dots};
	\node[t] (cn2)			at (4, -6) {$\text{t}^1_n$};
	\node[t] 				at (1.5, -7) {\vdots};
	\node[t] (c1k)			at (1.5, -8) {$\text{t}^p_0$};
	\node[t] 				at (2.75, -8) {\dots};
	\node[t] (cnk)			at (4, -8) {$\text{t}^q_n$};
	\draw[->, thick, rounded corners] (0.5, -4.3) -- ++ (0, -0.7) -- ++ (0.75, 0);
	\draw[->, thick, rounded corners] (0.5, -4.3) -- ++ (0, -1.7) -- ++ (0.75, 0);
	\draw[->, thick, rounded corners] (0.5, -4.3) -- ++ (0, -3.7) -- ++ (0.75, 0);
	\node[rectangle, draw=black, minimum height=120pt] (pr)
							at (7, -6.5)	{\begin{tabular}{c}Theorem\\ Prover\end{tabular}};	
	\draw[->] (cn1) ++ (0.4, 0) -- ++ (1.3, 0);
	\draw[->] (cn2) ++ (0.4, 0) -- ++ (1.3, 0);
	\draw[->] (cnk) ++ (0.4, 0) -- ++ (1.3, 0);
	\draw[->]  (cn1) ++ (4.3, -1.5) -- ++ (1, 0) node[right] {\{p\textsubscript{0}\dots p\textsubscript{k}\}};
	\end{tikzpicture}
	\caption{The archetypical categorial grammar parsing pipeline.}
	\label{figure:archetypical_parser}
\end{figure}

Obviously, this setup hits a brick wall in the sheer complexity of real human language.
As we have discussed in earlier chapters, a type system enacting a strict grammar logic is not just hard to design, but also entails a prohibitively ambiguous type lexicon.
Even if the theorem prover is perfectly optimized, the architecture will become bottlenecked at its input.
The total number of assignments to consider in a sentence increases exponentially with its length, so even a minor increase in the average number of types per lexical key will have a tremendous impact in processing time.
At the same time, a fixed lexicon is a severely limiting factor, as it effectively forbids processing sentences containing unseen lexical entries (i.e. in cases of a \textlangle word, type\textrangle{} pair missing from the lexicon).
Relaxing the structural properties of the type system to ease lexical pressure is not a panacea either.
Other than the prover becoming ambiguous, more proofs will become accessible, and inacessible proofs will take longer to reject.
From the implementer's perspective, lexicon and grammar are not synergistic but in conflict with one another, and a middle ground must be found for them to work well in tandem.

For the categorial program to come to fruition, these very real problems require equally real solutions.
The practitioner must often resort to tricks aimed at compressing or efficiently navigating the enormous search space.
The modern pipeline commonly outsources lexical disambiguation to a statistical component, referred to as the \textit{supertagger}.
The supertagger is tasked with ranking the possible assignments to a single key, given its context of appearance.
Assignments are ranked according to their likelihood, in turn approximated on the basis of some training data.
Depending on the quality and speed of the statistical estimator employed, the candidates returned are truncated depending on some threshold likelihood or by their count.
This (partially) sidesteps the explosive combinatorics of considering all potential assignments, setting an upper boundary to the cardinality of the parser's input.
The parser may also be sped up by allowing yet another statistical model to guide its actions anytime it hits a decision point.
As with all real solutions, perfect is unattainable; this time/space efficiency usually comes at the cost of approximation errors that translate to foregone rigidness, correctness and/or coverage.
The strategy we'll follow does not challenge this general model, but contributes some new insights to the operationalization of its components.

A foreword before we get to it: I imagine a crash course in machine learning to be redundant in the current day and age.
In any case, my intention is to help the purists make sense of what's going on, yet without obfuscating the implementation from fellow hackers.
To that end, I'll try keep the gory technical details contained and separate from the high level, abstract descriptions.
I hope the result is sensible and inclusive.


\section{Supertagging the Tail}
A supertagger is a statistical model, a parametric function $f_\theta$ tasked with producing the most likely type assignment sequence $\tseq := t_0 \dots t_n$ for a given sentence $\wseq := w_0 \dots w_n$.
\begin{equation}
	f_\theta(\wseq) \approx \underset{\tseq}{\mathsf{argmax}} ~ p(\tseq \ | \wseq, \theta)
\end{equation}
To do so, it should in theory approximate the probability of a type assignment sequence conditional on the input; in other words, feeding $f_\theta$ with any element of the product space $\lexicon^k$ should implicitly produce a total order over the product space $\types^{k}$, where $\lexicon$ the set of words in the language, $\types$ the type universe, and $k$ ranging over $\mathbb{N}$.
If that looks stupidly intractable, it's because it is.
Both domain and codomain are practically infinite: regardless of what the cardinality of $\lexicon$ and $\types$ are, the number of combinations between different sequences thereof quickly exceeds our current estimates for stars in the universe, as the sequence length increases.
Put simply, no amount of sample data would ever be able to overcome the problem's inherent sparsity and allow for a direct attempt at an approximation.
Therefore, in practice, some truncations and independence assumptions are necessary in how we choose to formulate the sequence-wide conditional assignment probability, the decomposition of which will be the focal point of our discussion:
\begin{equation}\label{equation:supertag}
p(\tseq \ | \ \wseq)
\end{equation}
A choice of assumptions and truncations is a prerequisite before we even get to contemplate the model's implementation.
Each choice can (and will) have a deep impact on the model's performance, most notably in phenomena inhabiting the more remote regions of the probability density function's landscape.
As a corollary, each choice will alter how suitable a model is to one single grammar depending exactly on how that landscape looks.
This last fact seems to have largely been dismissed by the broader practitioner community, who treat the problem with consistent indifference, changing the viewing lens only according to the quirks and fashions of contemporary machine learning standards.
We will shamelessly fall into the same last trap, but in our downfall we will at least be conscious of the intellectual and ideological roots the earlier chapters have established; those of revealing structure previously hidden, and paying that structure its due respect.

\subsection{A Brief History of Supertagging}
To actually perceive the structure, we must first notice its absence -- therefore (and for maintaining suspense), we will first outline the short but dense history of supertagging, and sketch out the paradigm shifts it has undergone throughout.

\subsubsection{Origins}
Supertagging (both the term and the idea) are due to the early insights of~\citet{joshi1994disambiguation}.
The two correctly pointed out that, for a strongly lexicalized grammar (in their case, a tree adjoining grammar), assigning the correct grammatic descriptor, or \textit{supertag} (in our case, a type), to each word in a sentence amounts to \textit{almost} parsing, and that even just weeding out some of the erroneous assignments significantly facilitates parsing.
Early literature was characterized by an almost single-minded attachment to localized computation, the justification being that supertagging must remain localized for it not to become ``too much like parsing''~\cite{bangalore1999supertagging}.
With the benefit of hindsight, we can safely call this a pragmatic consideration and an artifact of the times, with the scene largely dominated by window-based models.

A so-called unigram model assumes full independence between subsequent words, and (\ref{equation:supertag}) boils down to:
\begin{equation}
\prod_i^n p(t_i \ | \ w_i)
\end{equation}
where each local conditional can be estimated on the basis of corpus frequencies.
Despite competely breaking apart sequential sparity, this formulation is not much good on its own either; rarely occurring lexical keys hardly provide sufficient data for an empirical distribution to be extracted.
As a solution, plain part of speech tags would find use as an intermediary, i.e. $w_i$ would in practice be substituted by $pos_i$, which would in turn be supplied by an external tagger.
The resulting model is, alas, too simple to find real use: the assumptions made are exceedingly naive, and lexicalization is heavily bottlenecked by the coarse and undescriptive part of speech tags; we need to do better.

Invoking Bayes' rule and factoring out the denominator has (\ref{equation:supertag}) rewrite to the proportionate quantity:
\begin{align}
\propto p(\wseq \ | \ \tseq)~ p(\tseq)
\end{align}
Extending the context to a window of size $\kappa$, allows local decisions to excert direct influence to the next $\kappa$ predictions (and thus indirectly affect all future ones).
This requires approximating the \textit{contextual probability} $p(\tseq)$ as:
\begin{equation}\label{equation:contextual_prob_jb}
p(\tseq) \approx \prod_i^n (t_i \ | \ \seq{t}{i-\kappa:i-1})
\end{equation}
Going one step further and making the assumption that the \textit{emission probability} $p(\mathbf{w}_{0:n} \ | \ \mathbf{t}_{0:n})$ is position-separable and independent allows its rewrite to:
\begin{equation}\label{equation:emission_prob_jb}
p(\mathbf{w}_{0:n} \ | \ \mathbf{t}_{0:n}) \approx \prod_i^n p(w_i \ | \ t_i)
\end{equation}
Putting (\ref{equation:contextual_prob_jb}) and (\ref{equation:emission_prob_jb}) together, we get an approximation to (\ref{equation:supertag}) that is blatantly wrong.
Despite the fact, it is also workable, in having efficiently circumvented sparsity, adequate, in having accounted for the very important axis of output-to-output interactions, and practical, in allowing a tangible implementation as a hidden Markov model.


\subsubsection{CCGbank and the Original Sin}
The problem garnered attention and built significant impetus with the release of the CCGbank, whose large size and gold standard nature offered an excellent test bed for molding the first generation of supertaggers.
The first incarnation of a combinatory categorial grammar supertagger would be the original work of~\citet{clark2002supertagging}.
The model diverged from the implementation of~\citet{bangalore1999supertagging} in opting for a larger window size and foregoing the contextual effect of output-to-output dependencies.
In that setting, and for a window of size $2\kappa + 1$, (\ref{equation:supertag}) would take the form:
\begin{equation}
	\prod_i^n p(t_i \ | \ \seq{w}{i - \kappa : i+\kappa})
\end{equation}
The model would materialize as a log-linear feature weighter trained as a maximum entropy estimator.
The input would include several sparse heuristics, including morphological features and boolean context predicates, allowing a partial soft bypass of the fixed-key lexicon.

Novelty and ingenuity aside, the work set a number of precedents; some of those, reasonable as they may have been at the time, have since permeated through the problem statement, becoming \textit{de facto} practices rather than conscious design decisions.
Structural constraints were dropped, in part because they are less straightforward to deduce in frameworks other than tree adjoining grammars; they never found their way back into the mainstream, athough admittedly they never were particularly sophisticated to begin with.
This step away from structural discipline is exacerbated by having also dropped the supertag-to-supertag dependencies, since the model now has no chance of learning how to statistically filter out mutually incompatible assignments either.
To counteract the problem, the paper opted for a yet more radical solution: abandoning the sequential formulation (i.e. no more $\mathsf{argmax}$ing over the product) in favor of a \textit{multitagging} approach (i.e. returning all categories whose local probability exceeds some fixed ratio of the highest ranked candidate).
This was shown to tremendously improve coverage (by outsourcing heavier duty to the parser), but has to be understood as a practical overcorrection, an emergency measure to sidepass the model's inherent disregard to output-level sequential interactions.
The limitation is acknowledged by \citet{clark-curran-2004-importance}, and an attempt at resolution is offered by \citet{curran2006multi}.
There, the forward-backward algorithm is employed to efficiently calibrate the probability of an assignment (in the multitagging setup) as the sum of all sequential assignments containing it:
\begin{equation}
	p(t_i \ | \ \mathbf{w}_{0:n}) = \sum_{\mathbf{t}_{0:i-1}, \mathbf{t}_{i+1:n}} p(\mathbf{t}_{0:i-1}, t_i, \mathbf{t}_{i+1:n} \ | \ \mathbf{w}_{0:n})
\end{equation}
This does reinstate a notion of output-to-output dependencies in the form of estimated posteriors, but computational considerations have diffused the potential for widespread adoption in later frameworks. 
Finally, rare supertags, which were particularly problematic or near impossible to learn, were found to have very limited impact on overall coverage; this set the grounds for their (statistically) near inconsequential erasure, a choice that gradually became ingrained as a mandatory step of data sanitation and preprocessing.


\subsubsection{Distributed Word Vectors \& Neural Networks}
The advent of word embeddings and the gradual substitution of sparse features with continuous vectors paved the way for the incorporation of artificial neural networks.
\citet{10.1162/tacl_a_00186} employed a collection of pretrained embeddings combined with a window-based two-layer network in a ``semi-supervised'' manner (in today's jargon, a pretty much fully supervised separable convolution), to a dual effect.
On the one hand, the pretrained embeddings offered a natural generalization from the fixed size lexicon to the (still fixed, but much larger) set of pretrained` embeddings, single handedly obsoleting the long standing problem of tackling rare and unseen words~\cite{thomforde-steedman-2011-semi,deoskar-etal-2011-learning,deoskar2014generalizing}.
On the other hand, the parameter-sharing convolution improved the accuracy/ambiguity ratio and overall efficiency of the (then standard) log-linear supertagger of~\citet{clark2007wide}.
Unlike before, words were allowed to associate to any supertag, regardless of whether or not a \textlangle word, supertag\textrangle{} pair was observed during training; the lexicon thus turning from a hard imperative to a soft guideline.
Additional experiments involving a conditional random field were mildly succesful, but abandoned due to the prohibitively slow decoding -- yet, it is obvious to the people involved that something critical is missing.
\citet{xu-etal-2015-ccg} took the approach a step further by utilizing a simple recurrent network (RNN), and, in doing so, claimed to sidestep the locality of the previous neural model.
To escape the unidirectional constraint of the classical recurrent network (or perhaps out of force of habit?), they continued incorporating window-based features that provided a minimal amount of right context $\kappa$, thereby rewriting (\ref{equation:supertag}) as:
\begin{equation}
	\prod_i^n p(t_i \ | \ \seq{w}{0:i+\kappa})
\end{equation}
And while their approach does indeed offer a wider receptive field (and a better operationalization), it is still focused solely on the input side; output-to-output interactions are still nowhere to be seen, leaving their promise unfulfilled.

\subsubsection{Autoregressive Modeling}
By now (and despite earlier aphorisms), it is becoming increasingly evident that nothing deep or spiritual restricts supertagging to remaining local, as advancements in machine learning are progressively offering more opportunities for fast and efficient incorporation of ever wider context.
But the absence of output-to-output dependencies remains unresolved, despite it being a recurrent theme in the literature.
This changes with the work of \citet{vaswani-etal-2016-supertagging} who score two major points with their resourceful use of long short-term memory networks (LSTM).
First, they replace the simple recurrence of \citet{xu-etal-2015-ccg} with a bidirectional one (thus allowing unbounded left- and right- input interactions) --  an idea explored in parallel by multiple contemporary works~\cite[\textit{inter alia}]{ling-etal-2015-finding,xu-etal-2016-expected,lewis-etal-2016-lstm}.
More importantly and in addition to that, they introduce an intermediate recurrence that is to serve as a supertag-level language model, fusing the prediction history with the input context to produce each local prediction.
The two together alter (\ref{equation:supertag}) into a version far more elaborate than previous proposals:
\begin{equation}\label{equation:lstm_lm}
	\prod_i^n p(t_i \ | \ \seq{t}{0:i-1},\wseq)
\end{equation}
Under a modern lens, this is akin to a slighly clumsy implementation of an autoregressive sequence-to-sequence model, with the decoder meriting from perfect alignment between input and output tokens.
Unlike prior work, no occurrence threshold was imposed, and explicit evaluations over sparse lexical relations were provided.
The added expressivity and significantly wider receptive field granted LSTM models the state of the art badge, which was to remain uncontested for a surprisingly long two human years%
	\footnote{Approx. three centuries in machine learning years.}.

\subsubsection{Superwhat?}
More than just a testament to the LSTM's strengths, this momentary pause makes for a discontinuity in the velocity of progress; not because people suddenly lost interest in supertagging, but rather because machine learning architectures and their applications had slowly become exhausted.
This coincides with a stall across NLP in general, and a concurrent paradigm shift; specialized models started becoming outfashioned, and improvements would no longer be enabled by domain expertise and task-specific engineering, but rather by higher quality unsupervised and semi-supervised training routines over larger and larger models.
A case in point is the next major landmark, in fact reached by a structurally simplified model~\cite{clark-etal-2018-semi}, 
a plain bidirectional sequence encoder using the factorization:
\begin{equation}\label{equation:seq_cls}
	\prod_i^n p(t_i \ | \ \wseq)
\end{equation}
Despite taking a step backwards in terms of structural sophistication, the model managed a performance leap comparable to that of switching from a separable neural function to a recurrent one (see Figure~\ref{figure:supertag_history}), all by ``simply'' incorporating multiple tasks and losses in its training loop.
The same paradigm is today more dominant than ever, and has pushed conventional NLP outside of the spotlight, putting an end to an exciting but short golden era.

\begin{figure}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	    xlabel={Publication Year},
	    ylabel={Accuracy (greedy)},
%	    xmin=1, xmax=48,
%	    ymin=-0, ymax=1,
%	    xtick={1,10,100,1000,10000, 100000}, 
	    legend pos=north west,
	    ymajorgrids=true,
		minor y tick num=1,
	    yminorgrids=true,
	    xmajorgrids=false,
%	    xminorgrids=true,
	    axis line style={draw=none},
	    tick style={draw=none},
%        legend pos=north east,
		xticklabels={,,2002,,,,,,,,2018},
		x tick label style={/pgf/number format/.cd, 1000 sep={},},
        width=0.85\textwidth,
		enlarge x limits=0.1,
		point meta=explicit symbolic,
		nodes near coords,
         nodes near coords style={
         	anchor=west,
            font=\small,
        },
	]
	    
	 
 	\addplot[mark = *, only marks,mark options={black}]%]
 		table[x=X,y=Y,meta=M] {
 			X		Y		M
 			2002 	90.5 	\citet{clark2002supertagging}
 			2004	91.5	\citet{clark-curran-2004-importance}
 			2014	91.3	\citet{10.1162/tacl_a_00186}
 			2015	93.07	\citet{xu-etal-2015-ccg}
 			2016	94.24	\citet{vaswani-etal-2016-supertagging}
 			2016	94.7	\citet{lewis-etal-2016-lstm}
 			2018	96.05	\citet{clark-etal-2018-semi}
 		};
	 \end{axis}
	\end{tikzpicture}
	\caption{Supertagging performance in the CCGbank historically.}
	\label{figure:supertag_history}
\end{figure}


\subsection{Constructing Types}
The supertagging architectures reviewed are, from first to last, variations on a theme.
Regardless of whether the underlying statistical machinery is a hidden Markov model, a maximum entropy model, a neural sequence tagger or a sequence-to-sequence transducer, a single commonality characterizes them all: they start from the assumption of a finite codomain.
More than that, they don't just assume but \textit{require} that the zipfian tail of lexical type sparsity is practically irrelevant for the corpus, and, by extension, for language at large.
In other words, they require that most of the probability mass of type occurrences is concentrated around a central region of few but common types, and that exceptionally rare types are nothing but statistical artifacts which can safely be ignored.
This bias is not to be mistaken for a vice, nor for a deeply motivated ruling; it is a practical compromise that became an unwritten rule, similar to the (once proclaimed as necessary) locality of supertagging -- a notion since abandoned and forgotten with minimal remorse and deliberation as soon as technology allowed.
The issue is really quite shallow: statistical models have always had a very hard time dealing with under-represented samples (in our case, supertags), and correctly predicting items outside the training data is an open problem.

That, again, is not to say the compromise is an unjust one; its heedless proliferation does, however, come with two major side effects.
One, it forces parsers to give up on potentially rare syntactic phenomena, assuming those manifest through unique and uncommon supertags.
Even though a parser should in principle be able to handle any valid supertag (regardless of its statistical properties), the \textit{a priori} exclusion of rare ones corresponds to an externally imposed restriction to its generalization.
In other words, exactly those difficult phenomena that would benefit from the linguistic expertise of a robust parser are to be discarded in the first place!
The problem is of an epistemological nature: we'll only ever parse only what we can parse, but we can't parse what we won't even try to parse.
Two, in becoming part of the first page of the (as of yet unwritten) supertagging bible, the concept implicitly reinforces the belief that grammars not following a distribution of occurrences similar to (or denser than) that of the CCGbank are practically unusable.
This is really a quite serious pitfall containing a contradiction worth pointing out -- it basically says that lexicalization is good, but only as long as it's not too itemized! 
Lexicalized formalisms that are ``too lexicalized'' are condemned to a premature end in fear of never making it past the supertagging phase, while it is only through such formalisms we could ever hope to trivialize the parsing process and fulfill the lexicalist dream... go figure.

Epistemological ramblings aside and back to reality, the type grammar we have developed is not among the lucky few.
\AE thel is way sparser than the CCGbank, containing five times as many types, while test samples with at least one rare type (i.e. a type with less than 10 occurrences in the joined train and dev subsets) appear four times as often as in the CCGbank (14.5\% vs 3.5\%).
Disregarding rare types is making ourselves content with an idealized (unachievable) peak sentential parsing accuracy of 86.5\%, which is far from an aspiring start.
Worse yet, these statistics are suggestive of a big type universe%
	\footnote{Read as (big (type universe)) and not as ((big type) universe) -- if you don't see the difference, ignore this comment.}, 
of which we likely have a observed only but a glimpse through the lens of \AE thel.
In practical terms, we done messed up, and no existing technology will save us now.
But, as the proverb goes, ``necessity is the mother of invention''%
	\footnote{I like the thought of Invention as the daughter of a happy progressive family with three mothers, the other two being Laziness and Boredom.}
and we're definitely in need here, so we may as well invent something.

In reality, what we need is less of an invention and more of an observation.
The important thing to observe is that supertags (be them combinatorial categories, type-logical types or anything resembling them) are not \textit{ad hoc}, opaque and dissimilar \textit{units}, but highly regular, transparent and decomposable \textit{structures}, made of a small set of primitives and the binders that piece them together.
In our setup, complex types are the result of type forming operators applied to ``smaller'' types, the smallest types available being atomic propositions; recall the (strategically placed) exercise of Figure~\ref{figure:litten_acg_der}.
This insight is really quite trivial and won't come as a surprise to anyone that has even superficially dabbled in the joys of algebraic data types, context-free grammars, inductive tree structures, or any sort of the hierarchical recursions common in computer science.
Theoretically unsurprising as it may be, it offers an interesting applied perspective: why teach a statistical model how to \textit{disambiguate} (i.e. choose) between some candidate assignments (however many), when we could instead teach it to \textit{construct} (i.e. inductively describe) the most suitable assignments instead?
A system able to consult the present linguistic context in order to construct well-formed and well-motivated types would amount to the first ever specimen of a new species: a supertagger with an unrestricted codomain.
We'll call this species of supertaggers \textit{constructive}.
This perspective is in a sense orthogonal to the transition from fixed, corpus-extracted assignment frequencies to word vectors. Whereas one generalizes over rare and unseen items in the first coordinate of lexical entries (\textlangle word, type\textrangle{} pairs), the other does so over the second.
The two combined lift the closed world assumption, paving the way for the \textit{last supertagger} -- one able to reliably predict the correct type (be it rare or unseen) for any word (be it rare or unseen).

\subsection{Supertagging as NMT}
\label{subsection:snmt}
Our first attempt at a constructive supertagger is described in detail by~\citet{kogkalidis-etal-2019-constructive}.
Like all first attempts at anything, it is characterized by a degree of cute naivety combined with an overeager execution.
Types are first viewed as the corresponding formula trees, and then traversed in a depth-first left-first fashion (i.e. read off in Polish notation).
Each type thus yields a type-word, viewed as the produce of a tiny recursive grammar (a context free one), the alphabet of which would be the union of propositional constants and logical connectives.
A sequence of types is represented as the concatenation of the sequentialized types, each type-word separated from the next by an (in hindsight unecessary) special alphabet token.
This expansion of a type-word into multiple symbols inadvertently breaks the input-to-output alignment; words are no longer associated to a single output symbol, but rather a sequence thereof.
As expected, this means that a sequence tagger is no longer a fitting backend for our experimental ventures.
Thankfully, the two biggest buzzwords of machine learning in 2018 are both surprisingly relevant here.

\subsubsection{Buzzwords}
\paragraph{Neural Machine Translation} Neural machine translation (NMT) is the modern paradigm to machine translation, the task of automatically translating text from some source language to a target one.
The term made its explosive first appearance halfway through the last decade, taking the field by storm~\cite{kalchbrenner2013recurrent,cho2014learning,bahdanau2015neural}.
The dominant approach rests on a sequence-to-sequence neural model~\cite{cho2014learning,NIPS2014_a14ac55a}, which consists of two parts: a sequence \textit{encoder}, which builds a contextual representation of the input sequence, and a sequence \textit{decoder} which uses the input representation to iteratively produce the output sequence on a token by token basis.
For an input sequence $\seq{x}{0:M}$ mapped to an output sequence $\seq{y}{0:N}$, this corresponds to a conditional language model trained to maximize
\begin{equation}
	p(y_i \ | \ \seq{y}{0:i-1},\seq{x}{0:M})
\end{equation}

The above conditional is of course exactly identical to (\ref{equation:lstm_lm}); in fact the supertag language model of~\citet{vaswani-etal-2016-supertagging} \textit{is} a degenerate case of neural machine translation, where $\mathbf{y}$ is $\mathbf{t}$ and $\mathbf{x}$ is $\mathbf{w}$, and $M$ and $N$ coincide.
This is not an one-off, but rather an instace of a broader trend, referred to as \textit{generalized} machine translation.
The generalized part stems from the fact that neither the source nor the target language are in any way constrained to being natural (or human) languages; either of the two (or both!) may well be artificial languages.
The actually interesting bit is that they don't actually even need to be languages \textit{per se}; any complex data structure that can be canonically traversed into an unambiguous sequentialization makes for a valid input/output.
\citet{vinyals2015grammar} explore the idea in training a sequence-to-sequene parser by directly translating the input sentence into a linearized constituency tree; the model is surprisingly accurate in learning both \textit{how} to create valid trees (only occasionally producing malformed output), and \textit{which} valid tree to create for a given sentence (with an accuracy comparable or matching previous established models).

The paradigm is of course bland, making no assumptions about the output structure and requiring little to no task-specific tuning.
For the exact same reasons, it is also highly appealing, and a good starting point for experimentation -- we can just apply it virtually unchanged to the task at hand.
In our domain, the goal sequence $\mathbf{y}$ would be the sum of symbols together forming our sequence of type-words, and $\mathbf{x}$ will be none other than the sentence itself.
Using the doubly indexed $s_{i,j}$ to denote the $j$-th symbol of the $i$-th type (symbol enumeration following the depth-first left-first traversal of the formula tree), the conditional being model becomes:
\begin{equation}
	\prod_i^n \prod_j^{|| t_i ||} 
	p(s_{i, j} \ | \ 
		s_{k, :} : k < i,
		s_{i, k} : k < j,
		\wseq)
\end{equation}
where $||t_i||$ the number of symbols of type-word $i$.
Note that the above is essentially an expanded version of~(\ref{equation:lstm_lm}), in the sense of containing intermediate evaluations in between full supertags.
This view allows drawing a parallel between type-words made of primitive symbols and words made of subword units~\cite{sennrich-etal-2016-neural}. The two share the same high-level purpose of improving ``translation'' to rare (type-)words, even though the structural decomposition of types is much more regular and consistent than the morphological decomposition of words.

\paragraph{Neural Attention}
Encoding the input sequence to a fixed length vector is essentially lossy neural compression.
The longer the input and output sequences are, the more this compression may prove catastrophic in capturing long range dependencies~\cite{cho2014properties}.
As an alternative, attention-based models circumvent the need for compression by simply building a contextually informed representation of the full input, distributed evenly among its tokens (one representation per sequence element).
These representations can be dynamically weighted and summed, yielding a distinct view of the same structure based on an external aggregation context (a query).
Attention has its roots in neural image processing~\cite[\textit{inter alia}]{larochelle2010learning,NIPS2014_09c6c378}, but its application to language was essentially the catalyst that set the field ablaze~\cite{bahdanau2015neural}.

Even though attention was originally used as an enhancement on top of RNNs, the code of conduct today is basically attention only.
The instigator of that paradigm shift was the transformer architecture~\cite{vaswani2017attention}, which by now enjoys an unprecedented pop-science status (saving me the hassle of having to regurgitate yet another ``transformers explained'' pamphlet).
In high level terms (and consciously oversimplifying), the transformer is a heteroassociative memory mechanism.
It builds three distinct representations for each sequence token: \textit{queries} dictate what each token looks for, \textit{keys} dictate what each token associates with, and \textit{values} correspond to stored memories.
A distance metric (commonly a scaled dot-product) is used to induce a weighting over the keys matrix for each query vector; we may say that queries \textit{attend} to keys.
The resulting weights are normalized to sum to one, and act as multiplicative factors in the weighted averaging of the values matrix, yielding a vector acting as a distinct evaluation of the full sequence for each query.
This basic operation is trivial to parallelize, both across tokens within the same sequence, as well as across independent sequences, thus allowing an efficient many-to-many message passing contextualization that can be stacked multiple times in depth for expra expressivity.
Using this as a decoder is just as easy, since queries may come from a different sequence than keys and values, provided their dimensionalities (not the counts!) match.
The only requirement is a masking strategy that disallows autoregressed tokens from attending to their future while training, since that would be cheating.
This is significant for training in the NMT setup, as it trades the linear temporal delay of the RNN for the quadratic memory cost of the attention matrix (quadratic because all tokens must attend to all tokens).

\subsubsection{Implementation}
Our problem is ripe with long distance dependencies.
Moreover, these are not confined to being only between encoder-decoder token pairs, but may also exhibit within decoder token pairs alone.
Consider that the misalignment between input and output means that we must consult the full input sequence at each decoding step, while the structurally liberal type logic means that cues to the current step may be found locally (within the same type), or multiple types (and thus even more steps) away.
For this reason alone, the transformer seemed like a good candidate architecture.%
	\footnote{Nothing to do with it topping the machine learning citation charts of 2018.}
Adhering to evidence that pretrained language models seem to benefit either side of the encoder-decoder pipelne, the encoder would consist of a Dutch version of ELMo, the \textit{de facto} language model of the time~\cite{peters-etal-2018-deep,che-etal-2018-towards}.
To account for domain adaptation without having to compute the costly gradient updates for the over-parameterized language model, a single transformer encoder was used to contextualize ELMo's precomputed representations.
The encoder was connected to a tiny transformer decoder of two layers, allowing unhindered access to the full input and all previous outputs.

\subsubsection{Experiments \& Results}
\paragraph{Training}
The model was trained with teacher forcing, i.e. predicting the current step assuming perfect rather than predicted (noisy) context.
For regularization, and in order to discourage the model from memoizing common type patterns, the Kullback-Leibler divergence was employed as the loss function, computed between the model's predictions and the ground truth, with 20\% of the probability mass evenly distributed across the non-true entries (basically a naive implementation of the label smoothed cross entropy loss~\cite{szegedy2016rethinking}).
The training data would consist of samples counting less than 20 words, pulled from the current at the time version of \AE thel.
This historical version of the dataset is only vaguely reminiscent of its current incarnation, the core difference being the use of $\NLP$ as the type logic, with an informal decoration of the implication standing for today's modalities.
Despite formal and representational divergences, the distribution of types is practically identical in between the two versions; as a fun trivia, only about 85\% of the total unique types were present in the training split used.
Exact numbers aside, insights gained from that past venture do carry over to the present.

\paragraph{Evaluation}
Unlike work in CCGbank, evaluation cannot be done on a comparative basis, due to the absence of established baselines%
	\footnote{There's basically noone to beat.}.
Cross-framework comparisons are also irrelevant due to the vastly different problem formulations (i.e. different linguistic framework, corpus, language); to drive the point across, consider that accuracy was measured over a set of 5\,700 types, which is 1 order of magnitude above CCGbank's 425 non-thresholded categories.
What's worth exploring instead is (i) the architecture's potential at supertagging, and (ii) its ability to learn reasonable generalizations beyond its training data.
To that end, we may view constructive and discriminative supertagging not as two orthogonal approaches, but as the extreme points of a continuum.
At the intermediate points between these extremes, there exist alphabets containing composite symbols that correspond to notational shorthands for the most common type and sub-type patterns.
As more of notational shorthands are introduced, the target output's length is significantly decreased, but the model is exposed to progressively less constructions of full types.
This allow us to approximately map the landscape between a fully constructive supertagger and a fully discriminative one.

On a purely numerical basis, the results are not astounding. 
Constructive accuracy lies at a disheartening 88\%, which is far from sufficient for downstream parsing.
What is intriguing, though, is that accuracy gradually declines with the introduction of notational shorthands, falling all the way down to 87.2\% with the eventual collapse to a discriminative autoregressive tagger.
Let's repeat this once more: obfuscating type structure hinders performance.
The story looks even more promising when it comes to the far end of the zipfian tail: 19.2\% of type assignments involving unseen types are correctly predicted, as are 45.7\% of those involving rarely seen types; these plunge to an unavoidable 0 and 23.9\%, respectively, with the transition to a discriminative setup.
Furthermore, not a single type is malformed, indicating that the grammar of type formation is indeed learnable, even when incorporated within a challenging sequence labeling task.
Raw numbers aside, the results suffice to deem the experiment an objective success: we generated concrete evidence that a full dismantling of the lexicon is not just possible but in fact also beneficial for supertagging a sparse type grammar.

\subsubsection{Insights \& Observations}
\paragraph{Advantages}
The prime advantage is the acquisition of rare and unseen supertags, which is a major accomplishment in its own right.
Secondary advantage \#1 is the unintended provision of trained representations for zeroary and n-ary primitives%
	\footnote{Replace with appropriate framework-specific terminlogoy, e.g. atomic propositions and logical connectives, atomic categories and categorial combinators, etc.},
either contextual (i.e. as provided by the decoder) or stand-alone (i.e. as provided by the embedding layer).
In the first case, they enact contextual representation that live in the disputed zone between the input sentence and the output derivation, suggesting new routes to parsing -- we'll see about that in a bit.
In the second case, they may find use as high-granularity supertag representations, allowing the dynamic representation of \textit{any} valid supertag, akin to character-level embeddings for a character level model -- supertag representations could then find use in downstream applications as an extralingual input~\cite{kasai-etal-2017-tag}.
Secondary advantage \#2 is the possibility for a hyper-articulated heuristic search during decoding, as we are now able to branch off to different sequences of assignments by sampling not only across types, but also within them.
A different symbol might drastically alter and affect the future of the decoding, locally within the current type or globally across the full sequence.
Other than potentially improving the sample efficiency of beam search, this can further be used to strictly enforce structural constraints, as we will also see in a bit.

\paragraph{Downsides}
With the benefit of posterity, it is also quite clear the approach suffers from a series of limitations.
First and foremost, there's the superficial fact that overall accuracy is far from groundbreaking, pointing to the need for architectual search and hyper-optimization adjustments.
A deeper issue is the computational penalty of the naive application of the transformer; unfolding supertags to primitive symbols has added a second product in the formulation of (\ref{equation:lstm_lm}).
The sequential decoding inherited from NMT means that this extra product excerts a multiplicative influence to decoding time, made quadratic in terms of memory footprint (that's what we get for succumbing to popular trends); the model is computationally expensive, slow and bullky to optimize.
At the same time, we have not fully kept our initial promise; structure may have been revealed, but it was not paid the respect due.
Supertags were brutally leveled into one-dimensional decals, their original treeness reflected neither in the representations nor in the structural inductive bias of the learning machine.
We still have to do better.

\subsection{Geometric Constraints}
\citet{prange-etal-2021-supertagging} notice the problem and seek to resolve it by explicating the categorial tree structure.
Their methodology abides by the encoder-decoder paradigm, but with one crucial, task-specific adaptation: the decoders experimented with are tree biased, making them a far better fit for addressing the problem at hand.
The general setup has the encoder build a contextualized representation for each word in the input, which is to serve as the initial seed for the decoding of the respective supertag; the decoder is then independently applied among all trees.
Two decoders are considered; a tree-shaped variant of the gated recurrent unit~\cite{cho2014properties} and a positionally informed densely connected network.
The first recurses along the tree structure, generating each local symbol dependent on its direct ancestor.
The second sums the initial seed with the projection of a feature vector describing the local position and its ancestry (both fixed choices among some predetermined possibilities).

The approach makes for a well-motivated path in the right direction.
The new formulation completely eliminates the burden of \textit{how} trees are constructed, allowing the model to focus on \textit{which} trees to construct. 
At the same time, the decoders considered are now token-separable, i.e. they can be applied in parallel across both sequences and trees.
Where previously we would have to perform $\sum_i^n ||t_i||$ decoding steps, this now shrinks to $\mathsf{max}_i^n ~ \mathsf{depth}(t_i)$ -- practically a constant, and a reduction of at least one order of magnitude.
Furthermore, words and supertags are now structurally aligned, relieving the model from having to learn the implicit soft alignments necessary at each decoding step.
On the practical side, numbers are significantly improved across the board (except for the far end of the zipfian tail), making the model a real alternative to the discriminative \textit{status quo}.
This becomes even more relevant considering how easily the setup lends itself to the multitagging paradigm (an insight omitted from the authors), as multiple trees may be obtained by following along the path of the factorization (modulo accounting for depth-width smoothing):
\begin{equation}\label{equation:tree_ar}
	p(t_i \ | \ \wseq) = \prod_j^{||t_i||} p(s_{i,j} \ | \ s_{i,k} : k \in \mathsf{ancestors(j)}, \wseq)
\end{equation}
All these merits come, however, at a heavy price: in parallelizing decoding across trees, the architecture loses the ability to model auto-regressive interactions between output nodes belonging to \textit{different} trees; interactions that can be crucial at the granularity scale we are now at.
The task is morally reduced to a sequence classification once more, albeit now with a dynamically adaptive classifier; we are back at (\ref{equation:seq_cls}), except for each local decision being elaborated according to (\ref{equation:tree_ar}).

The two approaches seem to be at odds, but the tension between them is highly artificial.
Both merely suffer from the naivety of conflating structural biases and decoding order: one forgets about tree structure in opting for a sequential decoding, whereas the other does the exact opposite, forgetting about sequential structure in opting for a tree-like decoding.
What we need to do is disentangle the two concepts, observing first that the output type is neither $\mathsf{Seq}[s]$ nor $\mathsf{Tree}[s]$ but $\mathsf{Seq}[\mathsf{Tree}[s]]$.
And that's it.
Having done that, the work that remains is of purely technical nature; we just need to come up with the spatiotemporal dependencies that abide by \textit{both} structural axes, and then a neural architecture that can accommodate them.
The choice of temporal (decoding) order is easy: \citet{prange-etal-2021-supertagging} make a very compelling case for depth-parallel decoding, seeing as it's insanely fast (we are not temporally bottlenecked by left-to-right sequential dependencies) but also structurally elegant (trees are only built when/if licensed by non-terminal nodes, ensuring structural correctness virtually for free).
Sticking with depth-parallel decoding means necessarily foregoing some autoregressive interactions: we certainly cannot look to the future (i.e. tree nodes located deeper than the current level, since these should depend on the decision we are about to make), but neither to the present (i.e. tree nodes residing in the current level, since these will be all decided simultaneously).
This leaves some leeway as to what could constitute the decision context, and here's where we can improve upon prior work in adding the missing structural dependencies.
The maximalist position is nothing less than the entire past, i.e. \textit{all} the nodes we have so far decoded.
Crucially, this abolishes conservative ancestry biases, establishing ``diagonal'' structural interactions between autoregressed nodes without requiring them to be directly linked to one another, or even share the same ancestral heritage (belong to the same tree), casting (\ref{equation:supertag}) to:
\begin{equation}
	\prod_i^n \prod_j^{||t_i||} p(s_{i, j} \ | \ p(s_{:, k} : \mathsf{level}(k) < \mathsf{level}(j), \wseq)
\end{equation}

The point might seem stretched but it is really just subtle.
If you're having trouble following along, take a look at Figure~\ref{figure:canvas}, displaying an abstract (partial) canvas of the constructive supertagger's input/output space, where $w_a$, $w_b$, $w_c$ are the first three words of the input sequence, with corresponding goal trees $a$, $b$ and $c$, the nodes of which enumerated according to a left-first depth-first traversal.
Focusing on autoregressive interactions alone, the sequential approach we started from would have each node depend on all nodes to its left and below; without loss of generality, $b_6$ would for instance depend on all of $a$, but also $b_1$, $b_2$, $b_3$, $b_4$ and $b_5$, as well as any descendants of the last two.
The tree-biased approach would have each node depend on its ancestors; for $b_6$, these would be just $b_3$ and $b_1$.
The tree-sequential approach envisaged here has each node depend on all nodes below it; the prediction of $b_6$ is now informed by the contents of nodes $[a/b/c/\dots]_{1,2,3}$.
The convention is that shallow nodes (presumably the easiest ones) are decoded first, unraveling the next layer of the canvas (we won't need to waste any compute on predicting, say, $b_6$ if either of $b_3$ and $b_1$ was a terminal symbol), while providing disambiguation context for deeper nodes (presumably harder) along the entire sequence.

\begin{figure}
	\begin{tikzpicture}[
        nf/.style={text=black!60},
        ne/.style={draw=black!60},
        ts/.style={}]  
        \tikzset{grow'=up}
        \tikzset{sibling distance=0pt}
        \tikzset{level 1/.style={level distance=36pt}}
		\tikzset{level 2/.style={level distance=32pt}}
		\tikzset{level 3+/.style={level distance=28pt}}
			\begin{scope}[xshift=-100pt]
			\Tree 
				[.{$w_a$} 
				\edge[->];
					[.{$a_1$}
						[.{$a_2$}
							[.{$a_4$}
								\edge[dotted]; {\hphantom{\dots}}
								\edge[dotted]; {\hphantom{.}}
							]
							[.{$a_5$}
								\edge[dotted]; {\hphantom{.}}
								\edge[dotted]; {\hphantom{\dots}}
							]
						]
						[.{$a_3$}
							[.{$a_6$}
								\edge[dotted]; {\hphantom{\dots}}
								\edge[dotted]; {\hphantom{.}}
							]
							[.{$a_7$}
								\edge[dotted]; {\hphantom{.}}
								\edge[dotted]; {\hphantom{\dots}}
							]
						]
					]
				]
			\end{scope}
			\Tree 
				[.{$w_b$}
				\edge[->];
					[.{$b_1$}
						[.{$b_2$}
							[.{$b_4$}
								\edge[dotted]; {\hphantom{\dots}}
								\edge[dotted]; {\hphantom{.}}
							]
							[.{$b_5$}
								\edge[dotted]; {\hphantom{.}}
								\edge[dotted]; {\hphantom{\dots}}
							]
						]
						[.{$b_3$}
							[.{$b_6$}
								\edge[dotted]; {\hphantom{\dots}}
								\edge[dotted]; {\hphantom{.}}
							]
							[.{$b_7$}
								\edge[dotted]; {\hphantom{.}}
								\edge[dotted]; {\hphantom{\dots}}
							]
						]
					]
				]
			\begin{scope}[xshift=100pt]
			\Tree 
				[.{$w_c$}
				\edge[->];
					[.{$c_1$}
						[.{$c_2$}
							[.{$c_4$}
								\edge[dotted]; {\hphantom{\dots}}
								\edge[dotted]; {\hphantom{.}}
							]
							[.{$c_5$}
								\edge[dotted]; {\hphantom{.}}
								\edge[dotted]; {\hphantom{\dots}}
							]
						]
						[.{$c_3$}
							[.{$c_6$}
								\edge[dotted]; {\hphantom{\dots}}
								\edge[dotted]; {\hphantom{.}}
							]
							[.{$c_7$}
								\edge[dotted]; {\hphantom{.}}
								\edge[dotted]; {\hphantom{\dots}}
							]
						]
					]
				]
			\end{scope}
			\begin{scope}[xshift=170pt]
				\node (dots) at (0,0) {\dots};
			\end{scope}
    \end{tikzpicture}
    \caption{Abstract canvas of a constructive supertagger's I/O structure.}
    \label{figure:canvas}
\end{figure}

\subsubsection{Geometry-Aware Supertagging}
\label{subsubsection:gas}
The operationalization of this novel approach is described in an as of yet unpublished (and not for lack of effort%
\footnote{As to why this is the case, your guess is as good as mine. Qualitatively, the reviews are all over the place, ranging from ``I don't get it, reject.'' to ``I don't get it, but looks impressive!''. Quantitatively, they follow $\mathcal{N}(3.5, 1.15)$ (sample size 7, minimum 1, maximum 4). Props to the *ACL reviewing community, and the morons populating it for their consistency and commitment.})
manuscript~\cite{kogkalidis2022geometryaware}.
Seeing as this is the last and most authoritative word in supertagging to date, we'll expand upon it here.
First off, the spatiotemporal dependencies we seek to implement do not follow the inductive biases of any run-of-the-mill architecture we may find precompiled in some machine learning library.
The closest paradigm available are graph neural networks (GNNs), which are essentially the most general class of neural architectures, suitable for learning on arbitrary graphs and manifolds (points, sequences, canonical grids, trees -- these are all just very specific instances of graphs: every neural network is a subclass of a graph neural network).
GNNs are usually formulated on the basis of some graph structure, where primitive graph entries (edges, nodes or both) are iteratively updated in a series of so-called \textit{message passing} rounds.
The concrete implementation of the messaging scheme (including what the flow of communication is and how messages are constructed) are up for deliberation.

In our case, it would be straighforward to add direct messaging components that implement exactly the spatiotemporal dependencies described earlier.
But this lacks subtlety, making no attempt at exploiting the regularity of the output space; sure -- it may be neither sequence nor tree, but it's not an ad-hoc graph either!
Computationally, this would not bode very well either; the number of interactions to compute would be upper bound by the series:
\begin{equation}\label{equation:dense_graph}
\sum_{k=0}^{m := \mathsf{max}_i^n ~ \mathsf{depth}(t_i)}
	\Big(
	\hspace{-22.5pt}
	\underbrace{\vphantom{\sum\nolimits_{k'}^{k}} 2^{k}n}_{\text{\# prediction targets}}
	\hspace{-5pt}
	\times
	~~~
	\big(
	\underbrace{\sum\nolimits_{k'=0}^{k-1} 2^{k'}n}_{\text{\# context nodes}}
	~~
	+ 
	\underbrace{\vphantom{\sum\nolimits_{k'}^{k}}  n}_{\text{input length}}
	\hspace{-10pt}
	\big)
	\Big)
%= \frac{2}{3}(4^{d+1} - 1) \cdot n^2
\end{equation}
whose memory footprint grows as $O(2^{2m} n^2)$, scaling quadratically with sequence length and exponentially with twice the maximal tree depth -- yikes.
To keep this beast under check, we would do well to utilize the output's geometric constants, namely the words.
A reasonable way to do that would be as state tracking vectors (fixed both in count and in length).
Akin to RNN hidden states, these shall be iteratively updated by the decoding process, while simultaneously reining it in.
Practically, each decoding step shall be conditioned on the current states, with each state (word) informing only the nodes it is associated with (the supertag it will decode into) in a one-to-many fashion, i.e. $n$ parallel messaging rounds, each from a \textit{single} state to the (maximally) $2^k$ nodes above.
Conversely, after the step has concluded, states will receive feedback from the nodes last predicted, again respecting word boundaries, now in a many-to-one fashion, i.e. again $n$ parallel messaging rounds, now from the $2^k$ freshly decoded states back to the single state they are assocciated with (originate from).
Unlike the naive approach, the setup maintains the word/supertag alignment while also structurally fusing the input- and output-level interactions sources.
Nodes are indirectly informed by all \textit{local} nodes below, with a much more endearing complexity of just:
\begin{equation}
\sum_{k=0}^{m}
	\underbrace{2^{k} n}_{\text{prediction messages}}
	+
	\underbrace{2^{k} n}_{\text{feedback messages}}
\end{equation}
which now grows as $O(2^m n)$.
Of course, something is amiss: the depth-wise intra-tree interactions may well be captured, but the inter-tree ones are unaccounted.
In the same vein as before, we may bypass this by having the state vectors communicate with one another after each local feedback around, allowing non-local autoregressive context flows.
Having this done globally (all words communicating with all words) is certainly feasible and still preferrable to (\ref{equation:dense_graph}), but suboptimal: it inserts a $mn^2$ memory complexity component ($m$ messaging rounds in the cartesian product of words).
A better alternative can be found in the dusty scriptures of old: sliding windows.
Regulating and thresholding state interactions according to their relative distance reinstates computational well-behavedness%
	\footnote{Actually scratch that, this was meant to just preserve the \textit{locality} of supertagging all along.},
substituting $n^2$ with $\kappa^2$ (basically a constant, for window size $\kappa$) and setting the final memory footprint of the decoder at $O(2^mn +m)$.
	
Computational considerations aside, this formulation is also conducive to learning.
Having interactions modulated and bottlenecked by state tracking vectors reduces the number of statistical confounds accessible to the model, acting as an implicit regularizer and enforcing a degree of locality to the (otherwise distributed) neural representations.
It also justifies a heterogeneous formulation, which would have different graph elements inhabit different vector spaces.
State vectors are recurrent across depth and inter-communicating across width, thus meriting from high-dimensional representations; with that in mind, they can initially be supplied by an external high-horsepower encoder, solving the initial interfacing with the input sentence. 
Tree nodes, on the other hand, encode a decision over a very small vocabulary and are use-and-forget, justifying a low-dimensional representation.
Finally, implementing the forward, backward and horizontal message passing rounds as separable, parameter-sharing operations repeated both across depth as well as width reduces the model's parameter count and provides the inductive biases needed for strong generalization.
Summarizing, the decoding algorithm looks as follows:
\begin{enumerate}
	\item State vectors are initialized by some external encoder.
	\item An empty fringe consisting of $n$ blank nodes is instantiated, one such per word, rooting the corresponding supertag trees.
	\item Until a fix-point is reached (there is no longer a fringe):
		\begin{enumerate}
			\item States project class weights to their respective fringe nodes in a one-to-many fashion. Depending on the arity of the decoded symbols, a next fringe of unfilled nodes is constructed at the appropriate positions.
			\item Each state vector receives feedback in a many-to-one fashion from the just decoded nodes above (what used to be the fringe), yielding tree-contextual states.
			\item The updated state vectors exchange messages within their local neighborhoods in a many-to-many fashion, yielding tree-and-(sub)sequence-contextual states.
		\end{enumerate}
\end{enumerate}

\subsubsection{Implementation}
We are no longer in the distant past; high-level fluff will no longer suffice.
Scientific integrity and lack of peer approval also compel explication.
The paragraphs to follow detail how the abstract pipeline is executed in practice. 
Consider yourself warned: you are urged to skip to the next section if sensitive to machine learning jargon, or the calendar year in your frame of reference is greater or equal to 2026 (I expect every single word to be obsolete by then).

\paragraph{Node Embeddings}
State vectors are temporally dynamic and of size $d_w$; they are initialized to $\mathbf{h}_{0:n}^0 \in \mathbb{R}^{n\times d_w}$ by some external encoder, and are then updated through the fix-point iteration of three message passing rounds, as described in the next paragraphs.
Tree nodes, on the other hand, are not subject to temporal updates, but instead become dynamically ``revealed'' by the decoding process. 
Their representations of size $d_n$ are computed on the basis of (i) their primitive symbol and (ii) their position within a tree.

Primitive symbol embeddings are obtained from a standard embedding table $W_e: \mathcal{S} \to \mathbb{R}^{d_n}$ that contains a distinct vector for each symbol in the set of primitives $\mathcal{S}$. 
When it comes to embedding positions, we are presented with a number of options.
It would be straightforward to fix a vocabulary of positions, and learn a distinct vector for each.
But this is neither inclusive nor elegant: it imposes an ad-hoc bound to the shape and size of tree nodes that can be encoded (contradicting the constructive paradigm), and fails to account for the compositional nature of trees.
The structure-conscious route requires noting that \textit{paths} over binary branching trees form a semi-group, i.e. they consist of two primitives (namely a left and a right path), and an associative non-commutative binary operator that binds two paths together into a single new one.
The archetypical example of a semigroup is matrix multiplication; we therefore instantiate a tensor $P \in \mathbb{R}^{2 \times n_d \times n_d}$ encoding each of the two path primitives as a linear map over symbol embeddings.
From the above we can derive a function $p$ that converts positions to linear maps, by performing consecutive matrix multiplications of the primitive weights, as indexed by the binary word of a node's position; e.g. the linear map corresponding to position $12_{10} = 0011_{2}$ would be $p(12) = P_0P_0P_1P_1 \in \mathbb{R}^{d_n \times d_n}$.
We flatten the final map by evaluating it against an initial seed vector $\rho_0 \in \mathbb{R}^{d_n}$, corresponding to the tree root (or the initial hidden state in the RNN paradigm).
To stabilize training and avoid vanishing or exploding weights and gradients, we model paths as \textit{unitary} transformations by parameterizing the two matrices of $P$ to orthogonality using the exponentiation trick on skew-symmetric bases~\cite{bader2019computing,lezcano2019trivializations}.
Now, let tree node $s_{i, k}$ contain symbol $\sigma \in \mathcal{S}$; its embedding $n_{i,k}$ will be agnostic to its tree index $i$ and given as as the element-wise product of its tree-positional and content embeddings:
\begin{equation}
n_{i, k} = p(k)(\rho_0) \odot \left(W_e(\sigma)\right) \in \mathbb{R}^{d_n}
\end{equation}
The embedder is then essentially an instantiation of a unitary RNN~\cite{arjovsky2016unitary}, except applied on tree paths rather than symbol sequences.%
	\footnote{Concurrently, \citet{bernardy2022assessing} follow a similar approach in teaching a unitary RNN to recognize Dyck words, and find the unitary representations learned to respect the compositional properties of the task.
	Here we go the other way around, using the unitary recurrence exactly because we expect them to respect the compositional properties of the task.}
Since paths are shared across trees, their representations are in practice efficiently computed once per batch for each unique tree position during training, and stored as fixed embeddings during inference.

\paragraph{Node Prediction} 
Assuming at step $\tau$ a sequence of globally contextualized states $\mathbf{h}^{\tau}_{0:n}$, we need to use each element $h^{\tau}_i$ to obtain class weights for all of the node neighborhood $\mathcal{N}_{i, \tau}$ consisting of all nodes (if any) of tree $t_i$ that lie at depth $\tau$.%
	\footnote{That's a lot of indexing operations. Look at figure~\ref{figure:canvas} and assume an enumeration that starts from 0 for timesteps and sequence positions and 1 for node positions. Then $\mathcal{N}_{1,2}$ would be $\{b_4, b_5, b_6, b_7 \}$.}
We start by down-projecting the state vector into the node's dimensionality using a linear map $W_n$.
The resulting feature vectors are indistinguishable between all nodes of the same tree -- to discriminate between them (and obtain a unique prediction at each node), we gate the feature vectors against each node's positional embedding.
From the latter, we obtain class weights by matrix multiplying them against the transpose of the symbol embedding table~\cite{press-wolf-2017-using}:
\begin{equation}
\mathrm{weights}_{i,k} = \left(p(k)(\rho_0) \odot W_n h^{\tau}_i\right) W_e^\top
\end{equation}
The above weights are converted into a probability distribution over the alphabet symbols $\mathcal{S}$ by application of the $\mathsf{softmax}$ function. 
The distribution can be sampled over to generate a concrete prediction.

\paragraph{Autoregressive Feedback}
To update the states for the next iteration, we must first provide autoregressive feedback from the last decoded nodes.
We do so using a heterogeneous message-passing scheme based on graph attention networks~\cite{velivckovic2018graph,brody2021attentive}.
First, we use a a linear map $W_b$ to down-project the state vector into the nodes' dimensionality.
For each position $i$ and corresponding state $h^\tau_i$, we compute a self-loop score:
\begin{equation}
\tilde{\alpha}_{i,\circlearrowleft,\tau} = w_{a} \cdot (W_b(h^\tau_i) \ ||  \ \mathbf{0})
\end{equation}
where $w_a \in \mathbb{R}^{2d_n}$ a dot-product weight and $\mathbf{0}$ a $d_n$-dimensional zero vector.
Then we use the (now decoded) neighborhood $\mathcal{N}_{i, \tau}$ to generate a heterogeneous attention score for each node $s_{i, k} \in \mathcal{N}_{i, \tau}$:
\begin{equation}
\tilde{\alpha}_{i,k,\tau} = w_a \cdot (h^{\tau}_i \ || \ n_{i, k})
\end{equation}
Scores are passed through a leaky rectifier non-linearity before being normalized to attention coefficients $\alpha$.
These are used as weighting factors that scale the self-loop and input messages, the latter upscaled by a linear map $W_m$:
\begin{equation}
	\tilde{h}^\tau_i = \sum_{s_{i, k} \in \mathcal{N}_{i,\tau}} {\alpha}_{i,k,\tau}W_m n_{i,k} + {\alpha}_{i,\circlearrowleft,\tau} h^\tau_i
\end{equation}
This can also be seen as a dynamic residual connection -- $\alpha_{i,\circlearrowleft,\tau}$ acts as a gate that decides how open the state's representation should be to node feedback (or conversely, how strongly it should retain its current values).
States receiving no node feedback (i.e. states that have completed decoding more one or more time steps ago) are thus protected from updates, preserving their content.
In practice, attention coefficients and message vectors are computed for multiple independently attention heads as done by~\citet{vaswani2017attention}, but these are omitted from the above equations to avoid cluttering the notation.

\paragraph{Sequential Feedback}
At the end of the node feedback stage, we are left with a sequence of locally contextualized states $\tilde{h}^\tau_i$.
The sequential structure can be seen as a fully connected directed graph, nodes being states (words) and edges tabulated as the square matrix $\mathcal{E}$, with entry $\mathcal{E}_{i, j}$ containing the relative distance between words $i$ and $j$.
%$\mathcal{E}$ labeled by relative distances between word pairs.
We embed these distances into the encoder's vector space using an embedding table $W_r \in \mathbb{R}^{2\kappa \times d_w}$, where $\kappa$ the maximum allowed distance, a hyper-parameter.
Edges escaping the maximum distance threshold are truncated rather than clipped, in order to preserve memory and facilitate training, leading to a natural segmentation of the sentence into (overlapping) chunks.
Following standard practices, we project states into query, key and value vectors~\cite{vaswani2017attention}, and compute the attention scores between words $i$ and $j$ using relative-position weighted attention~\cite{shaw-etal-2018-self}:
\begin{equation}
\tilde{a}_{i,j} = d_w^{-1/2}~(W_q \tilde{h}^\tau_i \odot W_r\mathcal{E}_{i,j}) \cdot W_k\tilde{h}^\tau_j
\end{equation}
From the normalized attention scores we obtain a new set of aggregated messages:
\begin{equation}
m_{i,t}' = \sum_{j \in \{0..s\}} \frac{\mathsf{exp}(\tilde{a}_{i,j}) W_v\tilde{h}^\tau_j}{
\sum_{k\in \{0..s\}}
\mathsf{exp}(\tilde{a}_{i,k})}
\end{equation}
Same as before, queries, keys, values, edge embeddings and attention coefficients are distributed over many heads. 
Aggregated messages are passed through a swish-gated feed-forward layer~\cite{dauphin2017language,shazeer2020glu} to yield the next sequence of state vectors:
\begin{equation}
h^{\tau+1}_i = W_3\left(\mathsf{swish}_1(W_1 m_{i,\tau}')\odot W_2m_{i,\tau}'\right)
\end{equation}
where $W_{1,2}$ are linear maps from the encoder's dimensionality to an intermediate dimensionality, and vice versa for $W_3$.

\paragraph{Putting Things Together}
We compose the previously detailed components into a single layer, which acts a sequence-wide, recurrent-in-depth decoder.
We insert skip connections between the input and output of the message-passing and feed-forward layers~\cite{he2016deep}, and subsequently normalize each using root mean square normalization~\cite{zhang2019root}.

\subsubsection{Experiments \& Results}
\paragraph{Datasets}
The way to publication is often arduous, but a tried and true shortcut is to beat prior benchmarks in established datasets.
Another tried and true shortcut is to replicate others' work without proper attribution.
Both shortcuts are telling of the times; I don't care much about the former, but the latter I do find bothersome.
In any case, testing the architecture on multiple datasets is good practice, as it better affirms its potential, increases chances to glory and fame, and disarms publication sharks.
Hence, we shall employ it not just on \AE thel but also on the two versions of the CCGbank, as well as the French TLGbank; in total, 4 different datasets spanning three languages and as many grammar formalisms.

\begin{table}
    \centering
    \small{
    \begin{tabular}{@{}l@{\qquad}c@{\quad}c@{\qquad}c@{\qquad}c@{}}
        & \multicolumn{2}{c}{\textbf{\textit{CCGbank}}} 
        & \multirow{2}{*}{\textbf{\textit{TLGbank}}}
        & \multirow{2}{*}{\textbf{\textit{\AE thel}}}\\ 
        & \textit{original} & \textit{rebank} \\
        \toprule
        \textbf{Primitives}     & 37        & 40        & 27        & 83\\
        ~ Zeroary           & 35        & 38        & 19        & 33\\ 
        ~ Binary            & 2         & 2         & 8         & 50\\
        \midrule
        \textbf{Categories}     & 1\,323      & 1\,619      & 851      & 6\,008\\
        ~{in train}         & 1\,286      & 1\,575      & 803      & 5\,377\\
        ~{depth avg.}       & 1.94      & 1.96      & 1.99     &   1.82\\
        ~{depth max.}       & 6         & 6         & 7        & 35*\\ 
        \midrule
        \textbf{Test Sentences} & 2\,407      & 2\,407      & 1\,571**    & 5\,770 \\
        ~{length avg.}      & 23.00     & 24.27     & 27.58    &  16.52\\
        \midrule
        \textbf{Test Tokens}    & 55\,371     & 56\,395     & 44\,302     & 95\,331\\
        ~ Frequent {(100+)}   & 54\,825     & 55\,690     & 43\,289     & 91\,503\\
        ~ Uncommon {(10-99)}  & 442       & 563       & 833       & 2\,639\\
        ~ Rare {(1-9)}        & 75        & 107       & 149       & 826\\
        ~ Unseen {(OOV)}      & 22        & 27        & 31        & 363\\
        \addlinespace
        	\multicolumn{5}{@{}l}{*A beautiful conjunction of 35 noun phrases.}\\
      	\multicolumn{5}{@{}l}{**Random but consistent train/dev/test split of 80/10/10.}
    \end{tabular}}
    \caption{Bird's eye view of datasets employed and relevant statistics. Test tokens are binned according to their corresponding categories' occurrence count in the respective dataset's training set. Token counts are measured before pre-processing. Unique primitives and tree depths for the type-logical datasets are counted \textit{after} binarization.}
    \label{table:datasets}
\end{table}


A high-level overview of the datasets is presented in Table~\ref{table:datasets}.
The English CCGbank and its refined version~\cite[rebank]{honnibal2010rebanking} stand out in having combinatory categories as their supertags, built with the aid of two binary slash operators.
Combinatory rules take care of shifting, raising and function composition, allowing the lexicon to remain small and simple.
The key difference between the two versions lies in their tokenization and the plurality of categories assigned, the latter containing more assignments and a more fine-grained set of syntactic primitives, which in turn make it a slightly more challenging evaluation benchmark.
In more familiar grounds we have the French TLGbank, \AE thel's distant but cherished uncle. 
It uses modalities for control purposes, licensing or restricting the applicability of rules related to non-local syntactic phenomena.
Its supertags are therefore multimodal Lambek types, the tree representations of which are not strictly binary; to make compatible with the architecture with minimal effort, we cast unary operators into pseudo-binaries by inserting an artificial terminal tree in a fixed slot within them.
A similar strategy is applied to \AE thel (which I assume is by now familiar).
Unary branches are shortened by first merging diamond-box pairs into a single composite symbol, and then iteratively merging adjunct (resp. complement) markers (either plain or composite) with the subsequent (resp. preceding) binary operator.
The new symbols correspond to notational and temporal shorthands for multiple decisions compressed in a single time step, making for an unambiguous and invertible representational translation at the cost of an enlarged primitive alphabet.
These shorthands are not to be confused with the ones we experimented with earlier in Section~\ref{subsection:snmt}.
Here, we are establishing representational shorthands for composite type operators (tree constructors); there, we were creating shorthands for frequent (sub-)types (self standing trees).%
	\footnote{If you're really really observant here, you might see a potential problem with this. 
	If you can't, I'll spoil it for you.
	Some of the composite alphabet symbols may appear in the test set (or in the wild) without having ever appeared in the training set.
	These would be impossible to predict, even in the constructive setting -- not because they're absent from the vocabulary, but because there's no usage examples to learn from!
	In fact, one such symbol exists, corresponding to the type/tree pattern $\dxdia{x}\dxbox{x}\ddia{cmpbody} \textvisiblespace \li \textvisiblespace$ for hypothesizing a deeply nested body of a complementizer -- it has 3 occurrences in the dev set and 1 in the test set.
	This is basically the zipfian tai of the zipfian tail -- we'll let it slide without rearranging the train/dev/test split, as it poses an innocuous and fun little challenge and an easter egg of shorts.
	Obviously, dropping the practical restriction for binarity would solve the problem, at the cost of elongated trees and the loss of architectural uniformity.}
	
\paragraph{Training}
A single hyper-parameter setup is shared among all experiments, obtained after a minimal logarithmic search over sensible initial values.
Specifically, we set the node dimensionality $d_n$ to 128 with 4 heterogeneous attention heads and the state dimensionality $d_w$ to 768 with 8 homogeneous attention heads.
We train using AdamW~\cite{	loshchilov2018decoupled} and a variable learning rate scaled by a linear warmup and cosine decay schedule over 25 epochs, scaled by 10\% for the encoder.
During training we provide strict teacher forcing and apply feature and edge dropout at 20\% chance.
The loss signal is derived as the label-smoothed negative log-likelihood between the network's prediction and the ground truth label.
Base-sized BERT variants are procured from the transformers library~\cite{wolf2020transformers}: RoBERTa for English~\cite{liu2019roberta}, RobBERT for Dutch~\cite{delobelle2020robbert} and CamemBERT for French~\cite{martin2020camembert}, all fine-tuned during training.

\paragraph{Evaluation}
We perform model selection on the basis of validation accuracy, and gather the corresponding test scores according to the frequency bins of Table~\ref{table:datasets}.
Table~\ref{table:comparisons} presents our results compared to relevant published literature.
Evidently, our model surpasses established benchmarks in terms of overall accuracy, matching or surpassing the performance of both traditional supertaggers on common categories and constructive ones on the tail end of the frequency distribution, so there's that.

\begin{table}
    \newcommand{\ave}[2]{\multirow{2}{*}{~~#1\textsubscript{$\pm$#2}}}
	\newcommand{\num}[1]{\multirow{2}{*}{#1}}
	
    \centering
    {\smaller
    \begin{tabularx}{1.00\textwidth}{@{}l@{~}Xc@{}c@{}c@{}c@{}c@{}}
    & & \multicolumn{5}{c}{\textbf{accuracy} {(\%)}} \\
    \cmidrule(lr){3-7}
    & \multicolumn{1}{c}{\textbf{model}} & {overall} & {frequent} & {uncommon} & {rare} & {unseen}\\
    \toprule
    \multicolumn{7}{l}{\textit{\textbf{CCGbank (original)}}} \\
    & \multicolumn{1}{l}{Symbol Sequential LSTM /w n-grams}
    & \num{95.99} & \num{96.40} & \num{65.83} &  \multicolumn{2}{c}{\num{8.65\textsuperscript{!}}} \\
    	& \multicolumn{1}{l}{~\smaller\cite{Liu_Ji_Wu_Lan_2021}}\\
    & \multicolumn{1}{l}{Recursive Tree Addressing}
    & \num{96.09} & \num{96.44} & \num{68.10} & \num{\textbf{37.40}} & \num{\textbf{3.03}} \\
	& \multicolumn{1}{l}{~\smaller\cite{prange-etal-2021-supertagging}}\\
    & \multicolumn{1}{l}{Cross-View Training}
    & \num{96.10} & \num{--} & \num{--} & \num{--} & \num{n/a} \\ 
    & \multicolumn{1}{l}{~\smaller\cite{clark-etal-2018-semi}}\\
    & \multicolumn{1}{l}{Attentive Convolutions}
    & \num{\textbf{96.25}} & \num{\textbf{96.64}} & \num{71.04} & \num{n/a} & \num{n/a}\\ 
    & \multicolumn{1}{l}{~\smaller\cite{tian2020supertagging}} \\
    \addlinespace
    & \multicolumn{1}{l}{Geometry-Aware Convolutions}
    & \ave{\textbf{96.29}}{0.04}
    & \ave{\textbf{96.61}}{0.04}
    & \ave{\textbf{72.06}}{0.72}
    & \ave{34.45}{1.58}
    & \ave{\textbf{4.55}}{2.87}\\
    & \multicolumn{1}{l}{~\smaller (this work)}\\
    \addlinespace
    \addlinespace
%    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \multicolumn{2}{L}{\textit{\textbf{CCGbank (rebank)}}} \\
    & \multicolumn{1}{l}{Symbol Sequential Transformer\textsuperscript{\textdagger}}
    & \num{90.68} & \num{91.10} & \num{63.65} & \num{34.58} & \num{\textbf{7.41}} \\
	& \multicolumn{1}{l}{~\smaller\cite{kogkalidis-etal-2019-constructive}}\\
    & \multicolumn{1}{l}{TreeGRU}
    & \num{94.62} & \num{95.10} & \num{64.24} & \num{25.55} & \num{2.47} \\
	& \multicolumn{1}{l}{~\smaller\cite{prange-etal-2021-supertagging}}\\
    & \multicolumn{1}{l}{Recursive Tree Addressing}
    &  \num{94.70} & \num{95.11} & \num{68.86} & \num{\textbf{36.76}} & \num{4.94} \\
	& \multicolumn{1}{l}{~\smaller\cite{prange-etal-2021-supertagging}}\\
    \addlinespace
    & \multicolumn{1}{l}{Geometry-Aware Convolutions}
    & \ave{\textbf{95.07}}{0.04}
    & \ave{\textbf{95.45}}{0.04}
    & \ave{\textbf{71.40}}{1.15}
    & \ave{\textbf{37.19}}{1.81} 
    & \ave{{3.70}}{0.00}\\
    & \multicolumn{1}{l}{~\smaller (this work)}\\
    \addlinespace
    \addlinespace
%    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \multicolumn{7}{l}{\textit{\textbf{French TLGbank}}} \\
%    % \midrule
    & \multicolumn{1}{l}{ELMo \& LSTM Classification}
    & \num{93.20} & \num{95.10} & \num{75.19} & \num{25.85} & \num{n/a}\\
	& \multicolumn{1}{l}{~\smaller\cite{moot2019}}\\
    \addlinespace
    & \multicolumn{1}{l}{Geometry-Aware Convolutions}
    & \ave{\textbf{95.92}}{0.01}
    & \ave{96.40}{0.01} 
    & \ave{\textbf{81.48}}{0.97}
    & \ave{\textbf{55.37}}{1.00} 
    & \ave{\textbf{7.26}}{2.67} \\ 
    & \multicolumn{1}{l}{~\smaller (this work)}\\
    \addlinespace
    \addlinespace
%    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
    \multicolumn{7}{l}{\textit{\textbf{\AE thel}} (v0.4.0)} \\
%    % \midrule
    & \multicolumn{1}{l}{Symbol Sequential Transformer\textsuperscript{\textborn}}
    & \num{83.67} & \num{84.55} & \num{64.70} & \num{50.58} & \num{\textbf{24.55}} \\
	& \multicolumn{1}{l}{~\smaller\cite{kogkalidis-etal-2020-neural}}\\
	\addlinespace
	\addlinespace
    \multicolumn{7}{l}{\textit{\textbf{\AE thel}} (v1.0.0a4)} \\
    & \multicolumn{1}{l}{Geometry-Aware Convolutions}
    & \ave{\textbf{95.92}}{0.01}
    & \ave{96.40}{0.01} 
    & \ave{\textbf{81.48}}{0.97}
    & \ave{\textbf{55.37}}{1.00} 
    & \ave{\textbf{7.26}}{2.67} \\ 
    & \multicolumn{1}{l}{~\smaller (this work)}\\
    \bottomrule
    \addlinespace
    \multicolumn{7}{l}{\textsuperscript{!}Accuracy over both bins, with a frequency-truncated training set.}\\
    \multicolumn{7}{l}{{\textsuperscript{\textdagger}Numbers from~\citet{prange-etal-2021-supertagging}.}}\\
    \multicolumn{7}{l}{{\textsuperscript{\textborn}Parser-integrated model trained with tree sequences spanning less than 140 nodes in total.}}
    \end{tabularx}}
    \caption{Model performance across datasets and compared to recent studies. Numbers are taken from the papers cited unless otherwise noted. For our model, we report averages and standard deviations over 6 runs. Bold face fonts indicate (within standard deviation of) highest performance.}
    \label{table:comparisons}
\end{table}

To investigate the relative impact of each network component, we conduct an ablation study where message passing components are removed from their network in their entirety.
Removing the state feedback component collapses the network into a token-wise separable recurrence, akin to a graph-featured RNN without a hidden-to-hidden affine map.
Removing the node feedback component turns the network into a Universal Transformer~\cite{dehghani2018universal} composed with a dynamically adaptive classification head.
Removing both is equatable to a 1-to-many contextualized token classification that is structurally unfolded in depth.
Our results, presented in Table~\ref{table:ablations}, verify first a positive contribution from both components, indicating the importance of both information sharing axes.
In three out of the four datasets, the relative gains of incorporating state feedback outweigh those of node feedback, and are most pronounced in the case of \AE thel, likely due to its positionally agnostic types.
With the exception of CCGrebank, relinquishing both kinds of feedback largely underperforms having either one, experimentally affirming their compatibility.


\begin{table}
    \centering
    {\small
    \begin{tabular}{@{}l@{\qquad}ccc@{}}
        &  {node feedback only}  & {state feedback only} & {no feedback} \\
        \toprule
        \textit{\textbf{CCGbank} (original)} & -0.05 & -0.01 & -0.08 \\ 
        \textit{\textbf{CCGbank} (rebank)} & -0.12 & -0.04 & -0.07 \\
        \textit{\textbf{French TLGbank}} & -0.13 & -0.14 & -0.23 \\
        \textit{\textbf{\AE thel}} v1.0.0a4 & -0.24 & -0.12 & -0.37
    \end{tabular}}
    \caption{Absolute difference in overall accuracy when removing the state and node feedback components (averages of 3 repetitions).}
    \label{table:ablations}
\end{table}


\subsubsection{Insights \& Observations}
\paragraph{Advantages}
Claiming state of the art performance while still being able to predict rare and unseen types with relative reliability is a definite advantage, despite having gone under the radar.
Like before, the approach provisions contextual and self-standing representations for atomic components virtually for free.
Unlike before, the architecture boasts an extremely fast inference speed that goes toe-to-toe with conventional discriminative architectures, owing to its temporal upper bound scaling with maximal tree depth (practically a constant, except for really perverse cases).
Structure manipulation is kept to the bare minimum, even in the absence of oracle guidance, maximizing GPU utilization and data parallelism with high efficiency sparse routines~\cite{fey2019fast}.
Operations are batched and temporally iterated across all nodes and sequences without any CPU interruptions.
Each next fringe is dynamically generated by selecting decoded symbols that correspond to tree forming operators, isolating their indices, multiplying them by two (to create left children), offsetting by one (to create right children), and finally interleaving the two, thus yielding both descendants on a purely numerical basis.
In the same vein, the new state indices are simply the repetition of their respective ancestor indices.
The architecture's memory footprint and parameter count are also the product of careful design and thus well under check, facilitating training and parser integration (as we will soon see).

\paragraph{Supertagging and Sparsity}
Practice aside, the results obtained pose concrete evidence that lexical sparsity, historically deemed the categorial grammar's curse, might well just require a change of perspective to tame and deploy as the answer to the very problem it poses.
Crucially, the architecture's relative gains scale with respect to the task's complexity.
In the original version of the CCGbank, the model is only slightly superior to the next best performing model -- an ad-hoc graph neural network with built in lexical biases (quite literally the ideological antipode of our endeavour).
The difference becomes order of magnitude wider for the slightly more challenging rebank version.
The effect is maximally pronounced for the harder type-logical datasets.
For the French TLGbank, performance jumps up to CCGbank scales (despite it being significantly smaller and sparser).
For \AE thel, the absolute performance leap is about 10\% compared to the vanilla constructive tagger.
Even though there's some data distance between the two experiments (version gap, different data filtering, etc.) making strict numeric comparisons moot, the sizeable improvement is beyond doubt.
This is clearly to be attributed to increased returns from the rare and uncommon bins.
There is a synergistic effect between the larger population of these bins pronouncing even minor improvements, while at the same time the acquisition of rare categories apparently benefits from their plurality.
Put simply, learning sparse assignments is \textit{easier} in grammars that contain many and diverse rare assignments, and improvements there \textit{matter} more -- especially so if these don't come at the cost of stability at the higher frequency spectrum.
The impact of this finding alone is bigger than the menial architecture itself -- it is basically an open invitation to more elaborate, more strict and more regular lexicalized theories, and a promise that no matter how statistically unruly they might seem, there will always be an architectural solution to accommodate them.
In today's machine learning frenzy, it is a statement of purpose lost: tools for the task, and not tasks for the tools.

\paragraph{Downsides}
Despite its objective success, the methodology is not without limitations.
Most importantly, the parallel nature of the decoder trades inference speed for an incompatibility with greedy algorithms like beam search and an inability to produce local assignment rankings.
Put plainly, obtaining more than the "best" category assignment per word is not straightforward, a fact which can prove harmful for coverage of a downstream parser.
A possible solution would involve branching across multiple tree-slices (i.e. sequences of partial assignments) rather than single predictions, but efficiently computing scores and comparing between complex structures is uncharted territory and not trivial to implement.
The issue is of course not unique to this system, but common to all decoders that perform multiple assignments concurrently -- as such, there is some hope that insights might percolate from one field to another and eventually make their way to us.

\section{Neural Proof Search}
We have made significant progress with supertagging, first alleviating the scary-looking roadblock of lexical type sparsity, and then producing numbers bigger than any numbers seen before%
	\footnote{Actually, \citet{tian2020supertagging} were reporting even bigger numbers for a while, but it later turned out that these numbers were in error and their real numbers were a lot smaller than the numbers they originally reported.
	An honest mistake, no doubt due to churning out the same architecture 5 times over the span of a year -- they thought you're not supposed to measure performance on rare assignments at all.
	Long story short, don't trust numbers you haven't made yourself.}%
, earning a spot at at the top 10 pop hits leaderboard for the next couple of months.
But supertagging alone is not going to take us to the end of the road.
Far from it, in fact, since our chosen logic is extremely permissive, burning away any hope of making do using just the proof-theoretic tools we have available.
The problem is simple: our type assignments, even when fully correct, still allow more proofs than desired -- we need yet another piece of statistical machinery that somehow tells us which one of these many proofs is the linguistically sensible one.
And once more, the discriminator approach is not going to work: enumerating and ranking all well-formed proofs is a no-go -- we have to build the correct proof from scratch instead.

To move forward we need to look back.
Almost an entire book ago, we had a brief encounter with proof nets, and we saw them at work in the context of $\ILL_{\li}$ (by now rebranded as \NLP).
Back then, we were quick to dismiss them after a moment of shallow appreciation, seeing as they were too complicated of a proof format for conducting search on, and too ambiguous of a representational format for showing around.
Lots have changed since, and not for the better; we added modalities, which impose further structure on their own, in turn necessitating yet another rule of deduction ($\Extraction$) invisible to proof nets.
Why then should we turn our attention back to them now?

This blast from the past is justified by the inadequacy of alternatives.
The natural deduction presentation we have predominantly employed is hierarchical and tree-like; assuming perfect processing and regardless of search direction (bottom-up or top-down), computation is temporally bound by the proof depth -- bottlenecked by decisions due, or anticipating decisions to be.
The same is true for most proof theoretic alternatives (tableaus, the sequent presentation, $\lambda$ terms, etc.).%
Decisions cannot be detached from the structure they bind to and help form -- to make a choice requires knowing your options, in turn requiring explicit symbolic manipulation.
Exploring a choice requires also evaluating it, and building the structure that will allow future choices to be made.
Even if the proof is to be dynamically constructed, abolishing the need for enumerating all options globally, each local junction point still needs to be exhaustively expanded for the construction to proceed.
Type safety is not for free: it costs back-and-forth between the symbolic engine and the statistical learner.
Proof nets, on the other hand, are the embodiment of data parallelism, the most saught out property of neural computation and the key to efficient training and optimization.
Decision making and validity testing are detached: we must first make all the decisions that are to be made, and only then can we check whether they are structurally legitimate.
This laxness was the very reason we abandoned proof nets in the first place, but it is exactly what makes them so very appealing now.

\subsection{Parsing as Permutation}
So what exactly is the structure of decisions prescribed in a proof net?
This might be easier to digest with an illustration.
%\subsection{Neural Proof Nets}
%The trickeries described next were first made public 

%Going bottom-up, computation is bottlenecked by decisions due.
%complicated to perform proof search on and ambiguous as a representational format.
%\subsection{Blast from the Past}

\begin{figure*}
    \resizebox{1\textwidth}{!}{
		\begin{tikzpicture}
		    [t/.style={text height=1.5ex, text depth=.25ex, rectangle, outer sep=0pt},
		    node distance=10pt,
		    r/.style={},
		    g/.style={},
		    tree/.style={very thick},
		    link/.style={dashed}]
        \tikzset{grow'=up}
        \tikzset{sibling distance=0pt}
        \tikzset{level 1/.style={level distance=36pt}}
		\tikzset{level 2/.style={level distance=32pt}}
		\tikzset{level 3+/.style={level distance=28pt}}
		\Tree
			[.{{$\li\ddia{whbody}$}}
				[.{$\li\ddia{predc}$}
					{$\vnw^0$}
					{$\svi^1$}
				]
				{$\whq^2$}
			]			
		\begin{scope}[xshift=100pt]
			\Tree
			[.{{$\li\ddia{whbody}$}}
				[.{$\li\ddia{predc}$}
					{$\vnw^0$}
					{$\svi^1$}
				]
				{$\whq^2$}
			]		
		\end{scope}
%		\node[t] (wat)      at (0, 0) {\w{Wat}};
%		\node[t,g] (wat_fn_1) at (0, 1) {$\li\ddia{whbody}$};
%        \node[t,r] (wat_fn_2) at (-1.5, 2.5) {$\li\ddia{predc}$};
%        \node[t, g] (wat_pron) at (-2.5, 4) {$\vnw^0$};
%        \node[t, r] (wat_svi) at (-0.5, 4) {$\svi^1$};
%        \node[t, g] (wat_whq) at (1.5, 2.5) {$\whq^2$};
%
%		\node[t] (is) at (4, 0)     {\w{is}};
%		\node[t,g] (is_fn_1) at (4, 1)  {$\li\ddia{predc}$};
%		\node[t,g] (is_fn_2) at (5, 2.5) {$\li\ddia{su}$};
%		\node[t,r] (is_pron) at (3, 2.5) {$\vnw^3$};
%		\node[t,r] (is_np) at (4.25, 4) {$\np^4$};
%		\node[t,g] (is_svi) at (5.75, 4) {$\svi^5$};
%		
%		\node[t] (dat)   at (8, 0)   {\w{die}};
%		\node[t,g] (dat_fn) at (8, 1)  {$\dbox{det}\li$};
%		\node[t,r] (dat_n) at (7, 2.5) {$\n^6$};
%        \node[t,g] (dat_np) at (9, 2.5) {$\np^7$};
%        
%        \node[t] (rare)   at (11, 0)   {\w{rare}};
%		\node[t,g] (rare_fn) at (11, 1)  {$\dbox{mod}\li$};
%		\node[t,r] (rare_n_1) at (10, 2.5) {$\n^8$};
%        \node[t,g] (rare_n_2) at (12, 2.5) {$\n^9$};	
%        
%        \node[t] (tekening) at (14, 0) {\w{tekening}};
%        \node[t,g] (tekening_n) at (14, 1) {$\n^{10}$};
%
%        \draw[tree] (wat_fn_1) -- (wat_fn_2) -- (wat_pron);
%        \draw[tree] (wat_fn_2) -- (wat_svi);
%        \draw[tree] (wat_fn_1) -- (wat_whq);
%        
%        \draw[tree] (is_fn_1) -- (is_pron);
%        \draw[tree] (is_fn_1) -- (is_fn_2) -- (is_np);
%        \draw[tree] (is_fn_2) -- (is_svi);
%        
%        \draw[tree] (dat_fn) -- (dat_n);
%        \draw[tree] (dat_fn) -- (dat_np);
%        
%        \draw[tree] (rare_fn) -- (rare_n_1);
%        \draw[tree] (rare_fn) -- (rare_n_2);

%        \draw[->, dashed] (tekening_n) -- ($(tekening_n) + (0, 2.5)$) -| (rare_n_1);
%        \draw[->, dashed] (rare_n_2) -- ($(rare_n_2) + (0, 1.5)$) -| (dat_n);
%        \draw[->, dashed] (dat_np) -- ($(dat_np) + (0, 2.5)$) -| (is_np);
%        \draw[->, dashed] (is_svi) -- ($(is_svi) + (0, 1.5)$) -| (wat_svi);
%        \draw[->, dashed] (wat_pron) -- ($(wat_pron) + (0, 0.75)$) -| (is_pron);
%        \draw[->, dashed] (wat_whq) -- ++ (0, 4);
%         \draw[->] (de_np) -- ($(de_np) + (0, 2)$) -| (waarover_np_1);
%         \draw[->] (waarover_np_2) -- ($(waarover_np_2) + (0, 1)$) -| (zijn_np);
%         \draw[->] (dit_prn) -- ($(dit_prn) + (0, 4.5)$) -| (zijn_prn);
%         \draw[->] (zijn_smain) -- ($(zijn_smain) + (0, 2)$);
%         \draw[->] (wij_prn) -- ($(wij_prn) + (0, 4)$) -| (staan_prn);
%         \draw[->] (waarover_adv) -- ($(waarover_adv) + (0, 1.5)$) -| (staan_adv);
%         \draw[->] (staan_ssub) -- ($(staan_ssub) + (0, 2)$) -| (waarover_ssub);
	    \end{tikzpicture}
    }
    \caption{\todo}
%    Proof net equivalent of the proof of Figure~\ref{figure:nd_derivation}, with unary diamonds (resp. boxes) fused with the implication dominating (resp. dominated by) them for depth compression.
%    Atomic propositions are indexed by enumeration for identification purposes.
%    Color coding here serves to differentiate between resources we have (green) and resources we need (red) -- the rule is start green from the bottom, change (resp. keep) color for the left (resp. right) daughter of an implication.
%    Bold edges denote the tree structure underlying type assignments. 
%    Dashed edges denote the correct matching between resources of opposite polarity.}
    \label{figure:proofnet}
\end{figure*}


\section{Key References \& Further Reading}

The geometry-informed supertagger of Section~\ref{subsubsection:gas}	 bears semblance and owes credit to various contemporary lines of work, being an application-specific offspring of weight-tied architectures, dynamic graph convolutions and structured self-attention networks.
The depth recurrence is evocative of weight-tied architectures~\cite{dehghani2018universal,bai2019deep} and their graph-oriented variants~\cite{li2016gated}, which model neural computation as the fix-point iteration of a single layer against a structured input, thus allowing for a dynamically adaptive computation ``depth'' -- albeit with a constant parameter count.
Analogously to structure-aware self-attention networks~\cite{zhu-etal-2019-modeling,cai2020graph} and graph attentive networks~\cite{velivckovic2018graph,yun2019graph,ying2021transformers,brody2021attentive}, it also employs standard query/key and fully-connected attention mechanisms injected with structurally biased representations, either at the edge or at the node level.
Finally, akin to dynamic graph approaches~\cite{liao2019efficient,pareja2020evolvegcn}, it forms a closed loop system that autoregressively generates its own input, in the process becoming exposed to subgraph structures that drastically differ between time steps.

% biaffine
% richard
% french

\bibliographystyle{abbrvnat}
\bibliography{bibliography}
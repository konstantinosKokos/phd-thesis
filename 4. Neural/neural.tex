\chapter{Learning to Prove}
\label{chapter:chapter_4}


\chapabstract{\todo}


Reflections are normally reserved for the end of a chapter, but I'm in a pensive mood so I'll do them here instead.
This thesis was originally envisaged as part of a broader whole, a stepping stone toward an integrated approach at structural reasoning and meaning representation; the goal being a composition calculus for vector-based semantic modeling.
In the project's days of inception, that goal was not just noble but also technically feasible.
Distributional semantics and word vectors were in their heyday, machine learning was rapidly taking off, parsers all of a sudden started becoming reliable; everything seemed to point towards the imminent bloom of a new era in natural language processing (NLP), an era where the wisdoms of old would meet the machines of today, hinting at a bright and prosperous future for structure-aware semantic composition models.
And things did seem to go that way, at least for a few years.
But, unbeknownst to all, the new era in the evolutionary history of NLP would actually turn out to also be its dumbest one.
The advent of data efficient neural architectures (combined with their immense potential for commercial applications, and its allure to big corporations) brought large language models into the game: unsophisticated, profanely over-parameterized, general purpose systems, fed unprocessed texts for weeks on end until they'd learn to convincingly imitate its use.
Large language models usurped the heir apparent and condemned structure-aware semantic computation to obscurity; the future of computational semantics is to be opaque, boring, exclusive to big tech and provided as-a-service instead.
My thesis got caught in the blast of this change of power, necessitating a clear positioning in the current state of affairs, and a careful motivation for the chapter to follow; anything and everything done as -- or in the name of -- science requires justification after all.
So here goes.

Parsing is good.
Converting raw signals intro structured representations thereof allows us to standardize their machine processing, and elevates automated reasoning away from form and into substance.
The more well-behaved the representational format chosen, the more powerful, transparent and verifiable the reasoning can be.
The less localized and problem-specific the representational format chosen, the more adaptive and better understood the reasoning can be.
On the basis of these observations, $\lambda$ calculi make for an ideal representation format.
Choice of format aside, a formal system operating on formal representations is not prone to implicit biases, latent variables, ambiguity or inconsistency: erroneous outputs are the result of bad input or bad programming; there's always someone to blame.
Specifically in the natural language domain, advancing the conversion of text into formal representations is promoting accountable automation of textual processing, and eliminates the anthromorphic delusion of the ghost in the machine.
Parsing is therefore only superficially in competition with large language models, and its seeming obsoletion is just a by-product of the ephemeral and rapidly shifting pop science trends of the ``AI'' race.

That said, machine learning is not bad.
Shifting the focus away from the algorithm and toward the data can often be a reasonable concession in the automation of complex or labor-intensive tasks, provided that the task is not risk critical and that no intelligence is attributed to the end system.
This is especially the case if ``almost correct'' is almost as good as correct, or the problem being modeled is intractable, making an approximation the best we can hope for.
But employing machine learning has to be thought of as either a shortcut, or an admission of defeat.
In opting for a machine learning solution, one assigns more faith to a generic data cruncher in solving a problem, than to oneself in designing a solution to that very problem.

Interweaving symbolic and subsymbolic reasoning is then the responsible engineer's out.
Disassembling a system into two tiers of components promotes the selective expenditure of formal effort where it really is needed, while still benefiting from the  high horsepower of brute force statistical machinery.
Complicated but decipherable components, rich in hierarchical or recursive structure, and requiring or greatly benefiting from formal transparency are to be tackled explicitly, whereas components that are laborious but uninteresting, data intensive or intactable are to be isolated and outsourced to a machine worker.
In this here context, I'm claiming that large language models should be treated not as a substitute, but as complementary to logic-based systems.
This is exactly the route we'll follow in this chapter, where we'll go through the hoops of designing and implementing a formally disciplined but accurate and robust wide-coverage parser, a novel neurosymbolic architecture aimed at substructural logics of the linear lineage, instantiated here for \NLPplus{} and trained on \AE thel.

\section{Building a Categorial Parser}
We'll start with a high-level conceptualization of the categorial grammar parser.
In the infancy of categorial grammars, the parser would be thought of as nothing other than a lexicon and a theorem prover: the lexicon enumerating any and all the possible type assignments for each word, the theorem prover exhaustively iterating the combinatorial space of assignments to produce all possible proofs for each possible assignment (Figure~\ref{figure:archetypical_parser}).

\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}[t/.style={text height=1.5ex, text depth=.25ex, rectangle, outer sep=0pt}, node distance=10pt]
	\node[t] (w1) 			at (0, 0) {w\textsubscript{0}};
	\node[t] (wdots)		at (2, 0) {\dots};		
	\node[t] (wn) 			at (4, 0) {w\textsubscript{n}};
	\node[rectangle,draw=black, minimum width=120pt,minimum height=20pt] (lex)
						 	at (2, -1.5) {Lexicon};
	\draw[->]  (w1) -- ++ (0.6, -1);
	\draw[->] (wn) -- ++ (-0.6, -1);
	\node[t] (t1)			at (0.2, -3.2) {$\text{t}_0^0 \ | \ \text{t}_0^1 \ ... \text{t}_0^p$};
	\node[t] (tdots)		at (2, -3.2) 	  {\dots};
	\node[t] (tn)			at (3.8, -3.2) {$\text{t}_n^0 \ | \ \text{t}_n^1 \ ... \text{t}_n^q$};
	\draw[->] ($(w1) + (0.6, -2)$) -- ($(t1.north) + (0, 0.1)$);
	\draw[->] ($(wn) + (-0.6, -2)$) -- ($(tn.north) + (0, 0.1)$);
	\draw [thick,decorate,decoration={brace,aspect=0.2,amplitude=10pt,mirror}] (-0.5,-3.5) -- (4.5,-3.5) 
			node[black,xshift=-113.5pt,yshift=-0.6cm] {\footnotesize $\times$};
	\node[t] (c11)			at (1.5, -5) {$\text{t}^0_0$};
	\node[t] 				at (2.75, -5) {\dots};
	\node[t] (cn1)			at (4, -5) {$\text{t}^0_n$};
	\node[t] (c12)			at (1.5, -6) {$\text{t}^0_0$};
	\node[t] 				at (2.75, -6) {\dots};
	\node[t] (cn2)			at (4, -6) {$\text{t}^1_n$};
	\node[t] 				at (1.5, -7) {\vdots};
	\node[t] (c1k)			at (1.5, -8) {$\text{t}^p_0$};
	\node[t] 				at (2.75, -8) {\dots};
	\node[t] (cnk)			at (4, -8) {$\text{t}^q_n$};
	\draw[->, thick, rounded corners] (0.5, -4.3) -- ++ (0, -0.7) -- ++ (0.75, 0);
	\draw[->, thick, rounded corners] (0.5, -4.3) -- ++ (0, -1.7) -- ++ (0.75, 0);
	\draw[->, thick, rounded corners] (0.5, -4.3) -- ++ (0, -3.7) -- ++ (0.75, 0);
	\node[rectangle, draw=black, minimum height=120pt] (pr)
							at (7, -6.5)	{\begin{tabular}{c}Theorem\\ Prover\end{tabular}};	
	\draw[->] (cn1) ++ (0.4, 0) -- ++ (1.3, 0);
	\draw[->] (cn2) ++ (0.4, 0) -- ++ (1.3, 0);
	\draw[->] (cnk) ++ (0.4, 0) -- ++ (1.3, 0);
	\draw[->]  (cn1) ++ (4.3, -1.5) -- ++ (1, 0) node[right] {\{p\textsubscript{0}\dots p\textsubscript{k}\}};
	\end{tikzpicture}
	\caption{The archetypical categorial grammar parsing pipeline.}
	\label{figure:archetypical_parser}
\end{figure}

Obviously, this setup hits a brick wall in the sheer complexity of real human language.
As we have discussed in earlier chapters, a type system enacting a strict grammar logic is not just hard to design, but also entails a prohibitively ambiguous type lexicon.
Even if the theorem prover is perfectly optimized, the architecture will become bottlenecked at its input.
The total number of assignments to consider in a sentence increases exponentially with its length, so even a minor increase in the average number of types per lexical key will have a tremendous impact in processing time.
At the same time, a fixed lexicon is a severely limiting factor, as it effectively forbids processing sentences containing unseen lexical entries (i.e. in cases of a \textlangle word, type\textrangle{} pair missing from the lexicon).
Relaxing the structural properties of the type system to ease lexical pressure is not a panacea either.
Other than the prover becoming ambiguous, more proofs will become accessible, and inacessible proofs will take longer to reject.
From the implementer's perspective, lexicon and grammar are not synergistic but in conflict with one another, and a middle ground must be found for them to work well in tandem.

For the categorial program to come to fruition, these very real problems require equally real solutions.
The practitioner must often resort to tricks aimed at compressing or efficiently navigating the enormous search space.
The modern pipeline commonly outsources lexical disambiguation to a statistical component, referred to as the \textit{supertagger}.
The supertagger is tasked with ranking the possible assignments to a single key, given its context of appearance.
Assignments are ranked according to their likelihood, in turn approximated on the basis of some training data.
Depending on the quality and speed of the statistical estimator employed, the candidates returned are truncated depending on some threshold likelihood or by their count.
This (partially) sidesteps the explosive combinatorics of considering all potential assignments, setting an upper boundary to the cardinality of the parser's input.
The parser may also be sped up by allowing yet another statistical model to guide its actions anytime it hits a decision point.
As with all real solutions, perfect is unattainable; this time/space efficiency usually comes at the cost of approximation errors that translate to foregone rigidness, correctness and/or coverage.

The strategy we'll follow does not challenge this general model, but contributes some new insights to the operationalization of its components.


\section{Supertagging}
A supertagger is a statistical model, a parametric function $f_\theta$ tasked with producing the most likely type assignment sequence $t_0 \dots t_n$ for a given sentence $w_0 \dots w_n$.
\begin{equation}
	f_\theta(w_0 \dots w_n) \approx \underset{t_0\dots t_n}{\mathsf{argmax}} ~ p(t_0\dots t_n \ | \ w_0 \dots w_n, \theta)
\end{equation}
To do so, it should in theory approximate the probability of a type assignment sequence conditional on the input; in other words, feeding $f_\theta$ with any element of the product space $\lexicon^k$ should implicitly produce a total order over the product space $\types^{k}$, where $\lexicon$ the set of words in the language, $\types$ the type universe, and $k$ ranging over $\mathbb{N}$.
If that looks stupidly intractable, it's because it is.
Both domain and codomain are practically infinite: regardless of what the cardinality of $\lexicon$ and $\types$ are, the number of combinations between different sequences thereof quickly exceeds our current estimates for stars in the universe, as the sequence length increases.
Put simply, no amount of sample data would ever be able to overcome the problem's inherent sparsity and allow for a direct attempt at an approximation.
Therefore, in practice, some truncations and independence assumptions are necessary in how we choose to formulate the sequence-wide conditional assignment probability, the decomposition of which will be the focal point of our discussion:
\begin{equation}\label{equation:supertag}
p(t_0\dots t_n \ | \ w_0 \dots w_n)
\end{equation}
A choice of assumptions and truncations is a prerequisite before we even get to contemplate the model's implementation.
Each choice can (and will) have a deep impact on the model's performance, most notably in phenomena inhabiting the more remote regions of the probability density function's landscape.
As a corollary, each choice will alter how suitable a model is to one single grammar depending exactly on how that landscape looks.
This last fact seems to have largely been dismissed by the broader practitioner community, who treat the problem with consistent indifference, changing the viewing lens only according to the quirks and fashions of contemporary machine learning standards.
We will shamelessly fall into the same last trap, but in our downfall we will at least be conscious of the intellectual and ideological roots the earlier chapters have established; those of revealing structure previously hidden, and paying that structure its due respect.

\subsection{A Brief History of Supertagging}
To actually perceive the structure, we must first notice its absence -- therefore (and for maintaining suspense), we will first outline the short but dense history of supertagging, and sketch out the paradigm shifts it has undergone throughout.

\subsubsection{Origins}
Supertagging (both the term and the idea) are due to the early insights of~\citet{joshi1994disambiguation}.
The two correctly pointed out that, for a strongly lexicalized grammar (in their case, a tree adjoining grammar), assigning the correct grammatic descriptor, or \textit{supertag} (in our case, a type), to each word in a sentence amounts to \textit{almost} parsing, and that even just weeding out some of the erroneous assignments significantly facilitates parsing.
Early literature was characterized by an almost single-minded attachment to localized computation, the justification being that supertagging must remain localized for it not to become ``too much like parsing''~\cite{bangalore1999supertagging}.
With the benefit of hindsight, we can safely call this a pragmatic consideration and an artifact of the times, with the scene largely dominated by window-based models.

A so-called unigram model assumes full independence between subsequent words, and (\ref{equation:supertag}) boils down to:
\begin{equation}
\prod_i^n p(t_i \ | \ w_i)
\end{equation}
where each local conditional can be estimated on the basis of corpus frequencies.
Despite competely breaking apart sequential sparity, this formulation is not much good on its own either; rarely occurring lexical keys hardly provide sufficient data for an empirical distribution to be extracted.
As a solution, plain part of speech tags would find use as an intermediary, i.e. $w_i$ would in practice be substituted by $pos_i$, which would in turn be supplied by an external tagger.
The resulting model is, alas, too simple to find real use: the assumptions made are exceedingly naive, and lexicalization is heavily bottlenecked by the coarse and undescriptive part of speech tags; we need to do better.

Invoking Bayes' rule and factoring out the denominator has (\ref{equation:supertag}) rewrite to the proportionate quantity:
\begin{align}
\propto p(w_0\dots w_n \ | \ t_0\dots t_n)~ p(t_0\dots t_n)
\end{align}
Extending the context to a window of size $\kappa$, allows local decisions to excert direct influence to the next $\kappa$ predictions (and thus indirectly affect all future ones).
This requires approximating the \textit{contextual probability} $p(t_0 \dots t_n)$ as:
\begin{equation}\label{equation:contextual_prob_jb}
p(t_0 \dots t_n) \approx \prod_i^n (t_i \ | \ t_{i-\kappa}\dots t_{i-1})
\end{equation}
Going one step further and making the assumption that the \textit{emission probability} $p(w_0\dots w_n \ | \ t_0\dots t_n)$ is position-separable and independent allows its rewrite to:
\begin{equation}\label{equation:emission_prob_jb}
p(w_0\dots w_n \ | \ t_0\dots t_n) \approx \prod_i^n p(w_i \ | \ t_i)
% \ \prod_i^n (t_i \ | \ t_{i-\kappa}\dots t_{i-1})
\end{equation}
Putting (\ref{equation:contextual_prob_jb}) and (\ref{equation:emission_prob_jb}) together, we get an approximation to (\ref{equation:supertag}) that is blatantly wrong.
Despite the fact, it is also workable, in having efficiently circumvented sparsity, adequate, in having accounted for the very important axis of output-to-output interactions, and practical, in allowing a tangible implementation as a hidden Markov model.


\subsubsection{CCGbank and the Original Sin}
The problem garnered attention and built significant impetus with the release of the CCGbank, whose large size and gold standard nature offered an excellent test bed for molding the first generation of supertaggers.
The first incarnation of a combinatory categorial grammar supertagger would be the original work of~\citet{clark2002supertagging}.
The model diverged from the implementation of~\citet{bangalore1999supertagging} in opting for a larger window size and foregoing the contextual effect of output-to-output dependencies.
In that setting, and for a window of size $2\kappa + 1$, (\ref{equation:supertag}) would take the form:
\begin{equation}
	\prod_i^n p(t_i \ | \ w_{i-\kappa} \dots w_{i+\kappa})
\end{equation}
The model would materialize as a log-linear feature weighter trained as a maximum entropy estimator.
The input would include several sparse heuristics, including morphological features and boolean context predicates, allowing a partial soft bypass of the fixed-key lexicon.

Novelty and ingenuity aside, the work set a number of precedents; some of those, reasonable as they may have been at the time, have since permeated through the problem statement, becoming \textit{de facto} practices rather than conscious design decisions.
Structural constraints were dropped, in part because they are less straightforward to deduce in frameworks other than tree adjoining grammars; they never found their way back into the mainstream, athough admittedly they never were particularly sophisticated to begin with.
This step away from structural discipline is exacerbated by having also dropped the supertag-to-supertag dependencies, since the model now has no chance of learning how to statistically filter out mutually incompatible assignments either.
To counteract the problem, the paper opted for a yet more radical solution: abandoning the sequential formulation (i.e. no more $\mathsf{argmax}$ing over the product) in favor of a \textit{multitagging} approach (i.e. returning all categories whose local probability exceeds some fixed ratio of the highest ranked candidate).
This was shown to tremendously improve coverage (by outsourcing heavier duty to the parser), but has to be understood as a practical overcorrection, an emergency measure to sidepass the model's inherent disregard to output-level sequential interactions.
The limitation is acknowledged by \citet{clark-curran-2004-importance}, and an attempt at resolution is offered by \citet{curran2006multi}.
There, the forward-backward algorithm is employed to efficiently calibrate the probability of an assignment (in the multitagging setup) as the sum of all sequential assignments containing it:
\begin{equation}
	p(t_i \ | \ w_0\dots w_n) = \sum_{\mathbf{t}_{0:i-1}, \mathbf{t}_{i+1:n}} p(\mathbf{t}_{0:i-1}, t_i, \mathbf{t}_{i+1:n} \ | \ w_0 \dots w_n)
\end{equation}
This does reinstate a notion of output-to-output dependencies in the form of estimated posteriors, but computational considerations have diffused the potential for widespread adoption in later frameworks. 
Finally, rare supertags, which were particularly problematic or near impossible to learn, were found to have very limited impact on overall coverage; this set the grounds for their (statistically) near inconsequential erasure, a choice that gradually became ingrained as a mandatory step of data sanitation and preprocessing.


\subsubsection{Distributed Word Vectors \& Neural Networks}
The advent of word embeddings and the gradual substitution of sparse features with continuous vectors paved the way for the incorporation of artificial neural networks.
\citet{10.1162/tacl_a_00186} employed a collection of pretrained embeddings combined with a window-based two-layer network in a ``semi-supervised'' manner (in today's jargon, a pretty much fully supervised separable convolution), to a dual effect.
On the one hand, the pretrained embeddings offered a natural generalization from the fixed size lexicon to the (still fixed, but much larger) set of pretrained` embeddings, single handedly obsoleting the long standing problem of tackling rare and unseen words~\cite{thomforde-steedman-2011-semi,deoskar-etal-2011-learning,deoskar2014generalizing}.
On the other hand, the parameter-sharing convolution improved the accuracy/ambiguity ratio and overall efficiency of the (then standard) log-linear supertagger of~\citet{clark2007wide}.
Unlike before, words were allowed to associate to any supertag, regardless of whether or not a \textlangle word, supertag\textrangle{} pair was observed during training; the lexicon thus turning from a hard imperative to a soft guideline.
Additional experiments involving a conditional random field were mildly succesful, but abandoned due to the prohibitively slow decoding -- yet, it is obvious to the people involved that something critical is missing.
\citet{xu-etal-2015-ccg} took the approach a step further by utilizing a simple recurrent network, and, in doing so, claimed to sidestep the locality of the previous neural model.
To escape the unidirectional constraint of the classical recurrent network (or perhaps out of force of habit?), they continued incorporating window-based features that provided a minimal amount of right context $\kappa$, thereby rewriting (\ref{equation:supertag}) as:
\begin{equation}
	\prod_i^n p(t_i \ | \ w_0 \dots w_{i+\kappa})
\end{equation}
And while their approach does indeed offer a wider receptive field (and a better operationalization), it is still focused solely on the input side; output-to-output interactions are still nowhere to be seen, leaving their promise unfulfilled.

\subsubsection{Autoregressive Modeling}
By now (and despite earlier aphorisms), it is becoming increasingly evident that nothing deep or spiritual restricts supertagging to remaining local, as advancements in machine learning are progressively offering more opportunities for fast and efficient incorporation of ever wider context.
But the absence of output-to-output dependencies remains unresolved, despite it being a recurrent theme in the literature.
This changes with the work of \citet{vaswani-etal-2016-supertagging} who score two major points with their resourceful use of long short-term memory networks (LSTM).
First, they replace the simple recurrence of \citet{xu-etal-2015-ccg} with a bidirectional one (thus allowing unbounded left- and right- input interactions) --  an idea explored in parallel by multiple contemporary works~\cite[\textit{inter alia}]{ling-etal-2015-finding,xu-etal-2016-expected,lewis-etal-2016-lstm}.
More importantly and in addition to that, they introduce an intermediate recurrence that is to serve as a supertag-level language model, fusing the prediction history with the input context to produce each local prediction.
The two together alter (\ref{equation:supertag}) into a version far more elaborate than previous proposals:
\begin{equation}
	\prod_i^n p(t_i \ | \ t_0\dots t_{i-1},w_0\dots w_n)
\end{equation}
Under a modern lens, this is akin to a slighly clumsy implementation of an autoregressive sequence-to-sequence model, with the decoder meriting from perfect alignment between input and output tokens.
Unlike prior work, no occurrence threshold was imposed, and explicit evaluations over sparse lexical relations were provided.
The added expressivity and significantly wider receptive field granted LSTM models the state of the art badge, which was to remain uncontested for a surprisingly long two human years%
	\footnote{Approx. three centuries in machine learning years.}.

\subsubsection{Superwhat?}
More than just a testament to the LSTM's strengths, this momentary pause makes for a discontinuity in the velocity of progress; not because people suddenly lost interest in supertagging, but rather because machine learning architectures and their applications had slowly become exhausted.
This coincides with a stall across NLP in general, and a concurrent paradigm shift; specialized models started becoming outfashioned, and improvements would no longer be enabled by domain expertise and task-specific engineering, but rather by higher quality unsupervised and semi-supervised training routines over larger and larger models.
A case in point is the next major landmark, in fact reached by a structurally simplified model~\cite{clark-etal-2018-semi}, 
a plain bidirectional sequence encoder using the factorization:
\begin{equation}
	\prod_i^n p(t_i \ | \ w_0 \dots w_n)
\end{equation}
Despite taking a step backwards in terms of structural sophistication, the model managed a performance leap comparable to that of switching from a separable neural function to a recurrent one (see Figure~\ref{figure:supertag_history}), all by ``simply'' incorporating multiple tasks and losses in its training loop.
The same paradigm is today more dominant than ever, and has pushed conventional NLP outside of the spotlight, putting an end to an exciting but short golden era.

\begin{figure}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	    xlabel={Publication Year},
	    ylabel={Accuracy (greedy)},
%	    xmin=1, xmax=48,
%	    ymin=-0, ymax=1,
%	    xtick={1,10,100,1000,10000, 100000}, 
	    legend pos=north west,
	    ymajorgrids=true,
		minor y tick num=1,
	    yminorgrids=true,
	    xmajorgrids=false,
%	    xminorgrids=true,
	    axis line style={draw=none},
	    tick style={draw=none},
%        legend pos=north east,
		xticklabels={,,2002,,,,,,,,2018},
		x tick label style={/pgf/number format/.cd, 1000 sep={},},
        width=0.85\textwidth,
		enlarge x limits=0.1,
		point meta=explicit symbolic,
		nodes near coords,
         nodes near coords style={
         	anchor=west,
            font=\small,
        },
	]
	    
	 
 	\addplot[mark = *, only marks,mark options={black}]%]
 		table[x=X,y=Y,meta=M] {
 			X		Y		M
 			2002 	90.5 	\citet{clark2002supertagging}
 			2004	91.5	\citet{clark-curran-2004-importance}
 			2014	91.3	\citet{10.1162/tacl_a_00186}
 			2015	93.07	\citet{xu-etal-2015-ccg}
 			2016	94.24	\citet{vaswani-etal-2016-supertagging}
 			2016	94.7	\citet{lewis-etal-2016-lstm}
 			2018	96.05	\citet{clark-etal-2018-semi}
 		};
	 \end{axis}
	\end{tikzpicture}
	\caption{Supertagging performance in the CCGbank historically.}
	\label{figure:supertag_history}
\end{figure}


\subsection{Constructing Types}
The supertagging architectures reviewed are, from first to last, variations on a theme.
Regardless of whether the underlying statistical machinery is a hidden Markov model, a maximum entropy model, a neural sequence tagger or a sequence-to-sequence transducer, a single commonality characterizes them all: they start from the assumption of a finite codomain.
More than that, they don't just assume but \textit{require} that the zipfian tail of lexical type sparsity is practically irrelevant for the corpus, and, by extension, for language at large.
In other words, they require that most of the probability mass of type occurrences is concentrated around a central region of few but common types, and that exceptionally rare types are nothing but statistical artifacts which can safely be ignored.
This bias is not to be mistaken for a vice, nor for a deeply motivated ruling; it is a practical compromise that became an unwritten rule, similar to the (once proclaimed as necessary) locality of supertagging -- a notion since abandoned and forgotten with minimal remorse and deliberation as soon as technology allowed.
The issue is really quite shallow: statistical models have always had a very hard time dealing with under-represented samples (in our case, supertags), and correctly predicting items outside the training data is an open problem.

That, again, is not to say the compromise is an unjust one; its heedless proliferation does, however, come with two major side effects.
One, it forces parsers to give up on potentially rare syntactic phenomena, assuming those manifest through unique and uncommon supertags.
Even though a parser should in principle be able to handle any valid supertag (regardless of its statistical properties), the \textit{a priori} exclusion of rare ones corresponds to an externally imposed restriction to its generalization.
In other words, exactly those difficult phenomena that would benefit from the linguistic expertise of a robust parser are to be discarded in the first place!
The problem is of an epistemological nature: we'll only ever parse only what we can parse, but we can't parse what we won't even try to parse.
Two, in becoming part of the first page of the (as of yet unwritten) supertagging bible, the concept implicitly reinforces the belief that grammars not following a distribution of occurrences similar to (or denser than) that of the CCGbank are practically unusable.
This is really a quite serious pitfall containing a contradiction worth pointing out -- it basically says that lexicalization is good, but only as long as it's not too itemized! 
Lexicalized formalisms that are ``too lexicalized'' are condemned to a premature end in fear of never making it past the supertagging phase, while it is only through such formalisms we could ever hope to trivialize the parsing process and fulfill the lexicalist dream... go figure.

Epistemological ramblings aside and back to reality, the type grammar we have developed is not among the lucky few.
\AE thel is way sparser than the CCGbank, containing five times as many types, while test samples with at least one rare type (i.e. a type with less than 10 occurrences in the joined train and dev subsets) appear four times as often as in the CCGbank (14.5\% vs 3.5\%).
Disregarding rare types is making ourselves content with an idealized (unachievable) peak sentential parsing accuracy of 86.5\%, which is far from an aspiring start.
Worse yet, these statistics are suggestive of a big type universe%
	\footnote{Read as (big (type universe)) and not as ((big type) universe) -- if you don't see the difference, ignore this comment.}, 
of which we likely have a observed only but a glimpse through the lens of \AE thel.
In practical terms, we done messed up, and no existing technology will save us now.
But, as the proverb goes, ``necessity is the mother of invention''%
	\footnote{I like the thought of Invention as the daughter of a happy progressive family with three mothers, the other two being Laziness and Boredom.}
and we're definitely in need here, so we may as well invent something.

In reality, what we need is less of an invention and more of an observation.
The important thing to observe is that supertags (be them combinatorial categories, type-logical types or anything resembling them) are not \textit{ad hoc}, opaque and dissimilar \textit{units}, but highly regular, transparent and decomposable \textit{structures}, made of a small set of primitives and the binders that piece them together.
In our setup, complex types are the result of type forming operators applied to ``smaller'' types, the smallest types available being atomic propositions; recall the (strategically placed) exercise of Figure~\ref{figure:litten_acg_der}.
This insight is really quite trivial and won't come as a surprise to anyone that has even superficially dabbled in the joys of algebraic data types, context-free grammars, inductive tree structures, or any sort of the hierarchical recursions common in computer science.
Theoretically unsurprising as it may be, it offers an interesting applied perspective: why teach a statistical model how to \textit{disambiguate} (i.e. choose) between some candidate assignments (however many), when we could instead teach it to \textit{construct} (i.e. inductively describe) the most suitable assignments instead?
A system able to consult the present linguistic context in order to construct well-formed and well-motivated types would amount to the first ever specimen of a new species: a supertagger with an unrestricted codomain.
We'll call this species of supertaggers \textit{constructive}.
This perspective is in a sense orthogonal to the transition from fixed, corpus-extracted assignment frequencies to word vectors. Whereas one generalizes over rare and unseen items in the first coordinate of lexical entries (\textlangle word, type\textrangle{} pairs), the other does so over the second.
The two combined lift the closed world assumption, paving the way for the \textit{last supertagger} -- one able to reliably predict the correct type (be it rare or unseen) for any word (be it rare or unseen).

\subsection{Supertagging as NMT}
Our first attempt at a constructive supertagger is described in detail by~\citet{kogkalidis-etal-2019-constructive}.
Like all first attempts at anything, it is characterized by a degree of cute naivety combined with an overeager execution.
Types are as viewed the corresponding formula trees and then traversed in a depth-first left-first fashion (i.e. read off in Polish notation).
Each type thus yields a type-word, viewed as the produce of a tiny context free grammar, the alphabet of which would be the union of propositional constants and logical connectives.
A sequence of types is represented as the concatenation of the sequentialized types, each type-word separated from the next by an (in hindsight unecessary) special alphabet token.
This expansion of a type-word into multiple symbols inadvertently breaks the input-to-output alignment; words are no longer associated to a single output symbol, but rather a sequence thereof.
As expected, this means that a sequence tagger is no longer a fitting backend for our experimental ventures.
Thankfully, the two biggest buzzwords of machine learning in 2018 are both surprisingly relevant here.
%(generalized) neural machine translation (NMT) and Transformers; these give us an alternative way forward.

\subsubsection{Neural Machine Translation \& The Transformer}

%The resulting misalignment between words and types (type-words consisting of multiple symbols) necessitated an alternative to the industry standard sequence tagging architectures.
%The modal treatment of dependencies was still in its infancy; rather than multiple pairs of residuated modalities, we assumed a collection of different implication arrows, each arrow essentially the composition of a dependency modality and the unadulterated linear implication.

%The implementation would rely on the buzzwords of Transformers and generalized machine transa

\subsection{Geometric Constraints}




%The sped up pace of progress and the rapidly increasing technical offerings from machine learning techn
%In reality, the thing to be avoided
%A significant advancement came by~\citet{vaswani-etal-2016-supertagging}
%
% naturally generalizing from a fixed size lexicon to the (much larger) set of available embeddings while improving the accuracy/ambiguity ratio of 

%for all factors that should have been accounted, namely both input-to-output and output-to-output interactions.

%e faithfully approximated by $\prod_i^n p(w_i \ | \ t_i)$ 




%\prod_i^n p(t_i \ | \ t_{i-\kappa}\dots t_{i-1}) \prod_i^n p(w_i \ | \ t_i)
%\end{equation}

%\subsection{}
%The domain is infinite -- even constraining the word sequences to grammatical ones and the set of words to the set of lexical entries in the sample data, the possible inputs are endless.
%The same applies to the codomain, as the type universe is an inductive set -- constraining the type assignments considered to the ones witnessed in the lexicon certainly improves the picture, but this barely leaves any dent in the combinatorial 
%In any case, no amount of sample data would ever be close to enough for a direct approximation 
%The real distribution is impossible to approximate, as no sample data would ever be able to even remotely account for either the (practical) infinity of the domain or the sparsity of the codomain.
% independence asssumptions%Thankfully, we are only interested in obtaining the first few most likely sequences of assignments, and less so in their actual probabilities (or those of the less likely assignments).



%type assignments according to an approximation of their probability


%
%
%\section{Supertagging}
%
%\subsection{A Brief History}
%
%\subsection{Constructing Types}
%
%\subsection{Decoding Order vs. Structure}
%
%
%\section{Neural Proof Nets}

\bibliographystyle{abbrvnat}
\bibliography{bibliography}